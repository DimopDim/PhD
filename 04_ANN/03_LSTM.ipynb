{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754cd06-f12e-4548-b461-5bf2d8ba7bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import h5py # Save - Load 3D tensor\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# display Matplotlib plots directly within the notebook interface\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99fbcd6-5e7e-480e-8cd7-5261b46ef4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"inference.log\", encoding=\"utf-8\"),  # Ensure UTF-8 encoding\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c05d2-855b-4867-a77c-677a0ddacc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'o4_3D_four_dataframe_hierarchical_attemp_I'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0e25c-c96d-41c1-a831-67c7ef1c2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensors from the HDF5 file\n",
    "load_path = 'CSV/exports/tensors/o4_3D_four_dataframe_hierarchical.h5'\n",
    "\n",
    "logging.info(f\"Loading...\")\n",
    "with h5py.File(load_path, 'r') as hf:\n",
    "    train_tensor = hf['train_tensor'][:]\n",
    "    validate_tensor = hf['validate_tensor'][:]\n",
    "    test_tensor = hf['test_tensor'][:]\n",
    "    external_tensor = hf['external_tensor'][:]\n",
    "    # los\n",
    "    train_los_label = hf['train_los_label'][:]\n",
    "    validate_los_label = hf['validate_los_label'][:]\n",
    "    test_los_label = hf['test_los_label'][:]\n",
    "    external_los_label = hf['external_los_label'][:]\n",
    "    # mortality\n",
    "    train_mortality_label = hf['train_mortality_label'][:]\n",
    "    validate_mortality_label = hf['validate_mortality_label'][:]\n",
    "    test_mortality_label = hf['test_mortality_label'][:]\n",
    "    external_mortality_label = hf['external_mortality_label'][:]\n",
    "\n",
    "logging.info(f\"Train: {train_tensor.shape}, Los Label: {train_los_label.shape}, Mortality Label: {train_mortality_label.shape}\")\n",
    "logging.info(f\"Validate: {validate_tensor.shape}, Los Label: {validate_los_label.shape}, Mortality Label: {validate_mortality_label.shape}\")\n",
    "logging.info(f\"Test: {test_tensor.shape}, Los Label: {test_los_label.shape}, Mortality Label: {test_mortality_label.shape}\")\n",
    "logging.info(f\"External: {external_tensor.shape}, Los Label: {external_los_label.shape}, Mortality Label: {external_mortality_label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b108afd5-72de-4f89-bf28-d167de796761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Args:\n",
    "input_size: Number of input features.\n",
    "hidden_sizes: List of hidden sizes for each LSTM layer.\n",
    "dropout: Dropout probability.\n",
    "\"\"\"\n",
    "\n",
    "class LSTMAttentionMultiTask(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, dropout=0.2):\n",
    "        super(LSTMAttentionMultiTask, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_layers = len(hidden_sizes)\n",
    "\n",
    "        # Define LSTM layers\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_sizes[0], batch_first=True, dropout=dropout)\n",
    "        self.lstm2 = nn.LSTM(hidden_sizes[0], hidden_sizes[1], batch_first=True, dropout=dropout)\n",
    "        self.lstm3 = nn.LSTM(hidden_sizes[1], hidden_sizes[2], batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Attention Layer\n",
    "        self.attention = nn.Linear(hidden_sizes[2], 1)  # Learn attention scores\n",
    "\n",
    "        # Two Fully Connected Layers for Multi-Task Learning\n",
    "        self.fc_los = nn.Linear(hidden_sizes[2], 1)  # Predicts LOS (regression)\n",
    "        self.fc_mortality = nn.Linear(hidden_sizes[2], 1)  # Predicts Mortality (classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Initialize hidden and cell states\n",
    "        h1, c1 = torch.zeros(1, batch_size, self.hidden_sizes[0]).to(x.device), torch.zeros(1, batch_size, self.hidden_sizes[0]).to(x.device)\n",
    "        h2, c2 = torch.zeros(1, batch_size, self.hidden_sizes[1]).to(x.device), torch.zeros(1, batch_size, self.hidden_sizes[1]).to(x.device)\n",
    "        h3, c3 = torch.zeros(1, batch_size, self.hidden_sizes[2]).to(x.device), torch.zeros(1, batch_size, self.hidden_sizes[2]).to(x.device)\n",
    "\n",
    "        # Forward pass through LSTMs\n",
    "        out, (h1, c1) = self.lstm1(x, (h1, c1))\n",
    "        out, (h2, c2) = self.lstm2(out, (h2, c2))\n",
    "        out, (h3, c3) = self.lstm3(out, (h3, c3))  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Apply Attention\n",
    "        attn_weights = torch.tanh(self.attention(out))  # (batch_size, seq_len, 1)\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)  # Normalize across time steps\n",
    "\n",
    "        # Weighted sum of hidden states (context vector)\n",
    "        context_vector = torch.sum(attn_weights * out, dim=1)  # (batch_size, hidden_size)\n",
    "\n",
    "        # Two separate outputs\n",
    "        los_output = self.fc_los(context_vector)  # Predicts LOS\n",
    "        mortality_output = torch.sigmoid(self.fc_mortality(context_vector))  # Predicts Mortality (0-1)\n",
    "\n",
    "        return los_output, mortality_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b5a5b-a401-4d04-b786-bf5a72afa70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train = torch.tensor(train_tensor, dtype=torch.float32)\n",
    "y_train = torch.tensor(train_los_label, dtype=torch.float32)\n",
    "\n",
    "X_validate = torch.tensor(validate_tensor, dtype=torch.float32)\n",
    "y_validate = torch.tensor(validate_los_label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa4b42-51d9-4554-a4bd-a3a87b3bebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_size = X_train.shape[2]  # Number of features\n",
    "hidden_sizes = [256, 128, 64]  # LSTM layers sizes\n",
    "\n",
    "# Instantiate model\n",
    "model = LSTMAttentionMultiTask(input_size, hidden_sizes)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Print model architecture\n",
    "logging.info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f5ebe-4eb6-4d41-b1a5-2e32707b97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Losses & Optimizer\n",
    "criterion_los = nn.MSELoss()  # Loss for LOS (Regression)\n",
    "criterion_mortality = nn.BCELoss()  # Loss for Mortality (Binary Classification)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# Training Setup\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "patience = 5\n",
    "min_delta = 0.001\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "early_stop = False\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = TensorDataset(torch.tensor(train_tensor, dtype=torch.float32), \n",
    "                              torch.tensor(train_los_label, dtype=torch.float32),\n",
    "                              torch.tensor(train_mortality_label, dtype=torch.float32))\n",
    "\n",
    "validate_dataset = TensorDataset(torch.tensor(validate_tensor, dtype=torch.float32), \n",
    "                                 torch.tensor(validate_los_label, dtype=torch.float32),\n",
    "                                 torch.tensor(validate_mortality_label, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca604b-ae28-4ea7-866d-80860176e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    if early_stop:\n",
    "        logging.info(f\"Early stopping triggered at epoch {epoch}. Restoring best model...\")\n",
    "        break\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Use tqdm to track batch progress\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for X_batch, y_los_batch, y_mort_batch in progress_bar:\n",
    "        X_batch, y_los_batch, y_mort_batch = X_batch.to(device), y_los_batch.to(device), y_mort_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        los_pred, mort_pred, attn_weights = model(X_batch)\n",
    "\n",
    "        loss_los = criterion_los(los_pred, y_los_batch.view(-1, 1))\n",
    "        loss_mortality = criterion_mortality(mort_pred, y_mort_batch.view(-1, 1))\n",
    "\n",
    "        total_loss = loss_los + loss_mortality\n",
    "        total_loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += total_loss.item()\n",
    "\n",
    "        # Update tqdm progress bar\n",
    "        progress_bar.set_postfix(loss=total_loss.item())\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_los_batch, y_val_mort_batch in validate_loader:\n",
    "            X_val_batch, y_val_los_batch, y_val_mort_batch = X_val_batch.to(device), y_val_los_batch.to(device), y_val_mort_batch.to(device)\n",
    "\n",
    "            val_pred_los, val_pred_mort, _ = model(X_val_batch)\n",
    "\n",
    "            val_loss_los = criterion_los(val_pred_los, y_val_los_batch.view(-1, 1))\n",
    "            val_loss_mortality = criterion_mortality(val_pred_mort, y_val_mort_batch.view(-1, 1))\n",
    "\n",
    "            total_val_loss = val_loss_los + val_loss_mortality\n",
    "            val_loss += total_val_loss.item()\n",
    "\n",
    "    val_loss /= len(validate_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Print epoch results\n",
    "    logging.info(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    sys.stdout.flush()  # Force immediate print output\n",
    "\n",
    "    # Early stopping\n",
    "    if best_val_loss - val_loss > min_delta:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            early_stop = True\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "# Restore best model\n",
    "if best_model is not None:\n",
    "    model.load_state_dict(best_model)\n",
    "    logging.info(\"Restored best model from early stopping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e45444-8f07-4348-ae9e-8711c1448d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to CPU before saving\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Define the model save path\n",
    "model_save_path = f\"models/{file_name}_model.pth\"\n",
    "\n",
    "# Save the trained model's state dictionary\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# Confirm model saving\n",
    "print(f\"Model saved successfully at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457e8d1-c979-41d4-be7b-bc8258350291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step 1: Get Attention Scores for a Sample\"\"\"\n",
    "# Select a random sample from the validation set\n",
    "sample_idx = np.random.randint(len(validate_dataset))\n",
    "X_sample, _, _ = validate_dataset[sample_idx]  # Only take the input tensor\n",
    "X_sample = X_sample.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "# Get model output and attention weights\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, _, attn_weights = model(X_sample)  # Get attention scores\n",
    "\n",
    "# Convert attention weights to numpy\n",
    "attn_weights = attn_weights.squeeze().cpu().numpy()  # Shape: (seq_len, 1)\n",
    "seq_len = attn_weights.shape[0]\n",
    "\n",
    "# Plot attention weights\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(seq_len), attn_weights, marker=\"o\", linestyle=\"-\", label=\"Attention Score\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Attention Score\")\n",
    "plt.title(\"Attention Weights Over Time for a Single Patient\")\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/tensor/02_attention_weight/{file_name}_single_patient.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357eda1-f564-41c5-93b3-9f8a3324f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step 2: Compare Attention Across Multiple Patients\"\"\"\n",
    "num_samples = 5  # Number of patients to visualize\n",
    "all_attn_weights = []\n",
    "\n",
    "# Select multiple random samples\n",
    "random_indices = np.random.choice(len(validate_dataset), num_samples, replace=False)\n",
    "\n",
    "for idx in random_indices:\n",
    "    X_sample, _, _ = validate_dataset[idx]\n",
    "    X_sample = X_sample.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, _, attn_weights = model(X_sample)\n",
    "\n",
    "    attn_weights = attn_weights.squeeze().cpu().numpy()\n",
    "    all_attn_weights.append(attn_weights)\n",
    "\n",
    "# Convert to NumPy array and compute mean attention weights\n",
    "all_attn_weights = np.array(all_attn_weights)  # Shape: (num_samples, seq_len)\n",
    "mean_attn_weights = np.mean(all_attn_weights, axis=0)  # Average over patients\n",
    "\n",
    "# Plot Mean Attention Weights\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(seq_len), mean_attn_weights, marker=\"o\", linestyle=\"-\", color='red', label=\"Mean Attention Score\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Mean Attention Score\")\n",
    "plt.title(\"Average Attention Weights Over Multiple Patients\")\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/tensor/02_attention_weight/{file_name}_multiple_patients.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a44689-ed24-4c14-a88c-6371a5e4013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step 3: Visualize LOS vs. Mortality Attention Differences\"\"\"\n",
    "\n",
    "num_samples = 5  # Number of patients to visualize\n",
    "los_attn_weights = []\n",
    "mortality_attn_weights = []\n",
    "\n",
    "# Select multiple random samples\n",
    "random_indices = np.random.choice(len(validate_dataset), num_samples, replace=False)\n",
    "\n",
    "for idx in random_indices:\n",
    "    X_sample, _, _ = validate_dataset[idx]\n",
    "    X_sample = X_sample.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        los_pred, mort_pred, attn_weights = model(X_sample)\n",
    "\n",
    "    attn_weights = attn_weights.squeeze().cpu().numpy()\n",
    "\n",
    "    # Separate LOS and Mortality predictions' attention scores\n",
    "    los_attn_weights.append(attn_weights * los_pred.item())  # Weight by LOS\n",
    "    mortality_attn_weights.append(attn_weights * mort_pred.item())  # Weight by Mortality\n",
    "\n",
    "# Compute mean attention scores\n",
    "los_attn_weights = np.mean(np.array(los_attn_weights), axis=0)\n",
    "mortality_attn_weights = np.mean(np.array(mortality_attn_weights), axis=0)\n",
    "\n",
    "# Plot both attention weights\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(seq_len), los_attn_weights, marker=\"o\", linestyle=\"-\", label=\"LOS Attention\", color=\"blue\")\n",
    "plt.plot(range(seq_len), mortality_attn_weights, marker=\"o\", linestyle=\"-\", label=\"Mortality Attention\", color=\"green\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Weighted Attention Score\")\n",
    "plt.title(\"Comparison of Attention Weights for LOS vs. Mortality\")\n",
    "plt.savefig(f'plots/tensor/02_attention_weight/{file_name}_los_vs_mortality.png')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6980c-403c-4607-a7a0-69c09d5d5965",
   "metadata": {},
   "source": [
    "# Training Performance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fdca8c-6ccd-4add-ba20-5d884052b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure train_losses and val_losses are aligned\n",
    "min_len = min(len(train_losses), len(val_losses))  # Align lengths if different\n",
    "train_losses = train_losses[:min_len]\n",
    "val_losses = val_losses[:min_len]\n",
    "\n",
    "# Identify the best epoch where early stopping occurred\n",
    "best_epoch = min_len - patience_counter  # patience_counter tracks epochs without improvement\n",
    "\n",
    "# Plot Training Loss\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))  # Initialize plot with size\n",
    "line1 = ax1.plot(range(1, min_len + 1), train_losses, label='Training Loss', color='b')\n",
    "ax1.set_xlabel('Epochs')  # Label for X-axis\n",
    "ax1.set_ylabel('Training Loss', color='b')  # Label for Y-axis on the left\n",
    "ax1.tick_params(axis='y', labelcolor='b')  # Left Y-axis tick color\n",
    "ax1.grid(visible=True, linestyle='--', alpha=0.6)  # Add grid for clarity\n",
    "\n",
    "# Plot Validation Loss on a Secondary Y-Axis\n",
    "ax2 = ax1.twinx()  # Create twin axes for validation loss\n",
    "line2 = ax2.plot(range(1, min_len + 1), val_losses, label='Validation Loss', color='orange')\n",
    "ax2.set_ylabel('Validation Loss', color='orange')  # Label for Y-axis on the right\n",
    "ax2.tick_params(axis='y', labelcolor='orange')  # Right Y-axis tick color\n",
    "\n",
    "# Highlight Early Stopping Point\n",
    "line3 = ax1.axvline(best_epoch, color='r', linestyle='--', label='Early Stopping Point')\n",
    "\n",
    "# Combine Legends from Both Axes\n",
    "lines = line1 + line2 + [line3]  # Combine lines from both Y-axes\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='upper right')  # Display legend\n",
    "\n",
    "# Add Title and Final Touches\n",
    "plt.title('Training and Validation Loss Over Epochs with Early Stopping')\n",
    "fig.tight_layout()  # Adjust spacing to prevent overlap\n",
    "plt.savefig(f'plots/tensor/01_train_vall_loss/{file_name}_train_val_over_epoch.png')\n",
    "# Display the Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb9fd3-ce89-49cf-9e88-d1b8f1682c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Ensure external_tensor and external_los_label are PyTorch tensors\n",
    "if isinstance(external_tensor, np.ndarray):\n",
    "    external_tensor = torch.tensor(external_tensor, dtype=torch.float32)\n",
    "if isinstance(external_los_label, np.ndarray):\n",
    "    external_los_label = torch.tensor(external_los_label, dtype=torch.float32)\n",
    "\n",
    "# Print shapes to debug\n",
    "print(f\"external_tensor shape: {external_tensor.shape}\")\n",
    "print(f\"external_los_label shape: {external_los_label.shape}\")\n",
    "\n",
    "# Check if dimensions match\n",
    "if external_tensor.shape[0] != external_los_label.shape[0]:\n",
    "    raise ValueError(f\"Mismatch: external_tensor has {external_tensor.shape[0]} rows, \"\n",
    "                     f\"but external_los_label has {external_los_label.shape[0]} rows.\")\n",
    "\n",
    "# Create DataLoader for External Validation Set\n",
    "external_dataset = TensorDataset(external_tensor, external_los_label)\n",
    "external_loader = DataLoader(external_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "print(\"External DataLoader created successfully!\")\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Ensure external_tensor and external_los_label are PyTorch tensors\n",
    "if isinstance(external_tensor, np.ndarray):\n",
    "    external_tensor = torch.tensor(external_tensor, dtype=torch.float32)\n",
    "if isinstance(external_los_label, np.ndarray):\n",
    "    external_los_label = torch.tensor(external_los_label, dtype=torch.float32)\n",
    "\n",
    "# Reshape external_los_label to match the batch size\n",
    "external_los_label = external_los_label.view(-1)  # Flatten to (234720,)\n",
    "\n",
    "# Print shapes to debug\n",
    "print(f\"Final: external_tensor shape: {external_tensor.shape}\")  # (234720, 345, 4)\n",
    "print(f\"Final: external_los_label shape: {external_los_label.shape}\")  # (234720,)\n",
    "\n",
    "# Create DataLoader for External Validation Set\n",
    "external_dataset = TensorDataset(external_tensor, external_los_label)\n",
    "external_loader = DataLoader(external_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "print(\"External DataLoader created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c902be-ef4b-4d9d-a26d-ab4d1022ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Create DataLoader for Test Set\n",
    "test_dataset = TensorDataset(test_tensor, test_los_label)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "# Create DataLoader for External Validation Set\n",
    "external_dataset = TensorDataset(external_tensor, external_los_label)\n",
    "external_loader = DataLoader(external_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "# Move model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "y_test_preds, y_test_trues = [], []\n",
    "y_external_preds, y_external_trues = [], []\n",
    "\n",
    "# Run batch-wise inference for test set with progress bar\n",
    "logging.info(\"Running inference on the Test Set...\")\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in tqdm(test_loader, desc=\"Processing Test Set\", unit=\"batch\"):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        y_pred_batch, _, _ = model(X_batch)  # Get predictions\n",
    "\n",
    "        # Store predictions and labels\n",
    "        y_test_preds.append(y_pred_batch.cpu().numpy())\n",
    "        y_test_trues.append(y_batch.cpu().numpy())\n",
    "\n",
    "# Run batch-wise inference for external validation set with progress bar\n",
    "logging.info(\"Running inference on the External Validation Set...\")\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in tqdm(external_loader, desc=\"Processing External Set\", unit=\"batch\"):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        y_pred_batch, _, _ = model(X_batch)  # Get predictions\n",
    "\n",
    "        # Store predictions and labels\n",
    "        y_external_preds.append(y_pred_batch.cpu().numpy())\n",
    "        y_external_trues.append(y_batch.cpu().numpy())\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "y_test_true = np.concatenate(y_test_trues).squeeze()\n",
    "y_test_pred = np.concatenate(y_test_preds).squeeze()\n",
    "\n",
    "y_external_true = np.concatenate(y_external_trues).squeeze()\n",
    "y_external_pred = np.concatenate(y_external_preds).squeeze()\n",
    "\n",
    "# Calculate metrics for the Test Set\n",
    "logging.info(\"Calculating metrics for the Test Set...\")\n",
    "test_mse = mean_squared_error(y_test_true, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test_true, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test_true, y_test_pred) * 100  # R² in percentage\n",
    "\n",
    "# Calculate metrics for the External Validation Set\n",
    "logging.info(\"Calculating metrics for the External Validation Set...\")\n",
    "external_mse = mean_squared_error(y_external_true, y_external_pred)\n",
    "external_mae = mean_absolute_error(y_external_true, y_external_pred)\n",
    "external_rmse = np.sqrt(external_mse)\n",
    "external_r2 = r2_score(y_external_true, y_external_pred) * 100  # R² in percentage\n",
    "\n",
    "# Print results with progress messages\n",
    "logging.info(\"Final Results:\")\n",
    "logging.info(f\"Test Set - MSE: {test_mse:.2f}, MAE: {test_mae:.2f}, RMSE: {test_rmse:.2f}, R2: {test_r2:.2f}%\")\n",
    "logging.info(f\"External Validation - MSE: {external_mse:.2f}, MAE: {external_mae:.2f}, RMSE: {external_rmse:.2f}, R2: {external_r2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6439d3a4-6ed0-4de6-88ef-f445d9605e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging info\n",
    "logging.info(\"Calculating metrics for the Test Set...\")\n",
    "\n",
    "# Initialize error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [test_mse, test_mae, test_rmse]\n",
    "\n",
    "# Try to calculate MSLE\n",
    "try:\n",
    "    test_msle = mean_squared_log_error(y_test_true, y_test_pred)\n",
    "    error_metrics.append('MSLE')\n",
    "    values.append(test_msle)\n",
    "except ValueError:\n",
    "    test_msle = None  # Handle cases where MSLE cannot be calculated\n",
    "\n",
    "# Plot error metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics (Test Set)')\n",
    "\n",
    "# Annotate values on top of the bars\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "plt.savefig(f'plots/tensor/03_metrics/{file_name}_test_set_metrics.png')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the test set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if test_r2 >= 0:\n",
    "    plt.pie([test_r2, 100 - test_r2], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Test Set Explained Variance by R-squared (R2)')\n",
    "plt.savefig(f'plots/tensor/03_metrics/{file_name}_test_set_r2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80bb59d-cb92-468c-b0a7-bc98893e5a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging info\n",
    "logging.info(\"Calculating metrics for the Test Set...\")\n",
    "\n",
    "# Initialize error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [external_mse, external_mae, external_rmse]\n",
    "\n",
    "# Try to calculate MSLE\n",
    "try:\n",
    "    external_msle = mean_squared_log_error(y_external_true, y_external_pred)\n",
    "    error_metrics.append('MSLE')\n",
    "    values.append(test_msle)\n",
    "except ValueError:\n",
    "    external_msle = None  # Handle cases where MSLE cannot be calculated\n",
    "\n",
    "# Plot error metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics (Test Set)')\n",
    "\n",
    "# Annotate values on top of the bars\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "plt.savefig(f'plots/tensor/03_metrics/{file_name}_external_set_metrics.png')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the test set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if test_r2 >= 0:\n",
    "    plt.pie([test_r2, 100 - test_r2], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Test Set Explained Variance by R-squared (R2)')\n",
    "plt.savefig(f'plots/tensor/03_metrics/{file_name}_external_set_r2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6bdac6-33ab-4d84-9b5a-7ea4d73f6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test_true, y_test_pred, color='blue', label='Prediction')\n",
    "\n",
    "# Line for Perfect Prediction\n",
    "perfect_line = np.linspace(y_test_true.min(), y_test_true.max(), 100)\n",
    "plt.plot(perfect_line, perfect_line, color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Labels, legend, and grid\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Predicted LOS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Predicted vs. True LOS (Test Set)')\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "plt.savefig(f\"plots/tensor/04_prediction_plot/01_true_vs_pred/{file_name}_test_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# External Validation Set Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_external_true, y_external_pred, color='blue', label='Prediction')\n",
    "\n",
    "# Line for Perfect Prediction (y = x)\n",
    "perfect_line_ext = np.linspace(y_external_true.min(), y_external_true.max(), 100)\n",
    "plt.plot(perfect_line_ext, perfect_line_ext, color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Labels, legend, and grid\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Predicted LOS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Predicted vs. True LOS (External Validation Set)')\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "plt.savefig(f\"plots/tensor/04_prediction_plot/01_true_vs_pred/{file_name}_external_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a189cc5-8a67-4ab4-a266-66d590653716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_test to a 1D numpy array\n",
    "#y_test_true = y_test_true.numpy().flatten()\n",
    "\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test_true - y_test_pred\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test_true, residuals, color='blue', alpha=0.5, label=\"Residuals\")\n",
    "plt.axhline(y=0, color='red', linestyle='--', label=\"Zero Line\")\n",
    "plt.axhline(y=test_mae, color='green', linestyle='--', label=f\"MAE = {test_mae:.2f}\")\n",
    "plt.axhline(y=-test_mae, color='green', linestyle='--')\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Residuals (True - Predicted)')\n",
    "plt.title('Residuals Plot with MAE Bounds')\n",
    "plt.grid(True)\n",
    "\n",
    "# Place the legend outside of the plot\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "plt.savefig(f\"plots/tensor/04_prediction_plot/02_residuals/{file_name}_residuals_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7152240d-79d9-47f9-9881-bb6361213b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
