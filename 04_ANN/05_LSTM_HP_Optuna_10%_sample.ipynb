{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6754cd06-f12e-4548-b461-5bf2d8ba7bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import h5py # Save - Load 3D tensor\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# display Matplotlib plots directly within the notebook interface\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99fbcd6-5e7e-480e-8cd7-5261b46ef4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"logging.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500c05d2-855b-4867-a77c-677a0ddacc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'hierarchical_attempt_II'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc0e25c-c96d-41c1-a831-67c7ef1c2b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 11:42:34,451 - INFO - Loading...\n",
      "2025-02-22 11:42:35,466 - INFO - Train: (122496, 345, 4), Los Label: (122496, 1), Mortality Label: (122496, 1)\n",
      "2025-02-22 11:42:35,467 - INFO - Validate: (15312, 345, 4), Los Label: (15312, 1), Mortality Label: (15312, 1)\n",
      "2025-02-22 11:42:35,468 - INFO - Test: (15312, 345, 4), Los Label: (15312, 1), Mortality Label: (15312, 1)\n",
      "2025-02-22 11:42:35,470 - INFO - External: (234720, 345, 4), Los Label: (234720, 1), Mortality Label: (234720, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load tensors from the HDF5 file\n",
    "load_path = 'CSV/exports/tensors/o4_3D_four_dataframe_hierarchical.h5'\n",
    "\n",
    "logging.info(f\"Loading...\")\n",
    "with h5py.File(load_path, 'r') as hf:\n",
    "    train_tensor = hf['train_tensor'][:]\n",
    "    validate_tensor = hf['validate_tensor'][:]\n",
    "    test_tensor = hf['test_tensor'][:]\n",
    "    external_tensor = hf['external_tensor'][:]\n",
    "    # los\n",
    "    train_los_label = hf['train_los_label'][:]\n",
    "    validate_los_label = hf['validate_los_label'][:]\n",
    "    test_los_label = hf['test_los_label'][:]\n",
    "    external_los_label = hf['external_los_label'][:]\n",
    "    # mortality\n",
    "    train_mortality_label = hf['train_mortality_label'][:]\n",
    "    validate_mortality_label = hf['validate_mortality_label'][:]\n",
    "    test_mortality_label = hf['test_mortality_label'][:]\n",
    "    external_mortality_label = hf['external_mortality_label'][:]\n",
    "\n",
    "logging.info(f\"Train: {train_tensor.shape}, Los Label: {train_los_label.shape}, Mortality Label: {train_mortality_label.shape}\")\n",
    "logging.info(f\"Validate: {validate_tensor.shape}, Los Label: {validate_los_label.shape}, Mortality Label: {validate_mortality_label.shape}\")\n",
    "logging.info(f\"Test: {test_tensor.shape}, Los Label: {test_los_label.shape}, Mortality Label: {test_mortality_label.shape}\")\n",
    "logging.info(f\"External: {external_tensor.shape}, Los Label: {external_los_label.shape}, Mortality Label: {external_mortality_label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b108afd5-72de-4f89-bf28-d167de796761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttentionMultiTask(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_layers, dropout):\n",
    "        super(LSTMAttentionMultiTask, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define LSTM layers dynamically\n",
    "        self.lstms = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            input_dim = input_size if i == 0 else hidden_sizes[i-1]\n",
    "            hidden_dim = hidden_sizes[i]\n",
    "\n",
    "            if num_layers > 1:\n",
    "                self.lstms.append(nn.LSTM(input_dim, hidden_dim, batch_first=True, dropout=dropout))\n",
    "            else:\n",
    "                self.lstms.append(nn.LSTM(input_dim, hidden_dim, batch_first=True))  # No dropout when num_layers=1\n",
    "\n",
    "        # Attention Layer\n",
    "        self.attn = nn.Linear(hidden_sizes[-1], 1)\n",
    "\n",
    "        # Fully Connected Layers for Multi-task Learning\n",
    "        self.fc_los = nn.Linear(hidden_sizes[-1], 1)  # Regression Output (LOS)\n",
    "        self.fc_mortality = nn.Linear(hidden_sizes[-1], 1)  # Classification Output (Mortality)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        out = x\n",
    "\n",
    "        # Initialize hidden and cell states dynamically for each LSTM layer\n",
    "        hidden_states = [(torch.zeros(1, batch_size, h).to(x.device), torch.zeros(1, batch_size, h).to(x.device)) \n",
    "                         for h in self.hidden_sizes]\n",
    "\n",
    "        for i, lstm in enumerate(self.lstms):\n",
    "            out, hidden_states[i] = lstm(out, hidden_states[i])\n",
    "\n",
    "        # Attention Mechanism\n",
    "        attn_weights = torch.tanh(self.attn(out))\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        context_vector = torch.sum(attn_weights * out, dim=1)\n",
    "\n",
    "        # Multi-task outputs\n",
    "        los_output = self.fc_los(context_vector)  # Regression\n",
    "        mortality_output = torch.sigmoid(self.fc_mortality(context_vector))  # Classification\n",
    "\n",
    "        return los_output, mortality_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "307b5a5b-a401-4d04-b786-bf5a72afa70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train = torch.tensor(train_tensor, dtype=torch.float32)\n",
    "y_train = torch.tensor(train_los_label, dtype=torch.float32)\n",
    "\n",
    "X_validate = torch.tensor(validate_tensor, dtype=torch.float32)\n",
    "y_validate = torch.tensor(validate_los_label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9fb570a-ae95-4a47-aae1-106d6dfb35a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 11:43:04,643 - INFO - Loading tensors...\n",
      "2025-02-22 11:43:05,073 - INFO - Load ends...\n"
     ]
    }
   ],
   "source": [
    "# Load tensors from the HDF5 file\n",
    "load_path = 'CSV/exports/tensors/o4_3D_four_dataframe_hierarchical.h5'\n",
    "logging.info(\"Loading tensors...\")\n",
    "with h5py.File(load_path, 'r') as hf:\n",
    "    train_tensor = hf['train_tensor'][:]\n",
    "    train_los_label = hf['train_los_label'][:]\n",
    "    train_mortality_label = hf['train_mortality_label'][:]\n",
    "# Create a balanced 10% sample from the training data\n",
    "def create_stratified_sample(data, labels, sample_size=0.1):\n",
    "    # Find indices for each class\n",
    "    non_survivor_indices = np.where(labels == 1)[0]  # Non-survivors\n",
    "    survivor_indices = np.where(labels == 0)[0]      # Survivors\n",
    "\n",
    "    # Determine number of samples from each class\n",
    "    n_non_survivors = int(len(non_survivor_indices) * sample_size)\n",
    "    n_survivors = int(len(survivor_indices) * sample_size)\n",
    "\n",
    "    # Randomly select indices for a balanced dataset\n",
    "    sampled_non_survivors_indices = np.random.choice(non_survivor_indices, n_non_survivors, replace=False)\n",
    "    sampled_survivors_indices = np.random.choice(survivor_indices, n_survivors, replace=False)\n",
    "    sampled_indices = np.concatenate([sampled_non_survivors_indices, sampled_survivors_indices])\n",
    "\n",
    "    return sampled_indices\n",
    "\n",
    "# Usage of the function to sample data\n",
    "sampled_indices = create_stratified_sample(train_tensor, train_mortality_label)\n",
    "sampled_train_tensor = train_tensor[sampled_indices]\n",
    "sampled_train_los_label = train_los_label[sampled_indices]\n",
    "sampled_train_mortality_label = train_mortality_label[sampled_indices]\n",
    "logging.info(\"Load ends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfe17edb-26f5-4de5-adeb-22e2bf9d017d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-22 11:43:12,689] A new study created in memory with name: no-name-2aa8afbf-01a2-4a2b-a045-ebabeb782eae\n",
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [09:05<00:00,  1.42s/it, Loss=5.94]\n",
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [15:09<00:00,  2.37s/it, Loss=5.84]\n",
      "Epoch 3/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [15:48<00:00,  2.48s/it, Loss=5.83]\n",
      "[I 2025-02-22 12:23:18,792] Trial 0 finished with value: 5.8292386519379775 and parameters: {'num_layers': 2, 'hidden_size_0': 256, 'hidden_size_1': 256, 'dropout': 0.24931011617233784, 'lr': 0.0017358883046590327}. Best is trial 0 with value: 5.8292386519379775.\n",
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [03:35<00:00,  1.78it/s, Loss=5.96]\n",
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [03:32<00:00,  1.81it/s, Loss=5.86]\n",
      "Epoch 3/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [03:38<00:00,  1.75it/s, Loss=5.84]\n",
      "[I 2025-02-22 12:34:04,969] Trial 1 finished with value: 5.8398036483684965 and parameters: {'num_layers': 3, 'hidden_size_0': 128, 'hidden_size_1': 192, 'hidden_size_2': 192, 'dropout': 0.4265309812916264, 'lr': 0.0023066662107398017}. Best is trial 0 with value: 5.8292386519379775.\n",
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [10:01<00:00,  1.57s/it, Loss=5.94]\n",
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [11:51<00:00,  1.86s/it, Loss=5.84]\n",
      "Epoch 3/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [12:02<00:00,  1.89s/it, Loss=5.83]\n",
      "[I 2025-02-22 13:08:00,180] Trial 2 finished with value: 5.828604599203205 and parameters: {'num_layers': 1, 'hidden_size_0': 256, 'dropout': 0.34465148960819547, 'lr': 0.005210295271655594}. Best is trial 2 with value: 5.828604599203205.\n",
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [01:34<00:00,  4.04it/s, Loss=6.82]\n",
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [01:02<00:00,  6.17it/s, Loss=5.83]\n",
      "Epoch 3/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [01:01<00:00,  6.25it/s, Loss=5.82]\n",
      "[I 2025-02-22 13:11:38,315] Trial 3 finished with value: 5.8208330374784945 and parameters: {'num_layers': 3, 'hidden_size_0': 64, 'hidden_size_1': 128, 'hidden_size_2': 256, 'dropout': 0.4925482753045336, 'lr': 0.00024346406655313948}. Best is trial 3 with value: 5.8208330374784945.\n",
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [07:19<00:00,  1.15s/it, Loss=6.03]\n",
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [07:02<00:00,  1.10s/it, Loss=5.85]\n",
      "Epoch 3/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [07:03<00:00,  1.10s/it, Loss=5.83]\n",
      "[I 2025-02-22 13:33:03,923] Trial 4 finished with value: 5.829751380118626 and parameters: {'num_layers': 3, 'hidden_size_0': 192, 'hidden_size_1': 256, 'hidden_size_2': 192, 'dropout': 0.34531049131783115, 'lr': 0.0007371929169704221}. Best is trial 3 with value: 5.8208330374784945.\n",
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [01:05<00:00,  5.83it/s, Loss=5.93]\n",
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [01:50<00:00,  3.46it/s, Loss=5.84]\n",
      "Epoch 3/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [02:01<00:00,  3.15it/s, Loss=5.84]\n",
      "[I 2025-02-22 13:38:01,854] Trial 5 finished with value: 5.8363600642500595 and parameters: {'num_layers': 2, 'hidden_size_0': 64, 'hidden_size_1': 192, 'dropout': 0.009730106604545097, 'lr': 0.005468192378132976}. Best is trial 3 with value: 5.8208330374784945.\n",
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [02:44<00:00,  2.32it/s, Loss=6.31]\n",
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [03:33<00:00,  1.80it/s, Loss=5.83]\n",
      "Epoch 3/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [05:04<00:00,  1.26it/s, Loss=5.83]\n",
      "[I 2025-02-22 13:49:24,566] Trial 6 finished with value: 5.825353161784438 and parameters: {'num_layers': 1, 'hidden_size_0': 256, 'dropout': 0.48013836841322405, 'lr': 0.00028408927343101455}. Best is trial 3 with value: 5.8208330374784945.\n",
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [05:03<00:00,  1.26it/s, Loss=5.95]\n",
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [06:50<00:00,  1.07s/it, Loss=5.85]\n",
      "Epoch 3/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [06:45<00:00,  1.06s/it, Loss=5.84]\n",
      "[I 2025-02-22 14:08:05,175] Trial 7 finished with value: 5.840427101747797 and parameters: {'num_layers': 1, 'hidden_size_0': 192, 'dropout': 0.33300910686526325, 'lr': 0.0029973931563522815}. Best is trial 3 with value: 5.8208330374784945.\n",
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [00:28<00:00, 13.52it/s, Loss=7.17]\n",
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [00:32<00:00, 11.90it/s, Loss=5.82]\n",
      "Epoch 3/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [00:36<00:00, 10.35it/s, Loss=5.82]\n",
      "[I 2025-02-22 14:09:42,695] Trial 8 finished with value: 5.820837184592264 and parameters: {'num_layers': 1, 'hidden_size_0': 64, 'dropout': 0.4177993819897523, 'lr': 0.00031812493802993897}. Best is trial 3 with value: 5.8208330374784945.\n",
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [14:50<00:00,  2.33s/it, Loss=5.94]\n",
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [11:46<00:00,  1.84s/it, Loss=5.85]\n",
      "Epoch 3/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 383/383 [12:55<00:00,  2.02s/it, Loss=5.88]\n",
      "[I 2025-02-22 14:49:15,218] Trial 9 finished with value: 5.877786030034795 and parameters: {'num_layers': 3, 'hidden_size_0': 256, 'hidden_size_1': 256, 'hidden_size_2': 64, 'dropout': 0.3632828682849794, 'lr': 0.0017439887567245863}. Best is trial 3 with value: 5.8208330374784945.\n",
      "2025-02-22 14:49:15,219 - INFO - Best Hyperparameters: {'num_layers': 3, 'hidden_size_0': 64, 'hidden_size_1': 128, 'hidden_size_2': 256, 'dropout': 0.4925482753045336, 'lr': 0.00024346406655313948}\n"
     ]
    }
   ],
   "source": [
    "# Define the model class (modify according to your actual model implementation)\n",
    "class LSTMAttentionMultiTask(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_sizes[0], num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_sizes[0], 1)  # Output for LOS\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], 1)  # Output for mortality\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out_los = self.fc1(lstm_out[:, -1])\n",
    "        out_mortality = torch.sigmoid(self.fc2(lstm_out[:, -1]))\n",
    "        return out_los, out_mortality\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    hidden_sizes = [trial.suggest_int(f\"hidden_size_{i}\", 64, 256, step=64) for i in range(num_layers)]\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    \n",
    "    if num_layers == 1:\n",
    "        dropout = 0.0\n",
    "\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    model = LSTMAttentionMultiTask(input_size=4, hidden_sizes=hidden_sizes, num_layers=num_layers, dropout=dropout)\n",
    "    criterion_los = nn.MSELoss()\n",
    "    criterion_mortality = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Simulated train_loader (replace with actual DataLoader)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(sampled_train_tensor, dtype=torch.float32),\n",
    "                                            torch.tensor(sampled_train_los_label, dtype=torch.float32),\n",
    "                                            torch.tensor(sampled_train_mortality_label, dtype=torch.float32)),\n",
    "                              batch_size=32, shuffle=True)\n",
    "\n",
    "    # Training loop with progress bar\n",
    "    for epoch in range(3):  # Reduced number of epochs for simplicity\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{3}\", leave=True)  # Progress bar setup\n",
    "        for X_batch, y_los_batch, y_mort_batch in progress_bar:\n",
    "            pred_los, pred_mortality = model(X_batch)\n",
    "            loss_los = criterion_los(pred_los.squeeze(), y_los_batch.squeeze())\n",
    "            loss_mortality = criterion_mortality(pred_mortality.squeeze(), y_mort_batch.squeeze())\n",
    "            loss = loss_los + loss_mortality\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"Loss\": total_loss / len(train_loader)})  # Update progress bar with loss info\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)  # Reduced number of trials for simplicity\n",
    "\n",
    "# Log best hyperparameters\n",
    "best_params = study.best_params\n",
    "logging.info(f\"Best Hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caaf4680-c966-467a-a984-bd91aa62717e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 15:00:54,750 - INFO - Loading tensors...\n",
      "2025-02-22 15:00:55,067 - INFO - Train: (122496, 345, 4), Los Label: (122496, 1), Mortality Label: (122496, 1)\n",
      "Epoch 1/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3828/3828 [35:11<00:00,  1.81it/s, Loss=5.88]\n",
      "Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3828/3828 [35:14<00:00,  1.81it/s, Loss=5.43]\n",
      "Epoch 3/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3828/3828 [35:05<00:00,  1.82it/s, Loss=5.09]\n",
      "Epoch 4/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3828/3828 [35:08<00:00,  1.82it/s, Loss=4.55]\n",
      "Epoch 5/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3828/3828 [36:05<00:00,  1.77it/s, Loss=3.73]\n",
      "Epoch 6/10:   3%|███▌                                                                                                                           | 107/3828 [01:02<36:00,  1.72it/s, Loss=0.0954]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 88\u001b[0m\n\u001b[0;32m     86\u001b[0m loss_mortality \u001b[38;5;241m=\u001b[39m criterion_mortality(pred_mortality\u001b[38;5;241m.\u001b[39msqueeze(), y_mort_batch\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[0;32m     87\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_los \u001b[38;5;241m+\u001b[39m loss_mortality\n\u001b[1;32m---> 88\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     89\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     90\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load tensors from the HDF5 file\n",
    "load_path = 'CSV/exports/tensors/o4_3D_four_dataframe_hierarchical.h5'\n",
    "logging.info(\"Loading tensors...\")\n",
    "\n",
    "with h5py.File(load_path, 'r') as hf:\n",
    "    train_tensor = hf['train_tensor'][:]\n",
    "    train_los_label = hf['train_los_label'][:]\n",
    "    train_mortality_label = hf['train_mortality_label'][:]\n",
    "\n",
    "logging.info(f\"Train: {train_tensor.shape}, Los Label: {train_los_label.shape}, Mortality Label: {train_mortality_label.shape}\")\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "train_tensor = torch.tensor(train_tensor, dtype=torch.float32)\n",
    "train_los_label = torch.tensor(train_los_label, dtype=torch.float32)\n",
    "train_mortality_label = torch.tensor(train_mortality_label, dtype=torch.float32)\n",
    "\n",
    "# Setup the DataLoader for the training data\n",
    "train_dataset = TensorDataset(train_tensor, train_los_label, train_mortality_label)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "best_hidden_sizes = [best_params[f\"hidden_size_{i}\"] for i in range(best_params[\"num_layers\"])]\n",
    "best_dropout = best_params[\"dropout\"]\n",
    "best_lr = best_params[\"lr\"]\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LSTMAttentionMultiTask(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_layers, dropout):\n",
    "        super(LSTMAttentionMultiTask, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_layers = num_layers\n",
    "        self.lstms = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            input_dim = input_size if i == 0 else hidden_sizes[i-1]\n",
    "            hidden_dim = hidden_sizes[i]\n",
    "            if i < num_layers - 1:\n",
    "                self.lstms.append(nn.LSTM(input_dim, hidden_dim, batch_first=True, dropout=dropout))\n",
    "            else:\n",
    "                self.lstms.append(nn.LSTM(input_dim, hidden_dim, batch_first=True))\n",
    "\n",
    "        self.attn = nn.Linear(hidden_sizes[-1], 1)\n",
    "        self.fc_los = nn.Linear(hidden_sizes[-1], 1)\n",
    "        self.fc_mortality = nn.Linear(hidden_sizes[-1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        out = x\n",
    "        hidden_states = [(torch.zeros(1, batch_size, h).to(x.device), torch.zeros(1, batch_size, h).to(x.device)) for h in self.hidden_sizes]\n",
    "        for i, lstm in enumerate(self.lstms):\n",
    "            out, hidden_states[i] = lstm(out, hidden_states[i])\n",
    "        attn_weights = torch.tanh(self.attn(out))\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        context_vector = torch.sum(attn_weights * out, dim=1)\n",
    "        los_output = self.fc_los(context_vector)\n",
    "        mortality_output = torch.sigmoid(self.fc_mortality(context_vector))\n",
    "        return los_output, mortality_output\n",
    "\n",
    "\n",
    "# Instantiate and move the model to the appropriate device\n",
    "best_model = LSTMAttentionMultiTask(input_size=4, hidden_sizes=best_hidden_sizes, num_layers=best_params[\"num_layers\"], dropout=best_dropout).to(device)\n",
    "\n",
    "# Loss functions and optimizer\n",
    "criterion_los = nn.MSELoss()\n",
    "criterion_mortality = nn.BCELoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_lr)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    best_model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=True)\n",
    "    for X_batch, y_los_batch, y_mort_batch in progress_bar:\n",
    "        X_batch, y_los_batch, y_mort_batch = X_batch.to(device), y_los_batch.to(device), y_mort_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred_los, pred_mortality = best_model(X_batch)\n",
    "        loss_los = criterion_los(pred_los.squeeze(), y_los_batch.squeeze())\n",
    "        loss_mortality = criterion_mortality(pred_mortality.squeeze(), y_mort_batch.squeeze())\n",
    "        loss = loss_los + loss_mortality\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"Loss\": total_loss / len(train_loader)})\n",
    "\n",
    "logging.info(\"Training complete.\")\n",
    "torch.save(best_model.state_dict(), \"models/final_lstm_attention_model.pth\")\n",
    "logging.info(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c0dec-fc8d-4363-a6e3-2cbc740dfe3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f9278-8054-4650-a9d7-1f307128c7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88bb57c-4811-4ee8-b3e4-e31c0b008357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0916b0f-8d38-498a-8583-2f966c888870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adeb9f1-42a2-4490-b518-6b7d761e2688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa4b42-51d9-4554-a4bd-a3a87b3bebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_size = X_train.shape[2]  # Number of features\n",
    "hidden_sizes = [256, 128, 64]  # LSTM layers sizes\n",
    "\n",
    "# Instantiate model\n",
    "model = LSTMAttentionMultiTask(input_size, hidden_sizes)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Print model architecture\n",
    "logging.info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f5ebe-4eb6-4d41-b1a5-2e32707b97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Losses & Optimizer\n",
    "criterion_los = nn.MSELoss()  # Loss for LOS (Regression)\n",
    "criterion_mortality = nn.BCELoss()  # Loss for Mortality (Binary Classification)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# Training Setup\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "patience = 5\n",
    "min_delta = 0.001\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "early_stop = False\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = TensorDataset(torch.tensor(train_tensor, dtype=torch.float32), \n",
    "                              torch.tensor(train_los_label, dtype=torch.float32),\n",
    "                              torch.tensor(train_mortality_label, dtype=torch.float32))\n",
    "\n",
    "validate_dataset = TensorDataset(torch.tensor(validate_tensor, dtype=torch.float32), \n",
    "                                 torch.tensor(validate_los_label, dtype=torch.float32),\n",
    "                                 torch.tensor(validate_mortality_label, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e45444-8f07-4348-ae9e-8711c1448d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to CPU before saving\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Define the model save path\n",
    "model_save_path = f\"models/{file_name}_model.pth\"\n",
    "\n",
    "# Save the trained model's state dictionary\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# Confirm model saving\n",
    "print(f\"Model saved successfully at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457e8d1-c979-41d4-be7b-bc8258350291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step 1: Get Attention Scores for a Sample\"\"\"\n",
    "# Select a random sample from the validation set\n",
    "sample_idx = np.random.randint(len(validate_dataset))\n",
    "X_sample, _, _ = validate_dataset[sample_idx]  # Only take the input tensor\n",
    "X_sample = X_sample.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "# Get model output and attention weights\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, _, attn_weights = model(X_sample)  # Get attention scores\n",
    "\n",
    "# Convert attention weights to numpy\n",
    "attn_weights = attn_weights.squeeze().cpu().numpy()  # Shape: (seq_len, 1)\n",
    "seq_len = attn_weights.shape[0]\n",
    "\n",
    "# Plot attention weights\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(seq_len), attn_weights, marker=\"o\", linestyle=\"-\", label=\"Attention Score\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Attention Score\")\n",
    "plt.title(\"Attention Weights Over Time for a Single Patient\")\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/tensor/02_attention_weight/{file_name}_single_patient.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357eda1-f564-41c5-93b3-9f8a3324f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step 2: Compare Attention Across Multiple Patients\"\"\"\n",
    "num_samples = 5  # Number of patients to visualize\n",
    "all_attn_weights = []\n",
    "\n",
    "# Select multiple random samples\n",
    "random_indices = np.random.choice(len(validate_dataset), num_samples, replace=False)\n",
    "\n",
    "for idx in random_indices:\n",
    "    X_sample, _, _ = validate_dataset[idx]\n",
    "    X_sample = X_sample.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, _, attn_weights = model(X_sample)\n",
    "\n",
    "    attn_weights = attn_weights.squeeze().cpu().numpy()\n",
    "    all_attn_weights.append(attn_weights)\n",
    "\n",
    "# Convert to NumPy array and compute mean attention weights\n",
    "all_attn_weights = np.array(all_attn_weights)  # Shape: (num_samples, seq_len)\n",
    "mean_attn_weights = np.mean(all_attn_weights, axis=0)  # Average over patients\n",
    "\n",
    "# Plot Mean Attention Weights\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(seq_len), mean_attn_weights, marker=\"o\", linestyle=\"-\", color='red', label=\"Mean Attention Score\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Mean Attention Score\")\n",
    "plt.title(\"Average Attention Weights Over Multiple Patients\")\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/tensor/02_attention_weight/{file_name}_multiple_patients.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a44689-ed24-4c14-a88c-6371a5e4013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step 3: Visualize LOS vs. Mortality Attention Differences\"\"\"\n",
    "\n",
    "num_samples = 5  # Number of patients to visualize\n",
    "los_attn_weights = []\n",
    "mortality_attn_weights = []\n",
    "\n",
    "# Select multiple random samples\n",
    "random_indices = np.random.choice(len(validate_dataset), num_samples, replace=False)\n",
    "\n",
    "for idx in random_indices:\n",
    "    X_sample, _, _ = validate_dataset[idx]\n",
    "    X_sample = X_sample.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        los_pred, mort_pred, attn_weights = model(X_sample)\n",
    "\n",
    "    attn_weights = attn_weights.squeeze().cpu().numpy()\n",
    "\n",
    "    # Separate LOS and Mortality predictions' attention scores\n",
    "    los_attn_weights.append(attn_weights * los_pred.item())  # Weight by LOS\n",
    "    mortality_attn_weights.append(attn_weights * mort_pred.item())  # Weight by Mortality\n",
    "\n",
    "# Compute mean attention scores\n",
    "los_attn_weights = np.mean(np.array(los_attn_weights), axis=0)\n",
    "mortality_attn_weights = np.mean(np.array(mortality_attn_weights), axis=0)\n",
    "\n",
    "# Plot both attention weights\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(seq_len), los_attn_weights, marker=\"o\", linestyle=\"-\", label=\"LOS Attention\", color=\"blue\")\n",
    "plt.plot(range(seq_len), mortality_attn_weights, marker=\"o\", linestyle=\"-\", label=\"Mortality Attention\", color=\"green\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Weighted Attention Score\")\n",
    "plt.title(\"Comparison of Attention Weights for LOS vs. Mortality\")\n",
    "plt.savefig(f'plots/tensor/02_attention_weight/{file_name}_los_vs_mortality.png')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6980c-403c-4607-a7a0-69c09d5d5965",
   "metadata": {},
   "source": [
    "# Training Performance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fdca8c-6ccd-4add-ba20-5d884052b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure train_losses and val_losses are aligned\n",
    "min_len = min(len(train_losses), len(val_losses))  # Align lengths if different\n",
    "train_losses = train_losses[:min_len]\n",
    "val_losses = val_losses[:min_len]\n",
    "\n",
    "# Identify the best epoch where early stopping occurred\n",
    "best_epoch = min_len - patience_counter  # patience_counter tracks epochs without improvement\n",
    "\n",
    "# Plot Training Loss\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))  # Initialize plot with size\n",
    "line1 = ax1.plot(range(1, min_len + 1), train_losses, label='Training Loss', color='b')\n",
    "ax1.set_xlabel('Epochs')  # Label for X-axis\n",
    "ax1.set_ylabel('Training Loss', color='b')  # Label for Y-axis on the left\n",
    "ax1.tick_params(axis='y', labelcolor='b')  # Left Y-axis tick color\n",
    "ax1.grid(visible=True, linestyle='--', alpha=0.6)  # Add grid for clarity\n",
    "\n",
    "# Plot Validation Loss on a Secondary Y-Axis\n",
    "ax2 = ax1.twinx()  # Create twin axes for validation loss\n",
    "line2 = ax2.plot(range(1, min_len + 1), val_losses, label='Validation Loss', color='orange')\n",
    "ax2.set_ylabel('Validation Loss', color='orange')  # Label for Y-axis on the right\n",
    "ax2.tick_params(axis='y', labelcolor='orange')  # Right Y-axis tick color\n",
    "\n",
    "# Highlight Early Stopping Point\n",
    "line3 = ax1.axvline(best_epoch, color='r', linestyle='--', label='Early Stopping Point')\n",
    "\n",
    "# Combine Legends from Both Axes\n",
    "lines = line1 + line2 + [line3]  # Combine lines from both Y-axes\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='upper right')  # Display legend\n",
    "\n",
    "# Add Title and Final Touches\n",
    "plt.title('Training and Validation Loss Over Epochs with Early Stopping')\n",
    "fig.tight_layout()  # Adjust spacing to prevent overlap\n",
    "plt.savefig(f'plots/tensor/01_train_vall_loss/{file_name}_train_val_over_epoch.png')\n",
    "# Display the Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb9fd3-ce89-49cf-9e88-d1b8f1682c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Ensure external_tensor and external_los_label are PyTorch tensors\n",
    "if isinstance(external_tensor, np.ndarray):\n",
    "    external_tensor = torch.tensor(external_tensor, dtype=torch.float32)\n",
    "if isinstance(external_los_label, np.ndarray):\n",
    "    external_los_label = torch.tensor(external_los_label, dtype=torch.float32)\n",
    "\n",
    "# Print shapes to debug\n",
    "print(f\"external_tensor shape: {external_tensor.shape}\")\n",
    "print(f\"external_los_label shape: {external_los_label.shape}\")\n",
    "\n",
    "# Check if dimensions match\n",
    "if external_tensor.shape[0] != external_los_label.shape[0]:\n",
    "    raise ValueError(f\"Mismatch: external_tensor has {external_tensor.shape[0]} rows, \"\n",
    "                     f\"but external_los_label has {external_los_label.shape[0]} rows.\")\n",
    "\n",
    "# Create DataLoader for External Validation Set\n",
    "external_dataset = TensorDataset(external_tensor, external_los_label)\n",
    "external_loader = DataLoader(external_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "print(\"External DataLoader created successfully!\")\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Ensure external_tensor and external_los_label are PyTorch tensors\n",
    "if isinstance(external_tensor, np.ndarray):\n",
    "    external_tensor = torch.tensor(external_tensor, dtype=torch.float32)\n",
    "if isinstance(external_los_label, np.ndarray):\n",
    "    external_los_label = torch.tensor(external_los_label, dtype=torch.float32)\n",
    "\n",
    "# Reshape external_los_label to match the batch size\n",
    "external_los_label = external_los_label.view(-1)  # Flatten to (234720,)\n",
    "\n",
    "# Print shapes to debug\n",
    "print(f\"Final: external_tensor shape: {external_tensor.shape}\")  # (234720, 345, 4)\n",
    "print(f\"Final: external_los_label shape: {external_los_label.shape}\")  # (234720,)\n",
    "\n",
    "# Create DataLoader for External Validation Set\n",
    "external_dataset = TensorDataset(external_tensor, external_los_label)\n",
    "external_loader = DataLoader(external_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "print(\"External DataLoader created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c902be-ef4b-4d9d-a26d-ab4d1022ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Create DataLoader for Test Set\n",
    "test_dataset = TensorDataset(test_tensor, test_los_label)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "# Create DataLoader for External Validation Set\n",
    "external_dataset = TensorDataset(external_tensor, external_los_label)\n",
    "external_loader = DataLoader(external_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "# Move model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "y_test_preds, y_test_trues = [], []\n",
    "y_external_preds, y_external_trues = [], []\n",
    "\n",
    "# Run batch-wise inference for test set with progress bar\n",
    "logging.info(\"Running inference on the Test Set...\")\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in tqdm(test_loader, desc=\"Processing Test Set\", unit=\"batch\"):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        y_pred_batch, _, _ = model(X_batch)  # Get predictions\n",
    "\n",
    "        # Store predictions and labels\n",
    "        y_test_preds.append(y_pred_batch.cpu().numpy())\n",
    "        y_test_trues.append(y_batch.cpu().numpy())\n",
    "\n",
    "# Run batch-wise inference for external validation set with progress bar\n",
    "logging.info(\"Running inference on the External Validation Set...\")\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in tqdm(external_loader, desc=\"Processing External Set\", unit=\"batch\"):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        y_pred_batch, _, _ = model(X_batch)  # Get predictions\n",
    "\n",
    "        # Store predictions and labels\n",
    "        y_external_preds.append(y_pred_batch.cpu().numpy())\n",
    "        y_external_trues.append(y_batch.cpu().numpy())\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "y_test_true = np.concatenate(y_test_trues).squeeze()\n",
    "y_test_pred = np.concatenate(y_test_preds).squeeze()\n",
    "\n",
    "y_external_true = np.concatenate(y_external_trues).squeeze()\n",
    "y_external_pred = np.concatenate(y_external_preds).squeeze()\n",
    "\n",
    "# Calculate metrics for the Test Set\n",
    "logging.info(\"Calculating metrics for the Test Set...\")\n",
    "test_mse = mean_squared_error(y_test_true, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test_true, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test_true, y_test_pred) * 100  # R² in percentage\n",
    "\n",
    "# Calculate metrics for the External Validation Set\n",
    "logging.info(\"Calculating metrics for the External Validation Set...\")\n",
    "external_mse = mean_squared_error(y_external_true, y_external_pred)\n",
    "external_mae = mean_absolute_error(y_external_true, y_external_pred)\n",
    "external_rmse = np.sqrt(external_mse)\n",
    "external_r2 = r2_score(y_external_true, y_external_pred) * 100  # R² in percentage\n",
    "\n",
    "# Print results with progress messages\n",
    "logging.info(\"Final Results:\")\n",
    "logging.info(f\"Test Set - MSE: {test_mse:.2f}, MAE: {test_mae:.2f}, RMSE: {test_rmse:.2f}, R2: {test_r2:.2f}%\")\n",
    "logging.info(f\"External Validation - MSE: {external_mse:.2f}, MAE: {external_mae:.2f}, RMSE: {external_rmse:.2f}, R2: {external_r2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6439d3a4-6ed0-4de6-88ef-f445d9605e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging info\n",
    "logging.info(\"Calculating metrics for the Test Set...\")\n",
    "\n",
    "# Initialize error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [test_mse, test_mae, test_rmse]\n",
    "\n",
    "# Try to calculate MSLE\n",
    "try:\n",
    "    test_msle = mean_squared_log_error(y_test_true, y_test_pred)\n",
    "    error_metrics.append('MSLE')\n",
    "    values.append(test_msle)\n",
    "except ValueError:\n",
    "    test_msle = None  # Handle cases where MSLE cannot be calculated\n",
    "\n",
    "# Plot error metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics (Test Set)')\n",
    "\n",
    "# Annotate values on top of the bars\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "plt.savefig(f'plots/tensor/03_metrics/{file_name}_test_set_metrics.png')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the test set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if test_r2 >= 0:\n",
    "    plt.pie([test_r2, 100 - test_r2], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Test Set Explained Variance by R-squared (R2)')\n",
    "plt.savefig(f'plots/tensor/03_metrics/{file_name}_test_set_r2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80bb59d-cb92-468c-b0a7-bc98893e5a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging info\n",
    "logging.info(\"Calculating metrics for the Test Set...\")\n",
    "\n",
    "# Initialize error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [external_mse, external_mae, external_rmse]\n",
    "\n",
    "# Try to calculate MSLE\n",
    "try:\n",
    "    external_msle = mean_squared_log_error(y_external_true, y_external_pred)\n",
    "    error_metrics.append('MSLE')\n",
    "    values.append(test_msle)\n",
    "except ValueError:\n",
    "    external_msle = None  # Handle cases where MSLE cannot be calculated\n",
    "\n",
    "# Plot error metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics (Test Set)')\n",
    "\n",
    "# Annotate values on top of the bars\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "plt.savefig(f'plots/tensor/03_metrics/{file_name}_external_set_metrics.png')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the test set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if test_r2 >= 0:\n",
    "    plt.pie([test_r2, 100 - test_r2], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Test Set Explained Variance by R-squared (R2)')\n",
    "plt.savefig(f'plots/tensor/03_metrics/{file_name}_external_set_r2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6bdac6-33ab-4d84-9b5a-7ea4d73f6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test_true, y_test_pred, color='blue', label='Prediction')\n",
    "\n",
    "# Line for Perfect Prediction\n",
    "perfect_line = np.linspace(y_test_true.min(), y_test_true.max(), 100)\n",
    "plt.plot(perfect_line, perfect_line, color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Labels, legend, and grid\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Predicted LOS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Predicted vs. True LOS (Test Set)')\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "plt.savefig(f\"plots/tensor/04_prediction_plot/01_true_vs_pred/{file_name}_test_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# External Validation Set Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_external_true, y_external_pred, color='blue', label='Prediction')\n",
    "\n",
    "# Line for Perfect Prediction (y = x)\n",
    "perfect_line_ext = np.linspace(y_external_true.min(), y_external_true.max(), 100)\n",
    "plt.plot(perfect_line_ext, perfect_line_ext, color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Labels, legend, and grid\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Predicted LOS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Predicted vs. True LOS (External Validation Set)')\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "plt.savefig(f\"plots/tensor/04_prediction_plot/01_true_vs_pred/{file_name}_external_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a189cc5-8a67-4ab4-a266-66d590653716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_test to a 1D numpy array\n",
    "#y_test_true = y_test_true.numpy().flatten()\n",
    "\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test_true - y_test_pred\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test_true, residuals, color='blue', alpha=0.5, label=\"Residuals\")\n",
    "plt.axhline(y=0, color='red', linestyle='--', label=\"Zero Line\")\n",
    "plt.axhline(y=test_mae, color='green', linestyle='--', label=f\"MAE = {test_mae:.2f}\")\n",
    "plt.axhline(y=-test_mae, color='green', linestyle='--')\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Residuals (True - Predicted)')\n",
    "plt.title('Residuals Plot with MAE Bounds')\n",
    "plt.grid(True)\n",
    "\n",
    "# Place the legend outside of the plot\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "plt.savefig(f\"plots/tensor/04_prediction_plot/02_residuals/{file_name}_residuals_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
