{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754cd06-f12e-4548-b461-5bf2d8ba7bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import logging\n",
    "import h5py # Save - Load 3D tensor\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# display Matplotlib plots directly within the notebook interface\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99fbcd6-5e7e-480e-8cd7-5261b46ef4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"data_loading.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0e25c-c96d-41c1-a831-67c7ef1c2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensors from the HDF5 file\n",
    "load_path = 'CSV/exports/tensors/o1_3D_four_dataframe.h5'\n",
    "\n",
    "logging.info(f\"Loading...\")\n",
    "with h5py.File(load_path, 'r') as hf:\n",
    "    train_tensor = hf['train_tensor'][:]\n",
    "    validate_tensor = hf['validate_tensor'][:]\n",
    "    test_tensor = hf['test_tensor'][:]\n",
    "    external_tensor = hf['external_tensor'][:]\n",
    "    # los\n",
    "    train_los_label = hf['train_los_label'][:]\n",
    "    validate_los_label = hf['validate_los_label'][:]\n",
    "    test_los_label = hf['test_los_label'][:]\n",
    "    external_los_label = hf['external_los_label'][:]\n",
    "    # mortality\n",
    "    train_mortality_label = hf['train_mortality_label'][:]\n",
    "    validate_mortality_label = hf['validate_mortality_label'][:]\n",
    "    test_mortality_label = hf['test_mortality_label'][:]\n",
    "    external_mortality_label = hf['external_mortality_label'][:]\n",
    "\n",
    "logging.info(f\"Train: {train_tensor.shape}, Los Label: {train_los_label.shape}, Mortality Label: {train_mortality_label.shape}\")\n",
    "logging.info(f\"Validate: {validate_tensor.shape}, Los Label: {validate_los_label.shape}, Mortality Label: {validate_mortality_label.shape}\")\n",
    "logging.info(f\"Test: {test_tensor.shape}, Los Label: {test_los_label.shape}, Mortality Label: {test_mortality_label.shape}\")\n",
    "logging.info(f\"External: {external_tensor.shape}, Los Label: {external_los_label.shape}, Mortality Label: {external_mortality_label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b108afd5-72de-4f89-bf28-d167de796761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer for output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states (h0 and c0)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate the RNN\n",
    "        out, _ = self.rnn(x, (h0, c0))  # Output shape: (batch_size, time_steps, hidden_size)\n",
    "        \n",
    "        # Take the output of the last time step\n",
    "        out = out[:, -1, :]  # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)  # Shape: (batch_size, output_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b5a5b-a401-4d04-b786-bf5a72afa70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (example with your pre-loaded tensors)\n",
    "X_train = torch.tensor(train_tensor, dtype=torch.float32)\n",
    "y_train = torch.tensor(train_los_label, dtype=torch.float32)\n",
    "\n",
    "X_validate = torch.tensor(validate_tensor, dtype=torch.float32)\n",
    "y_validate = torch.tensor(validate_los_label, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "validate_dataset = TensorDataset(X_validate, y_validate)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa4b42-51d9-4554-a4bd-a3a87b3bebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_size = X_train.shape[2]  # Number of features\n",
    "hidden_size = 64  # Number of hidden units\n",
    "num_layers = 2    # Number of RNN layers\n",
    "output_size = 1   # Predicting LOS as a single output\n",
    "\n",
    "model = RNNModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca604b-ae28-4ea7-866d-80860176e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store losses for plotting\n",
    "train_losses = []  # List to store training losses\n",
    "val_losses = []    # List to store validation losses\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "min_delta = 0.001\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "early_stop = False\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "logging.info(f\"Training RNN for {epochs} epochs with batch size {batch_size}.\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if early_stop:\n",
    "        logging.info(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "        break\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch.unsqueeze(1).squeeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)  # Store training loss\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in validate_loader:\n",
    "            val_predictions = model(X_val_batch)\n",
    "            val_loss += criterion(val_predictions, y_val_batch.unsqueeze(1).squeeze(-1)).item()\n",
    "    val_loss /= len(validate_loader)\n",
    "    val_losses.append(val_loss)  # Store validation loss\n",
    "\n",
    "    # Early stopping logic\n",
    "    if best_val_loss - val_loss > min_delta:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            early_stop = True\n",
    "\n",
    "    # Logging progress\n",
    "    logging.info(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fdca8c-6ccd-4add-ba20-5d884052b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure train_losses and val_losses are aligned\n",
    "min_len = min(len(train_losses), len(val_losses))  # Align lengths if different\n",
    "train_losses = train_losses[:min_len]\n",
    "val_losses = val_losses[:min_len]\n",
    "\n",
    "# Identify the best epoch where early stopping occurred\n",
    "best_epoch = min_len - patience_counter  # patience_counter tracks epochs without improvement\n",
    "\n",
    "# Plot Training Loss\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))  # Initialize plot with size\n",
    "line1 = ax1.plot(range(1, min_len + 1), train_losses, label='Training Loss', color='b')\n",
    "ax1.set_xlabel('Epochs')  # Label for X-axis\n",
    "ax1.set_ylabel('Training Loss', color='b')  # Label for Y-axis on the left\n",
    "ax1.tick_params(axis='y', labelcolor='b')  # Left Y-axis tick color\n",
    "ax1.grid(visible=True, linestyle='--', alpha=0.6)  # Add grid for clarity\n",
    "\n",
    "# Plot Validation Loss on a Secondary Y-Axis\n",
    "ax2 = ax1.twinx()  # Create twin axes for validation loss\n",
    "line2 = ax2.plot(range(1, min_len + 1), val_losses, label='Validation Loss', color='orange')\n",
    "ax2.set_ylabel('Validation Loss', color='orange')  # Label for Y-axis on the right\n",
    "ax2.tick_params(axis='y', labelcolor='orange')  # Right Y-axis tick color\n",
    "\n",
    "# Highlight Early Stopping Point\n",
    "line3 = ax1.axvline(best_epoch, color='r', linestyle='--', label='Early Stopping Point')\n",
    "\n",
    "# Combine Legends from Both Axes\n",
    "lines = line1 + line2 + [line3]  # Combine lines from both Y-axes\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='upper right')  # Display legend\n",
    "\n",
    "# Add Title and Final Touches\n",
    "plt.title('Training and Validation Loss Over Epochs with Early Stopping')\n",
    "fig.tight_layout()  # Adjust spacing to prevent overlap\n",
    "\n",
    "# Display the Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdcda57-a41b-4e95-bfc6-1d140d918216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test phase\n",
    "model.eval()\n",
    "test_tensor = torch.tensor(test_tensor, dtype=torch.float32)\n",
    "test_los_label = torch.tensor(test_los_label, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(test_tensor)\n",
    "    test_loss = criterion(test_predictions, test_los_label.unsqueeze(1))\n",
    "    logging.info(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91841c52-1d22-4197-927b-1483ef7b54a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Ensure test and external datasets are PyTorch tensors\n",
    "logging.info(\"Converting datasets to PyTorch tensors...\")\n",
    "X_test_tensor = test_tensor if isinstance(test_tensor, torch.Tensor) else torch.tensor(test_tensor, dtype=torch.float32)\n",
    "y_test_tensor = test_los_label if isinstance(test_los_label, torch.Tensor) else torch.tensor(test_los_label, dtype=torch.float32)\n",
    "\n",
    "X_external_tensor = external_tensor if isinstance(external_tensor, torch.Tensor) else torch.tensor(external_tensor, dtype=torch.float32)\n",
    "y_external_tensor = external_los_label if isinstance(external_los_label, torch.Tensor) else torch.tensor(external_los_label, dtype=torch.float32)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "logging.info(\"Creating DataLoaders for Test and External Validation datasets...\")\n",
    "batch_size = 32  # Process data in smaller batches\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "external_dataset = TensorDataset(X_external_tensor, y_external_tensor)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "external_loader = DataLoader(external_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model evaluation\n",
    "logging.info(\"Starting model evaluation...\")\n",
    "model.eval()\n",
    "y_test_preds = []\n",
    "y_external_preds = []\n",
    "\n",
    "# Predictions for Test Set\n",
    "logging.info(\"Generating predictions for the Test Set...\")\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (X_batch, _) in enumerate(test_loader):\n",
    "        logging.info(f\"Processing Test Batch {batch_idx+1}/{len(test_loader)}\")\n",
    "        batch_preds = model(X_batch).squeeze()\n",
    "        y_test_preds.append(batch_preds.cpu().numpy())  # Store predictions\n",
    "\n",
    "# Predictions for External Validation Set\n",
    "logging.info(\"Generating predictions for the External Validation Set...\")\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (X_batch, _) in enumerate(external_loader):\n",
    "        logging.info(f\"Processing External Batch {batch_idx+1}/{len(external_loader)}\")\n",
    "        batch_preds = model(X_batch).squeeze()\n",
    "        y_external_preds.append(batch_preds.cpu().numpy())  # Store predictions\n",
    "\n",
    "# Combine all batch predictions\n",
    "logging.info(\"Combining all batch predictions...\")\n",
    "y_test_pred = np.concatenate(y_test_preds)\n",
    "y_external_pred = np.concatenate(y_external_preds)\n",
    "\n",
    "# Ground truth labels\n",
    "y_test_true = y_test_tensor.numpy()\n",
    "y_external_true = y_external_tensor.numpy()\n",
    "\n",
    "# Calculate metrics for the Test Set\n",
    "logging.info(\"Calculating metrics for the Test Set...\")\n",
    "test_mse = mean_squared_error(y_test_true, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test_true, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test_true, y_test_pred) * 100  # R² in percentage\n",
    "\n",
    "# Calculate metrics for the External Validation Set\n",
    "logging.info(\"Calculating metrics for the External Validation Set...\")\n",
    "external_mse = mean_squared_error(y_external_true, y_external_pred)\n",
    "external_mae = mean_absolute_error(y_external_true, y_external_pred)\n",
    "external_rmse = np.sqrt(external_mse)\n",
    "external_r2 = r2_score(y_external_true, y_external_pred) * 100  # R² in percentage\n",
    "\n",
    "# Print results\n",
    "logging.info(\"Final Results:\")\n",
    "print(f\"Test Set - MSE: {test_mse:.2f}, MAE: {test_mae:.2f}, RMSE: {test_rmse:.2f}, R2: {test_r2:.2f}%\")\n",
    "print(f\"External Validation - MSE: {external_mse:.2f}, MAE: {external_mae:.2f}, RMSE: {external_rmse:.2f}, R2: {external_r2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a4597-c72c-4e42-8f28-21d88ec9fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics calculation\n",
    "mse = mean_squared_error(y_test_true, y_test_pred)\n",
    "mae = mean_absolute_error(y_test_true, y_test_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_true, y_test_pred) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse:.4f}\")\n",
    "print(f\"Test Set MAE: {mae:.4f}\")\n",
    "print(f\"Test Set RMSE: {rmse:.4f}\")\n",
    "print(f\"Test Set R2: {r2:.4f}\")\n",
    "\n",
    "# Initialize error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [mse, mae, rmse]\n",
    "\n",
    "# Try to calculate MSLE\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test_true, y_test_pred)\n",
    "    print(f\"Test Set MSLE: {msle:.4f}\")\n",
    "    \n",
    "    # Add MSLE to the list of metrics if applicable\n",
    "    error_metrics.append('MSLE')\n",
    "    values.append(msle)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")\n",
    "\n",
    "# Plot error metrics (with or without MSLE)\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics')\n",
    "\n",
    "# Annotate values on top of the bars\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "#plt.savefig(f'plots/03_metrics/{file_name}_internal.png')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the test set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if r2 >= 0:\n",
    "    plt.pie([r2, 100 - r2], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Test Set Explained Variance by R-squared (R2)')\n",
    "#plt.savefig(f'plots/03_metrics/{file_name}_internal_R2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97e88da-dfc7-436a-be2c-1a2f41bb03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for external validation set\n",
    "mse_external = mean_squared_error(y_external_true, y_external_pred)\n",
    "mae_external = mean_absolute_error(y_external_true, y_external_pred)\n",
    "rmse_external = np.sqrt(mse_external)\n",
    "r2_external = r2_score(y_external_true, y_external_pred) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external:.4f}\")\n",
    "print(f\"External Validation Set MAE: {mae_external:.4f}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external:.4f}\")\n",
    "print(f\"External Validation Set R2: {r2_external:.4f}\")\n",
    "\n",
    "# Initialize error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [mse_external, mae_external, rmse_external]\n",
    "\n",
    "# Try to calculate MSLE\n",
    "try:\n",
    "    msle_external = mean_squared_log_error(y_external_true, y_external_pred)\n",
    "    print(f\"External Validation Set MSLE: {msle_external:.4f}\")\n",
    "    \n",
    "    # Add MSLE to the list of metrics if applicable\n",
    "    error_metrics.append('MSLE')\n",
    "    values.append(msle_external)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")\n",
    "\n",
    "# Plot error metrics (with or without MSLE)\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics')\n",
    "\n",
    "# Annotate values on top of the bars\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "#plt.savefig(f'plots/03_metrics/{file_name}_external.png')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the external validation set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if r2_external >= 0:\n",
    "    plt.pie([r2_external, 100 - r2_external], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Validation Set Explained Variance by R-squared (R2)')\n",
    "#plt.savefig(f'plots/03_metrics/{file_name}_external_R2.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
