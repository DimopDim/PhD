{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032a1ce8-5983-4e55-9e97-d95caa3b4aad",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b1d3ab-adbb-4e80-8118-96e78a4df776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import shap\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f325999-bfaa-40bc-9983-f75093b263ed",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f815e91a-e99b-45d8-861c-9eee517658b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We convert the data into float32 because PyTorch expects float 32 values\n",
    "- Compatibility with DL\n",
    "- Memory Efficiency\n",
    "- Prevent errors in training (backpropagation etc)\n",
    "\"\"\"\n",
    "# Define subfolder\n",
    "subfolder = \"o6_GAN/o02\"\n",
    "\n",
    "# Load CSV files into corresponding variables\n",
    "X_external = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/X_external.csv\").astype('float32')\n",
    "y_external = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/y_external.csv\").values.ravel().astype('float32')\n",
    "X_train = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/X_train.csv\").astype('float32')\n",
    "y_train = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/y_train.csv\").values.ravel().astype('float32')\n",
    "X_validate = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/X_validate.csv\").astype('float32')\n",
    "y_validate = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/y_validate.csv\").values.ravel().astype('float32')\n",
    "X_test = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/X_test.csv\").astype('float32')\n",
    "y_test = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/y_test.csv\").values.ravel().astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b9c234-969a-4262-9baa-dcc44e77d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All computations in PyTorch are performed by the use of tensors\n",
    "and not with pandas dataframes or NymPy arrays.\n",
    "\n",
    "X_train.values extract NumPy arrays from pandas dataframe\n",
    "and torch.tensor converts it to PyTorch tensor.\n",
    "\"\"\"\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train.values)\n",
    "y_train = torch.tensor(y_train) # this is already an NymPy array\n",
    "X_validate = torch.tensor(X_validate.values)\n",
    "y_validate = torch.tensor(y_validate) # this is already an NymPy array\n",
    "X_test = torch.tensor(X_test.values)\n",
    "y_test = torch.tensor(y_test) # this is already an NymPy array\n",
    "X_external = torch.tensor(X_external.values)\n",
    "y_external = torch.tensor(y_external) # this is already an NymPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518ce2f-934f-46fe-bcc7-adf2d86850df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Feed Forward ANN without HP\n",
    "## Two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadeadd1-df2f-4ae4-99c8-62f082d67a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Layers\n",
    "- Input 128 -> 64 -> 32 -> 1\n",
    "- Each layer use ReLU activation function.\n",
    "\"\"\"\n",
    "\n",
    "# ANN architecture\n",
    "class ANNModel(nn.Module):\n",
    "    # input_dim is the input features and are the same as the dataframe.\n",
    "    def __init__(self, input_dim): \n",
    "        super(ANNModel, self).__init__()\n",
    "        # Define layers. Full features as input and 128 outputs.\n",
    "        # Weight and biases are initialized automatically.\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        # Its techique that randomly set neurons to zero to avoid overfitting\n",
    "        # I must check it further. Propability 0.2 = 20%\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        # Next layer with 128 inputs and 64 outputs.\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # Its techique that randomly set neurons to zero to avoid overfitting\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        # Next layer with 64 inputs and 32 outputs.\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        # Output layer with 64 inputs 1 output.\n",
    "        self.output = nn.Linear(32, 1)\n",
    "\n",
    "# Feedforward Network\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) # Pass the x throught the first dense fc1 with relu function\n",
    "        x = self.dropout1(x) # turn off neurons to prevent overfitting\n",
    "        x = torch.relu(self.fc2(x)) # Pass the x throught the first dense fc2 with relu function\n",
    "        x = self.dropout2(x) # turn off neurons to prevent overfitting\n",
    "        x = torch.relu(self.fc3(x)) # Pass the x throught the first dense fc3 with relu function\n",
    "        x = self.output(x) # output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd8ca8-657d-4c62-8e81-428e4717d04e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Feed Forward ANN without HP\n",
    "## Three hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0746b18b-64fe-4a68-be00-cab1045c6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ANNModel, self).__init__()\n",
    "        # Increase the width of the first two layers and add more layers\n",
    "        self.fc1 = nn.Linear(input_dim, 320)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(320, 192)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(192, 128)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(128, 96)  # Additional layer\n",
    "        self.dropout4 = nn.Dropout(0.2)  # Additional dropout\n",
    "        self.output = nn.Linear(96, 1)  # Output layer remains the same\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))  # Pass through additional layer\n",
    "        x = self.dropout4(x)  # Apply dropout\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc3ef6-0e98-4931-95d3-a2a8130ec1f9",
   "metadata": {},
   "source": [
    "# HyperOpt Feed Forward ANN\n",
    "## Two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb7c154-aaed-4570-bb1c-47e6b13990ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [07:44<40:36, 58.02s/trial, best loss: 6.0211920738220215]"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the ANN model class\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim, layer1, layer2, dropout_rate):\n",
    "        super(ANNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, layer1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(layer1, layer2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(layer2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)  # No activation in the output layer for regression\n",
    "        return x\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    # Unpack parameters\n",
    "    layer1 = int(params['layer1'])\n",
    "    layer2 = int(params['layer2'])\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    learning_rate = params['learning_rate']\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ANNModel(input_dim=X_train.shape[1], layer1=layer1, layer2=layer2, dropout_rate=dropout_rate)\n",
    "    model.to(device)  # Move model to GPU if available\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 10  # Use a small number for quick evaluation\n",
    "    batch_size = 32\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[i:i + batch_size].to(device)\n",
    "            y_batch = y_train[i:i + batch_size].unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_validate.to(device))\n",
    "        val_loss = mean_squared_error(\n",
    "            y_validate.cpu().numpy(),\n",
    "            val_predictions.cpu().numpy()\n",
    "        )\n",
    "\n",
    "    # Return the loss as the optimization metric\n",
    "    return {'loss': val_loss, 'status': STATUS_OK}\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_space = {\n",
    "    'layer1': hp.quniform('layer1', 64, 256, 32),\n",
    "    'layer2': hp.quniform('layer2', 32, 128, 16),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.5),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -4, -2),  # log scale for small LR\n",
    "}\n",
    "\n",
    "# Initialize Trials object\n",
    "trials = Trials()\n",
    "\n",
    "# Perform hyperparameter search\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=param_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,  # Number of iterations\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best Hyperparameters:\", best)\n",
    "\n",
    "# Initialize the model with the best hyperparameters\n",
    "best_model = ANNModel(\n",
    "    input_dim=X_train.shape[1],\n",
    "    layer1=int(best['layer1']),\n",
    "    layer2=int(best['layer2']),\n",
    "    dropout_rate=best['dropout_rate']\n",
    ")\n",
    "best_model.to(device)\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d5640-14d6-46d1-9bef-e81e460e073c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Feed forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663de2e5-fc4a-4b7a-b02a-9911c8d0c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "\n",
    "# prepare model to take inputs\n",
    "input_dim = X_train.shape[1] # retrive input features\n",
    "model = ANNModel(input_dim) # creates the model\n",
    "\n",
    "# computes how far from the true values are the predictions\n",
    "criterion = nn.MSELoss() # loss function MSE\n",
    "\n",
    "# update the weights to minimize loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # set Adam optimizer | lr = learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe1918-0f8f-4359-8ac5-f5016706aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping parameters\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "min_delta = 0.001  # Minimum change in validation loss to qualify as an improvement\n",
    "best_val_loss = float('inf')  # Initialize to a very large value\n",
    "patience_counter = 0  # Counter for epochs without improvement\n",
    "\n",
    "# Training the model with early stopping\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "early_stop = False\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "for epoch in range(epochs):\n",
    "    if early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size].unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_validate)\n",
    "        val_loss = criterion(val_predictions, y_validate.view(-1, 1))\n",
    "\n",
    "    # Append losses for plotting\n",
    "    train_losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss.item())\n",
    "    \n",
    "    #print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Logging progress\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logging.info(f\"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Check for improvement\n",
    "    if best_val_loss - val_loss.item() > min_delta:\n",
    "        best_val_loss = val_loss.item()\n",
    "        patience_counter = 0  # Reset the counter\n",
    "    else:\n",
    "        patience_counter += 1  # Increment the counter\n",
    "        if patience_counter >= patience:\n",
    "            early_stop = True\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs with Early Stopping')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c60895-0a6d-4ecc-956d-b742f5cb8f27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test & External Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1922c8-3480-4b34-a4d7-6a75d149f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and external validation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test).squeeze().numpy()\n",
    "    y_external_pred = model(X_external).squeeze().numpy()\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, y_test_pred)*100\n",
    "\n",
    "external_mse = mean_squared_error(y_external, y_external_pred)\n",
    "external_mae = mean_absolute_error(y_external, y_external_pred)\n",
    "external_rmse = np.sqrt(external_mse)\n",
    "external_r2 = r2_score(y_external, y_external_pred)*100\n",
    "\n",
    "print(f\"Test Set - MSE: {test_mse:.2f}, MAE: {test_mae:.2f}, RMSE: {test_rmse:.2f}, R2: {test_r2:.2f}\")\n",
    "print(f\"External Validation - MSE: {external_mse:.2f}, MAE: {external_mae:.2f}, RMSE: {external_rmse:.2f}, R2: {external_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba7784-37f6-4126-b512-c3e9b1a83b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics calculation\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "mae = mean_absolute_error(y_test, y_test_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_test_pred) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse:.4f}\")\n",
    "print(f\"Test Set MAE: {mae:.4f}\")\n",
    "print(f\"Test Set RMSE: {rmse:.4f}\")\n",
    "print(f\"Test Set R2: {r2:.4f}\")\n",
    "\n",
    "# Initialize error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [mse, mae, rmse]\n",
    "\n",
    "# Try to calculate MSLE\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_test_pred)\n",
    "    print(f\"Test Set MSLE: {msle:.4f}\")\n",
    "    \n",
    "    # Add MSLE to the list of metrics if applicable\n",
    "    error_metrics.append('MSLE')\n",
    "    values.append(msle)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")\n",
    "\n",
    "# Plot error metrics (with or without MSLE)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the test set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if r2 >= 0:\n",
    "    plt.pie([r2, 100 - r2], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Test Set Explained Variance by R-squared (R2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1530c7e1-73f1-4cad-8ce3-3bb218993da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for external validation set\n",
    "mse_external = mean_squared_error(y_external, y_external_pred)\n",
    "mae_external = mean_absolute_error(y_external, y_external_pred)\n",
    "rmse_external = np.sqrt(mse_external)\n",
    "r2_external = r2_score(y_external, y_external_pred) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external:.4f}\")\n",
    "print(f\"External Validation Set MAE: {mae_external:.4f}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external:.4f}\")\n",
    "print(f\"External Validation Set R2: {r2_external:.4f}\")\n",
    "\n",
    "# Initialize error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [mse_external, mae_external, rmse_external]\n",
    "\n",
    "# Try to calculate MSLE\n",
    "try:\n",
    "    msle_external = mean_squared_log_error(y_external, y_external_pred)\n",
    "    print(f\"External Validation Set MSLE: {msle_external:.4f}\")\n",
    "    \n",
    "    # Add MSLE to the list of metrics if applicable\n",
    "    error_metrics.append('MSLE')\n",
    "    values.append(msle_external)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")\n",
    "\n",
    "# Plot error metrics (with or without MSLE)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the external validation set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if r2_external >= 0:\n",
    "    plt.pie([r2_external, 100 - r2_external], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Validation Set Explained Variance by R-squared (R2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891ac33-2a3d-4dbe-a740-249a72defeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_test_pred, color='blue', label='Prediction')\n",
    "\n",
    "# Line for Perfect Prediction\n",
    "perfect_line = np.linspace(y_test.min(), y_test.max(), 100)\n",
    "plt.plot(perfect_line, perfect_line, color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Labels, legend, and grid\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Predicted LOS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Predicted vs. True LOS (Test Set)')\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "#plt.savefig(\"plots/02_Prediction_Plot/02_true_vs_pred/57_true_vs_pred_test_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# External Validation Set Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_external, y_external_pred, color='blue', label='Prediction')\n",
    "\n",
    "# Line for Perfect Prediction (y = x)\n",
    "perfect_line_ext = np.linspace(y_external.min(), y_external.max(), 100)\n",
    "plt.plot(perfect_line_ext, perfect_line_ext, color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Labels, legend, and grid\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Predicted LOS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Predicted vs. True LOS (External Validation Set)')\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "#plt.savefig(\"plots/02_Prediction_Plot/02_true_vs_pred/57_true_vs_pred_external_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d76b1-d9af-4790-88d0-752b252b32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_test to a 1D numpy array\n",
    "y_test = y_test.numpy().flatten()\n",
    "\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, residuals, color='blue', alpha=0.5, label=\"Residuals\")\n",
    "plt.axhline(y=0, color='red', linestyle='--', label=\"Zero Line\")\n",
    "plt.axhline(y=mae, color='green', linestyle='--', label=f\"MAE = {mae:.2f}\")\n",
    "plt.axhline(y=-mae, color='green', linestyle='--')\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Residuals (True - Predicted)')\n",
    "plt.title('Residuals Plot with MAE Bounds')\n",
    "plt.grid(True)\n",
    "\n",
    "# Place the legend outside of the plot\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "#plt.savefig(\"plots/02_Prediction_Plot/01_residuals/57_residuals_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdcb3a1-b975-4325-b2af-eaa3539f2b67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4832b64-6291-4691-8919-ed8db53c19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file\n",
    "subfolder = \"o01_feed_forward.pth\"\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), f\"models/{subfolder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fc07dc-610f-43da-bbc3-3f1951d15037",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262423c6-8c12-492e-b6cd-ab78ec3d7b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file\n",
    "subfolder = \"o01_feed_forward.pth\"\n",
    "\n",
    "# Reinitialize the model architecture\n",
    "input_dim = X_test.shape[1]  # Ensure this matches the original input dimension\n",
    "model = ANNModel(input_dim)\n",
    "\n",
    "# Load the saved model state\n",
    "model.load_state_dict(torch.load(f\"models/{subfolder}\"))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\"\"\"\n",
    "After that I must run the block with layers.\n",
    "Be careful, the layers must be exaclty the same. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9418b819-e643-4f8e-9268-c6e1e45cbb78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d389220-2d54-4e61-9331-c3ef98a4c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tensor_X = torch.tensor(X, dtype=torch.float32)\n",
    "        predictions = model(tensor_X).numpy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0defa-f61d-4841-831e-a65f62e163d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensors to Pandas DataFrames\n",
    "X_sample_df = pd.DataFrame(X_sample_np, columns=[f\"Feature_{i}\" for i in range(X_sample_np.shape[1])])\n",
    "X_validate_df = pd.DataFrame(X_validate_np, columns=[f\"Feature_{i}\" for i in range(X_validate_np.shape[1])])\n",
    "\n",
    "# Use KernelExplainer with Pandas DataFrame\n",
    "explainer = shap.KernelExplainer(model_predict, X_sample_df)\n",
    "shap_values = explainer.shap_values(X_validate_df)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_validate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d6a10-d882-427f-bae1-eaa908c80ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
