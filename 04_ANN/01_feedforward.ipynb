{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032a1ce8-5983-4e55-9e97-d95caa3b4aad",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1d3ab-adbb-4e80-8118-96e78a4df776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Logging Configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "logger.info(\"Logging initialized. Starting script execution.\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import os\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "# display Matplotlib plots directly within the notebook interface\n",
    "%matplotlib inline \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  # neural network\n",
    "import torch.nn.functional as F  # Move data forward in function\n",
    "import torch.optim as optim\n",
    "import ray\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.air import session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f325999-bfaa-40bc-9983-f75093b263ed",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f815e91a-e99b-45d8-861c-9eee517658b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We convert the data into float32 because PyTorch expects float 32 values\n",
    "- Compatibility with DL\n",
    "- Memory Efficiency\n",
    "- Prevent errors in training (backpropagation etc)\n",
    "\"\"\"\n",
    "# Data Loading and Conversion to float32\n",
    "logger.info(\"Starting to load datasets...\")\n",
    "\n",
    "# Define subfolder\n",
    "subfolder = \"o6_GAN/o02\"\n",
    "\n",
    "try:\n",
    "    # Load CSV files and convert to float32 for compatibility\n",
    "    X_external = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/X_external.csv\").astype('float32')\n",
    "    y_external = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/y_external.csv\").values.ravel().astype('float32')\n",
    "    X_train = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/X_train.csv\").astype('float32')\n",
    "    y_train = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/y_train.csv\").values.ravel().astype('float32')\n",
    "    X_validate = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/X_validate.csv\").astype('float32')\n",
    "    y_validate = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/y_validate.csv\").values.ravel().astype('float32')\n",
    "    X_test = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/X_test.csv\").astype('float32')\n",
    "    y_test = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/y_test.csv\").values.ravel().astype('float32')\n",
    "    \n",
    "    logger.info(\"Datasets loaded and converted to float32 successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error occurred while loading datasets: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9c234-969a-4262-9baa-dcc44e77d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All computations in PyTorch are performed by the use of tensors\n",
    "and not with pandas dataframes or NymPy arrays.\n",
    "\n",
    "X_train.values extract NumPy arrays from pandas dataframe\n",
    "and torch.tensor converts it to PyTorch tensor.\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "logger.info(f\"X_validate shape: {X_validate.shape}, y_validate shape: {y_validate.shape}\")\n",
    "logger.info(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "logger.info(f\"X_external shape: {X_external.shape}, y_external shape: {y_external.shape}\")\n",
    "\n",
    "logger.info(\"Converting datasets to PyTorch tensors...\")\n",
    "\n",
    "try:\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train.values)\n",
    "    y_train = torch.tensor(y_train)  # Already a NumPy array\n",
    "    X_validate = torch.tensor(X_validate.values)\n",
    "    y_validate = torch.tensor(y_validate)  # Already a NumPy array\n",
    "    X_test = torch.tensor(X_test.values)\n",
    "    y_test = torch.tensor(y_test)  # Already a NumPy array\n",
    "    X_external = torch.tensor(X_external.values)\n",
    "    y_external = torch.tensor(y_external)  # Already a NumPy array\n",
    "\n",
    "    logger.info(\"Datasets successfully converted to PyTorch tensors.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error occurred during tensor conversion: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ace419-4bfc-420b-b83b-99f14a69dcb0",
   "metadata": {},
   "source": [
    "# SOS !!!\n",
    "\n",
    "## Define file name for save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114d54c-06b3-44a9-8187-b9981f97341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file name\n",
    "file_name = \"o06_FF_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518ce2f-934f-46fe-bcc7-adf2d86850df",
   "metadata": {},
   "source": [
    "# Feed Forward ANN without HP\n",
    "## Two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadeadd1-df2f-4ae4-99c8-62f082d67a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Layers\n",
    "- Input 128 -> 64 -> 32 -> 1\n",
    "- Each layer use ReLU activation function.\n",
    "\"\"\"\n",
    "\n",
    "# ANN architecture\n",
    "class ANNModel(nn.Module):\n",
    "    # input_dim is the input features and are the same as the dataframe.\n",
    "    def __init__(self, input_dim): \n",
    "        super(ANNModel, self).__init__()\n",
    "        # Define layers. Full features as input and 128 outputs.\n",
    "        # Weight and biases are initialized automatically.\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        # Its techique that randomly set neurons to zero to avoid overfitting\n",
    "        # I must check it further. Propability 0.2 = 20%\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        # Next layer with 128 inputs and 64 outputs.\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # Its techique that randomly set neurons to zero to avoid overfitting\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        # Next layer with 64 inputs and 32 outputs.\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        # Output layer with 64 inputs 1 output.\n",
    "        self.output = nn.Linear(32, 1)\n",
    "\n",
    "# Feedforward Network\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) # Pass the x throught the first dense fc1 with relu function\n",
    "        x = self.dropout1(x) # turn off neurons to prevent overfitting\n",
    "        x = torch.relu(self.fc2(x)) # Pass the x throught the first dense fc2 with relu function\n",
    "        x = self.dropout2(x) # turn off neurons to prevent overfitting\n",
    "        x = torch.relu(self.fc3(x)) # Pass the x throught the first dense fc3 with relu function\n",
    "        x = self.output(x) # output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd8ca8-657d-4c62-8e81-428e4717d04e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Feed Forward ANN without HP\n",
    "## Three hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0746b18b-64fe-4a68-be00-cab1045c6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ANNModel, self).__init__()\n",
    "        # Increase the width of the first two layers and add more layers\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(64, 32)  # Additional layer\n",
    "        self.dropout4 = nn.Dropout(0.2)  # Additional dropout\n",
    "        self.output = nn.Linear(32, 1)  # Output layer remains the same\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))  # Pass through additional layer\n",
    "        x = self.dropout4(x)  # Apply dropout\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d5640-14d6-46d1-9bef-e81e460e073c",
   "metadata": {},
   "source": [
    "# Train Feed forward model without HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663de2e5-fc4a-4b7a-b02a-9911c8d0c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "# prepare model to take inputs\n",
    "input_dim = X_train.shape[1] # retrive input features\n",
    "model = ANNModel(input_dim) # creates the model\n",
    "\n",
    "# computes how far from the true values are the predictions\n",
    "\n",
    "# criterion = nn.MSELoss() # loss function MSE\n",
    "# or\n",
    "# criterion = nn.L1Loss() # loss function MAE\n",
    "# or\n",
    "criterion = nn.SmoothL1Loss() # loss function Humber Loss\n",
    "\n",
    "# update the weights to minimize loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # set Adam optimizer | lr = learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe1918-0f8f-4359-8ac5-f5016706aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping parameters\n",
    "patience = 5  # The number of consecutive epochs allowed without significant improvement before stopping.\n",
    "min_delta = 0.001  # Minimum required improvement in validation loss to reset the patience counter.\n",
    "best_val_loss = float('inf')  # Track the best validation loss.\n",
    "patience_counter = 0  # Epochs counter without improvement.\n",
    "\n",
    "# Training the model with early stopping\n",
    "train_losses = [] # Store training loss for each epoch.\n",
    "val_losses = [] # Store validation loss for each epoch.\n",
    "early_stop = False # Early stop flag.\n",
    "\n",
    "epochs = 50 # Maximum number of iterations over the entire training dataset.\n",
    "batch_size = 16 # Number of training examples\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "validate_dataset = TensorDataset(X_validate, y_validate)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) # Shuffle take rows in order\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "logging.info(f\"Parameters: epochs={epochs}, batch_size={batch_size}\")\n",
    "logging.info(f\"Early stopping parameters: patience={patience}, min_delta={min_delta}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if early_stop:\n",
    "        logging.info(f\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in validate_loader:\n",
    "            val_predictions = model(X_val_batch)\n",
    "            val_loss += criterion(val_predictions, y_val_batch.unsqueeze(1)).item()\n",
    "    val_loss /= len(validate_loader)\n",
    "\n",
    "    # Append losses for plotting\n",
    "    train_losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    #print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Logging progress\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logging.info(f\"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Check for improvement\n",
    "    if best_val_loss - val_loss > min_delta:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # Reset the counter\n",
    "    else:\n",
    "        patience_counter += 1  # Increment the counter\n",
    "        if patience_counter >= patience:\n",
    "            early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34696a-aeb2-4623-beb6-55110e162163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Loss\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "line1 = ax1.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='b')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Training Loss', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "ax1.grid(visible=True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot Validation Loss on a Secondary Y-Axis\n",
    "ax2 = ax1.twinx()\n",
    "line2 = ax2.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', color='orange')\n",
    "ax2.set_ylabel('Validation Loss', color='orange')\n",
    "ax2.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "# Add Early Stopping Point\n",
    "line3 = ax1.axvline(len(train_losses), color='r', linestyle='--', label='Early Stopping Point')\n",
    "\n",
    "# Combine legends from both axes\n",
    "lines = line1 + line2 + [line3]\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='upper right')\n",
    "\n",
    "# Add Title\n",
    "plt.title('Training and Validation Loss Over Epochs with Early Stopping')\n",
    "fig.tight_layout()\n",
    "plt.savefig(f'plots/01_train_vall_loss/{file_name}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc3ef6-0e98-4931-95d3-a2a8130ec1f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HyperOpt Feed Forward ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb7c154-aaed-4570-bb1c-47e6b13990ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    # Extract hyperparameters\n",
    "    n_layers = int(params['n_layers'])  # Convert to integer\n",
    "    hidden_size = int(params['hidden_size'])  # Convert to integer\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    lr = params['lr']\n",
    "    batch_size = int(params['batch_size'])  # Convert to integer\n",
    "    optimizer_name = params['optimizer']\n",
    "    activation_name = params['activation']\n",
    "    alpha = params['alpha']\n",
    "    lambda_ = params['lambda']\n",
    "\n",
    "    # Dynamically build the model\n",
    "    class ANNModel(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(ANNModel, self).__init__()\n",
    "            layers = []\n",
    "            in_features = input_dim\n",
    "            \n",
    "            # Create hidden layers dynamically\n",
    "            for _ in range(n_layers):\n",
    "                layers.append(nn.Linear(in_features, hidden_size))\n",
    "                if activation_name == \"ReLU\":\n",
    "                    layers.append(nn.ReLU())\n",
    "                elif activation_name == \"LeakyReLU\":\n",
    "                    layers.append(nn.LeakyReLU())\n",
    "                elif activation_name == \"Tanh\":\n",
    "                    layers.append(nn.Tanh())\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "                in_features = hidden_size\n",
    "            \n",
    "            # Output layer\n",
    "            layers.append(nn.Linear(hidden_size, 1))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x)\n",
    "    \n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = ANNModel(X_train.shape[1])\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=alpha)\n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=alpha)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=alpha)\n",
    "\n",
    "    # DataLoader\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    validate_dataset = TensorDataset(X_validate, y_validate)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validate_loader = DataLoader(validate_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for epoch in range(5):  # Reduced epochs for faster tuning\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in validate_loader:\n",
    "            val_predictions = model(X_val_batch)\n",
    "            val_loss += criterion(val_predictions, y_val_batch.unsqueeze(1)).item()\n",
    "    val_loss /= len(validate_loader)\n",
    "\n",
    "    return {'loss': val_loss, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "search_space = {\n",
    "    'n_layers': hp.quniform('n_layers', 2, 10, 1),\n",
    "    'hidden_size': hp.quniform('hidden_size', 32, 256, 32),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.5),\n",
    "    'lr': hp.loguniform('lr', np.log(1e-4), np.log(1e-2)),\n",
    "    'batch_size': hp.quniform('batch_size', 16, 128, 16),\n",
    "    'optimizer': hp.choice('optimizer', ['Adam', 'SGD', 'RMSprop']),\n",
    "    'activation': hp.choice('activation', ['ReLU', 'LeakyReLU', 'Tanh']),\n",
    "    'alpha': hp.loguniform('alpha', np.log(1e-6), np.log(1e-1)),\n",
    "    'lambda': hp.loguniform('lambda', np.log(1e-6), np.log(1e-1))\n",
    "}\n",
    "\n",
    "# Run HyperOpt\n",
    "trials = Trials()\n",
    "best_params = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e451cb55-2a08-4a45-9bb4-afb13ae6b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best hyperparameters\n",
    "best_n_layers = int(best_params['n_layers'])\n",
    "best_hidden_size = int(best_params['hidden_size'])\n",
    "best_dropout_rate = best_params['dropout_rate']\n",
    "best_lr = best_params['lr']\n",
    "best_batch_size = int(best_params['batch_size'])\n",
    "best_optimizer_name = ['Adam', 'SGD', 'RMSprop'][best_params['optimizer']]  # Mapping choice index to value\n",
    "best_activation_name = ['ReLU', 'LeakyReLU', 'Tanh'][best_params['activation']]\n",
    "best_alpha = best_params['alpha']\n",
    "best_lambda = best_params['lambda']\n",
    "\n",
    "# Define the model with the best hyperparameters\n",
    "class FinalANNModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FinalANNModel, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        \n",
    "        # Create hidden layers dynamically\n",
    "        for _ in range(best_n_layers):\n",
    "            layers.append(nn.Linear(in_features, best_hidden_size))\n",
    "            if best_activation_name == \"ReLU\":\n",
    "                layers.append(nn.ReLU())\n",
    "            elif best_activation_name == \"LeakyReLU\":\n",
    "                layers.append(nn.LeakyReLU())\n",
    "            elif best_activation_name == \"Tanh\":\n",
    "                layers.append(nn.Tanh())\n",
    "            layers.append(nn.Dropout(best_dropout_rate))\n",
    "            in_features = best_hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(best_hidden_size, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = FinalANNModel(X_train.shape[1])\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "if best_optimizer_name == \"Adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_lr, weight_decay=best_alpha)\n",
    "elif best_optimizer_name == \"SGD\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=best_lr, weight_decay=best_alpha)\n",
    "elif best_optimizer_name == \"RMSprop\":\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=best_lr, weight_decay=best_alpha)\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "validate_dataset = TensorDataset(X_validate, y_validate)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=best_batch_size)\n",
    "\n",
    "# Train the model\n",
    "epochs = 50  # Number of training epochs\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "early_stop = False\n",
    "patience = 5\n",
    "min_delta = 0.001\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in validate_loader:\n",
    "            val_predictions = model(X_val_batch)\n",
    "            val_loss += criterion(val_predictions, y_val_batch.unsqueeze(1)).item()\n",
    "    val_loss /= len(validate_loader)\n",
    "\n",
    "    # Save losses for plotting\n",
    "    train_losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    logging.info(f\"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if best_val_loss - val_loss > min_delta:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            early_stop = True\n",
    "\n",
    "# Evaluate the model on the test set and external validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test).squeeze().numpy()\n",
    "    y_external_pred = model(X_external).squeeze().numpy()\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, y_test_pred)*100\n",
    "\n",
    "external_mse = mean_squared_error(y_external, y_external_pred)\n",
    "external_mae = mean_absolute_error(y_external, y_external_pred)\n",
    "external_rmse = np.sqrt(external_mse)\n",
    "external_r2 = r2_score(y_external, y_external_pred)*100\n",
    "\n",
    "print(f\"Test Set - MSE: {test_mse:.2f}, MAE: {test_mae:.2f}, RMSE: {test_rmse:.2f}, R2: {test_r2:.2f}\")\n",
    "print(f\"External Validation - MSE: {external_mse:.2f}, MAE: {external_mae:.2f}, RMSE: {external_rmse:.2f}, R2: {external_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f8f3f6-dfd6-4a3f-8afa-9114eb62cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Loss\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))  # Increase width for the subplots as well\n",
    "line1 = ax1.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='b')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Training Loss', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "ax1.grid(visible=True, linestyle='--', alpha=0.6)  # Add grid to the primary axis\n",
    "\n",
    "# Plot Validation Loss on a Secondary Y-Axis\n",
    "ax2 = ax1.twinx()\n",
    "line2 = ax2.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', color='orange')\n",
    "ax2.set_ylabel('Validation Loss', color='orange')\n",
    "ax2.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "# Add Early Stopping Point\n",
    "line3 = ax1.axvline(len(train_losses), color='r', linestyle='--', label='Early Stopping Point')\n",
    "\n",
    "# Combine legends from both axes\n",
    "lines = line1 + line2 + [line3]\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='upper right')\n",
    "\n",
    "# Add Title\n",
    "plt.title('Training and Validation Loss Over Epochs with Early Stopping')\n",
    "fig.tight_layout()\n",
    "plt.savefig(f'plots/01_train_vall_loss/{file_name}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f58b8-77a1-4d7f-8c1a-a9f1956439d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Optuna HP\n",
    "## Test field (include alpha and lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527afc8-fced-4513-bd81-12168a41933e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 10)  # Number of hidden layers\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 256, step=32)  # Neurons in hidden layers\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5, step=0.1)  # Dropout probability\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)  # Learning rate\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 128, step=16)  # Batch size\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])  # Optimizer\n",
    "    activation_name = trial.suggest_categorical(\"activation\", [\"ReLU\", \"LeakyReLU\", \"Tanh\"])  # Activation function\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-6, 1e-1, log=True)  # L2 regularization term\n",
    "    lambda_ = trial.suggest_float(\"lambda\", 1e-6, 1e-1, log=True)  # L1 regularization term\n",
    "\n",
    "    # Define activation function\n",
    "    activation = {\n",
    "        \"ReLU\": nn.ReLU(),\n",
    "        \"LeakyReLU\": nn.LeakyReLU(),\n",
    "        \"Tanh\": nn.Tanh()\n",
    "    }[activation_name]\n",
    "\n",
    "    # Define the model\n",
    "    class ANNModel(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(ANNModel, self).__init__()\n",
    "            layers = []\n",
    "            layers.append(nn.Linear(input_dim, hidden_size))\n",
    "            layers.append(activation)\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "            for _ in range(n_layers - 1):\n",
    "                layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "                layers.append(activation)\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "            layers.append(nn.Linear(hidden_size, 1))  # Output layer\n",
    "            self.model = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    # Prepare the model\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = ANNModel(input_dim)\n",
    "\n",
    "    # Select optimizer with weight decay (L2 regularization)\n",
    "    optimizer = {\n",
    "        \"Adam\": optim.Adam(model.parameters(), lr=learning_rate, weight_decay=alpha),\n",
    "        \"SGD\": optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=alpha),\n",
    "        \"RMSprop\": optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=alpha)\n",
    "    }[optimizer_name]\n",
    "\n",
    "    # Add learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Prepare DataLoader\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = 5\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(100):  # Max epochs\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch.unsqueeze(1))\n",
    "            # Add L1 regularization manually\n",
    "            l1_loss = lambda_ * sum(p.abs().sum() for p in model.parameters())\n",
    "            total_loss = loss + l1_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(X_validate)\n",
    "            val_loss = criterion(val_predictions, y_validate.view(-1, 1)).item()\n",
    "\n",
    "        # Update the scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Log the current learning rate\n",
    "        current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss/len(train_loader):.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# Run the hyperparameter optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f9c7c-6034-49db-a905-34bf3aad506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f2843-f87f-4f9f-a5ab-0ed71fcf28b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize History results\n",
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c7e20-650a-4273-a8b0-980a5f209aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Importance results\n",
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9353b31-594c-48a5-a8bb-ab0ecf45625f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the best hyperparameters from the Optuna study\n",
    "best_params = study.best_params\n",
    "n_layers = best_params[\"n_layers\"]\n",
    "hidden_size = best_params[\"hidden_size\"]\n",
    "dropout_rate = best_params[\"dropout_rate\"]\n",
    "learning_rate = best_params[\"lr\"]\n",
    "batch_size = best_params[\"batch_size\"]\n",
    "optimizer_name = best_params[\"optimizer\"]\n",
    "activation_name = best_params[\"activation\"]\n",
    "alpha = best_params[\"alpha\"]  # L2 regularization term\n",
    "lambda_ = best_params[\"lambda\"]  # L1 regularization term\n",
    "\n",
    "# Define the activation function\n",
    "activation = {\n",
    "    \"ReLU\": nn.ReLU(),\n",
    "    \"LeakyReLU\": nn.LeakyReLU(),\n",
    "    \"Tanh\": nn.Tanh()\n",
    "}[activation_name]\n",
    "\n",
    "# Define the model using the best hyperparameters\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ANNModel, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_size))\n",
    "        layers.append(activation)\n",
    "        layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(activation)\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_size, 1))  # Output layer\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Prepare the model\n",
    "input_dim = X_train.shape[1]\n",
    "model = ANNModel(input_dim)\n",
    "\n",
    "# Select the optimizer with weight decay (L2 regularization)\n",
    "optimizer = {\n",
    "    \"Adam\": optim.Adam(model.parameters(), lr=learning_rate, weight_decay=alpha),\n",
    "    \"SGD\": optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=alpha),\n",
    "    \"RMSprop\": optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=alpha)\n",
    "}[optimizer_name]\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Prepare DataLoader for training\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop with L1 regularization\n",
    "model.train()\n",
    "for epoch in range(100):  # Number of epochs\n",
    "    epoch_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch.unsqueeze(1))\n",
    "        \n",
    "        # Add L1 regularization manually\n",
    "        l1_loss = lambda_ * sum(p.abs().sum() for p in model.parameters())\n",
    "        total_loss = loss + l1_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    logging.info(f\"Epoch {epoch+1}, Training Loss: {epoch_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce298e59-9ffd-4dfa-a829-89fd573a7ea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "best_val_loss = float(\"inf\")  # Initialize to a very large value\n",
    "patience_counter = 0  # Counter for epochs without improvement\n",
    "\n",
    "# Prepare Validation DataLoader\n",
    "validate_data = TensorDataset(X_validate, y_validate)\n",
    "validate_loader = DataLoader(validate_data, batch_size=batch_size)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch.unsqueeze(1))\n",
    "        \n",
    "        # Add L1 regularization manually\n",
    "        l1_loss = lambda_ * sum(p.abs().sum() for p in model.parameters())\n",
    "        total_loss = loss + l1_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in validate_loader:\n",
    "            val_predictions = model(X_val_batch)\n",
    "            val_loss += criterion(val_predictions, y_val_batch.unsqueeze(1)).item()\n",
    "    val_loss /= len(validate_loader)\n",
    "\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Logging\n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    val_losses.append(val_loss)\n",
    "    current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    logging.info(\n",
    "        f\"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss / len(train_loader):.4f}, \"\n",
    "        f\"Validation Loss: {val_loss:.4f}, Learning Rate: {current_lr:.6f}\"\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save the best model\n",
    "        best_model_path = f\"models/{file_name}.pth\"\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            logging.info(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "logging.info(\"Training complete. Best model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fcd320-fe14-4f90-8f38-994d2c0b507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Loss\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))  # Increase width for the subplots as well\n",
    "line1 = ax1.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='b')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Training Loss', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "ax1.grid(visible=True, linestyle='--', alpha=0.6)  # Add grid to the primary axis\n",
    "\n",
    "# Plot Validation Loss on a Secondary Y-Axis\n",
    "ax2 = ax1.twinx()\n",
    "line2 = ax2.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', color='orange')\n",
    "ax2.set_ylabel('Validation Loss', color='orange')\n",
    "ax2.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "# Add Early Stopping Point\n",
    "line3 = ax1.axvline(len(train_losses), color='r', linestyle='--', label='Early Stopping Point')\n",
    "\n",
    "# Combine legends from both axes\n",
    "lines = line1 + line2 + [line3]\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='upper right')\n",
    "\n",
    "# Add Title\n",
    "plt.title('Training and Validation Loss Over Epochs with Early Stopping')\n",
    "fig.tight_layout()\n",
    "plt.savefig(f'plots/01_train_vall_loss/{file_name}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d4e31-d39c-4137-b072-b2479db3c9db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ray HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1dbf71-ab24-4e40-a84d-0cb53ba9706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ann(config, checkpoint_dir=None):\n",
    "    # Extract hyperparameters\n",
    "    n_layers = config[\"n_layers\"]\n",
    "    hidden_size = config[\"hidden_size\"]\n",
    "    dropout_rate = config[\"dropout_rate\"]\n",
    "    lr = config[\"lr\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    activation_name = config[\"activation\"]\n",
    "    max_epochs = 50  # You can configure this\n",
    "\n",
    "\n",
    "\n",
    "# Store large objects in the Ray object store\n",
    "X_train_ref = ray.put(X_train)\n",
    "y_train_ref = ray.put(y_train)\n",
    "X_validate_ref = ray.put(X_validate)\n",
    "y_validate_ref = ray.put(y_validate)\n",
    "\n",
    "def train_ann(config, checkpoint_dir=None):\n",
    "    # Retrieve objects from the Ray object store\n",
    "    X_train = ray.get(X_train_ref)\n",
    "    y_train = ray.get(y_train_ref)\n",
    "    X_validate = ray.get(X_validate_ref)\n",
    "    y_validate = ray.get(y_validate_ref)\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    n_layers = config[\"n_layers\"]\n",
    "    hidden_size = config[\"hidden_size\"]\n",
    "    dropout_rate = config[\"dropout_rate\"]\n",
    "    lr = config[\"lr\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    activation_name = config[\"activation\"]\n",
    "    max_epochs = config[\"max_epochs\"] \n",
    "\n",
    "    # Define the model\n",
    "    class ANNModel(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(ANNModel, self).__init__()\n",
    "            layers = []\n",
    "            in_features = input_dim\n",
    "            for _ in range(n_layers):\n",
    "                layers.append(nn.Linear(in_features, hidden_size))\n",
    "                if activation_name == \"ReLU\":\n",
    "                    layers.append(nn.ReLU())\n",
    "                elif activation_name == \"LeakyReLU\":\n",
    "                    layers.append(nn.LeakyReLU())\n",
    "                elif activation_name == \"Tanh\":\n",
    "                    layers.append(nn.Tanh())\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "                in_features = hidden_size\n",
    "            layers.append(nn.Linear(hidden_size, 1))  # Output layer\n",
    "            self.network = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.network(x)\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = ANNModel(X_train.shape[1])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    validate_dataset = TensorDataset(X_validate, y_validate)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validate_loader = DataLoader(validate_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 2\n",
    "    counter = 0\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val_batch, y_val_batch in validate_loader:\n",
    "                val_predictions = model(X_val_batch)\n",
    "                val_loss += criterion(val_predictions, y_val_batch.unsqueeze(1)).item()\n",
    "        val_loss /= len(validate_loader)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            logging.info(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        # Print progress for debugging (optional)\n",
    "        logging.info(f\"Epoch {epoch + 1}/{max_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Report metrics to Ray Tune\n",
    "    session.report({\"val_loss\": val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d103df8-ff40-4950-9ad0-8f07085042f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"n_layers\": tune.randint(2, 10),\n",
    "    \"hidden_size\": tune.randint(32, 256),\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.5),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"batch_size\": tune.choice([16, 32, 64, 128]),\n",
    "    \"activation\": tune.choice([\"ReLU\", \"LeakyReLU\", \"Tanh\"]),\n",
    "    \"max_epochs\": tune.choice([10, 20, 50])  # Dynamically choose epochs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30212eed-2da2-46a0-a3fa-3b86a188476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the scheduler\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"val_loss\",  # Metric to optimize\n",
    "    mode=\"min\",         # We aim to minimize the validation loss\n",
    "    max_t=50,           # Max number of epochs\n",
    "    grace_period=5,     # Minimum number of epochs before stopping\n",
    "    reduction_factor=2  # Halve trials that perform poorly\n",
    ")\n",
    "\n",
    "# Set up the search algorithm\n",
    "search_algo = HyperOptSearch(\n",
    "    metric=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# Use an absolute path for storage_path\n",
    "absolute_storage_path = os.path.abspath(\"ray_results\")\n",
    "\n",
    "def trial_dirname_creator(trial):\n",
    "    # Shorten trial directory names by removing the full parameter list\n",
    "    return f\"trial_{trial.trial_id}\"\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_ann,                    # Training function\n",
    "    config=search_space,          # Hyperparameter search space\n",
    "    scheduler=scheduler,          # Scheduler for pruning bad trials\n",
    "    search_alg=search_algo,       # Search algorithm\n",
    "    num_samples=10,               # Number of trials to run\n",
    "    resources_per_trial={\"cpu\": 3, \"gpu\": 0},  # Resource allocation per trial\n",
    "    max_concurrent_trials=2,  # Allow only 2 trials to run concurrently\n",
    "    storage_path=absolute_storage_path,       # Use absolute path\n",
    "    trial_dirname_creator=trial_dirname_creator,  # Shorten trial names\n",
    ")\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_config = analysis.get_best_config(metric=\"val_loss\", mode=\"min\")\n",
    "print(\"Best Hyperparameters:\", best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c60895-0a6d-4ecc-956d-b742f5cb8f27",
   "metadata": {},
   "source": [
    "# Test & External Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1922c8-3480-4b34-a4d7-6a75d149f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and external validation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test).squeeze().numpy()\n",
    "    y_external_pred = model(X_external).squeeze().numpy()\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, y_test_pred)*100\n",
    "\n",
    "external_mse = mean_squared_error(y_external, y_external_pred)\n",
    "external_mae = mean_absolute_error(y_external, y_external_pred)\n",
    "external_rmse = np.sqrt(external_mse)\n",
    "external_r2 = r2_score(y_external, y_external_pred)*100\n",
    "\n",
    "print(f\"Test Set - MSE: {test_mse:.2f}, MAE: {test_mae:.2f}, RMSE: {test_rmse:.2f}, R2: {test_r2:.2f}\")\n",
    "print(f\"External Validation - MSE: {external_mse:.2f}, MAE: {external_mae:.2f}, RMSE: {external_rmse:.2f}, R2: {external_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba7784-37f6-4126-b512-c3e9b1a83b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics calculation\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "mae = mean_absolute_error(y_test, y_test_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_test_pred) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse:.4f}\")\n",
    "print(f\"Test Set MAE: {mae:.4f}\")\n",
    "print(f\"Test Set RMSE: {rmse:.4f}\")\n",
    "print(f\"Test Set R2: {r2:.4f}\")\n",
    "\n",
    "# Initialize error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [mse, mae, rmse]\n",
    "\n",
    "# Try to calculate MSLE\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_test_pred)\n",
    "    print(f\"Test Set MSLE: {msle:.4f}\")\n",
    "    \n",
    "    # Add MSLE to the list of metrics if applicable\n",
    "    error_metrics.append('MSLE')\n",
    "    values.append(msle)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")\n",
    "\n",
    "# Plot error metrics (with or without MSLE)\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics')\n",
    "\n",
    "# Annotate values on top of the bars\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.savefig(f'plots/03_metrics/{file_name}_internal.png')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the test set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if r2 >= 0:\n",
    "    plt.pie([r2, 100 - r2], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Test Set Explained Variance by R-squared (R2)')\n",
    "plt.savefig(f'plots/03_metrics/{file_name}_internal_R2.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1530c7e1-73f1-4cad-8ce3-3bb218993da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for external validation set\n",
    "mse_external = mean_squared_error(y_external, y_external_pred)\n",
    "mae_external = mean_absolute_error(y_external, y_external_pred)\n",
    "rmse_external = np.sqrt(mse_external)\n",
    "r2_external = r2_score(y_external, y_external_pred) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external:.4f}\")\n",
    "print(f\"External Validation Set MAE: {mae_external:.4f}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external:.4f}\")\n",
    "print(f\"External Validation Set R2: {r2_external:.4f}\")\n",
    "\n",
    "# Initialize error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [mse_external, mae_external, rmse_external]\n",
    "\n",
    "# Try to calculate MSLE\n",
    "try:\n",
    "    msle_external = mean_squared_log_error(y_external, y_external_pred)\n",
    "    print(f\"External Validation Set MSLE: {msle_external:.4f}\")\n",
    "    \n",
    "    # Add MSLE to the list of metrics if applicable\n",
    "    error_metrics.append('MSLE')\n",
    "    values.append(msle_external)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")\n",
    "\n",
    "# Plot error metrics (with or without MSLE)\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics')\n",
    "\n",
    "# Annotate values on top of the bars\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.savefig(f'plots/03_metrics/{file_name}_external.png')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the external validation set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if r2_external >= 0:\n",
    "    plt.pie([r2_external, 100 - r2_external], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Validation Set Explained Variance by R-squared (R2)')\n",
    "plt.savefig(f'plots/03_metrics/{file_name}_external_R2.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891ac33-2a3d-4dbe-a740-249a72defeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_test_pred, color='blue', label='Prediction')\n",
    "\n",
    "# Line for Perfect Prediction\n",
    "perfect_line = np.linspace(y_test.min(), y_test.max(), 100)\n",
    "plt.plot(perfect_line, perfect_line, color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Labels, legend, and grid\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Predicted LOS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Predicted vs. True LOS (Test Set)')\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "plt.savefig(f\"plots/02_prediction_plot/02_true_vs_pred/{file_name}_test_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# External Validation Set Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_external, y_external_pred, color='blue', label='Prediction')\n",
    "\n",
    "# Line for Perfect Prediction (y = x)\n",
    "perfect_line_ext = np.linspace(y_external.min(), y_external.max(), 100)\n",
    "plt.plot(perfect_line_ext, perfect_line_ext, color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Labels, legend, and grid\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Predicted LOS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Predicted vs. True LOS (External Validation Set)')\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "plt.savefig(f\"plots/02_prediction_plot/02_true_vs_pred/{file_name}_external_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d76b1-d9af-4790-88d0-752b252b32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_test to a 1D numpy array\n",
    "#y_test = y_test.numpy().flatten()\n",
    "\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, residuals, color='blue', alpha=0.5, label=\"Residuals\")\n",
    "plt.axhline(y=0, color='red', linestyle='--', label=\"Zero Line\")\n",
    "plt.axhline(y=mae, color='green', linestyle='--', label=f\"MAE = {mae:.2f}\")\n",
    "plt.axhline(y=-mae, color='green', linestyle='--')\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Residuals (True - Predicted)')\n",
    "plt.title('Residuals Plot with MAE Bounds')\n",
    "plt.grid(True)\n",
    "\n",
    "# Place the legend outside of the plot\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "plt.savefig(f\"plots/02_Prediction_Plot/01_residuals/{file_name}_residuals_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdcb3a1-b975-4325-b2af-eaa3539f2b67",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4832b64-6291-4691-8919-ed8db53c19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"models/{file_name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fc07dc-610f-43da-bbc3-3f1951d15037",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262423c6-8c12-492e-b6cd-ab78ec3d7b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file\n",
    "subfolder = \"o01_feed_forward.pth\"\n",
    "\n",
    "# Reinitialize the model architecture\n",
    "input_dim = X_test.shape[1]  # Ensure this matches the original input dimension\n",
    "model = ANNModel(input_dim)\n",
    "\n",
    "# Load the saved model state\n",
    "model.load_state_dict(torch.load(f\"models/{subfolder}\"))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\"\"\"\n",
    "After that I must run the block with layers.\n",
    "Be careful, the layers must be exaclty the same. \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
