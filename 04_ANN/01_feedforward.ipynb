{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032a1ce8-5983-4e55-9e97-d95caa3b4aad",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b1d3ab-adbb-4e80-8118-96e78a4df776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "# display Matplotlib plots directly within the notebook interface\n",
    "%matplotlib inline \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  # neural network\n",
    "import torch.nn.functional as F  # help us move our data forward in function\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "# Hyperparameter\n",
    "import optuna\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f325999-bfaa-40bc-9983-f75093b263ed",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f815e91a-e99b-45d8-861c-9eee517658b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We convert the data into float32 because PyTorch expects float 32 values\n",
    "- Compatibility with DL\n",
    "- Memory Efficiency\n",
    "- Prevent errors in training (backpropagation etc)\n",
    "\"\"\"\n",
    "# Define subfolder\n",
    "subfolder = \"o6_GAN/o02\"\n",
    "\n",
    "# Load CSV files into corresponding variables\n",
    "X_external = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/X_external.csv\").astype('float32')\n",
    "y_external = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/y_external.csv\").values.ravel().astype('float32')\n",
    "X_train = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/X_train.csv\").astype('float32')\n",
    "y_train = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/y_train.csv\").values.ravel().astype('float32')\n",
    "X_validate = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/X_validate.csv\").astype('float32')\n",
    "y_validate = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/y_validate.csv\").values.ravel().astype('float32')\n",
    "X_test = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/X_test.csv\").astype('float32')\n",
    "y_test = pd.read_csv(f\"../03_External_Validation/CSV/exports/impute/{subfolder}/y_test.csv\").values.ravel().astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b9c234-969a-4262-9baa-dcc44e77d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All computations in PyTorch are performed by the use of tensors\n",
    "and not with pandas dataframes or NymPy arrays.\n",
    "\n",
    "X_train.values extract NumPy arrays from pandas dataframe\n",
    "and torch.tensor converts it to PyTorch tensor.\n",
    "\"\"\"\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train.values)\n",
    "y_train = torch.tensor(y_train) # this is already an NymPy array\n",
    "X_validate = torch.tensor(X_validate.values)\n",
    "y_validate = torch.tensor(y_validate) # this is already an NymPy array\n",
    "X_test = torch.tensor(X_test.values)\n",
    "y_test = torch.tensor(y_test) # this is already an NymPy array\n",
    "X_external = torch.tensor(X_external.values)\n",
    "y_external = torch.tensor(y_external) # this is already an NymPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ded2b6-44ca-4ebd-9c63-938bb258a525",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test Simple Feed Forward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a885af-5426-4b0a-b8ba-c2b3190d75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Model Class that inherits nn.Module\n",
    "\n",
    "class Model(nn.Module):\n",
    "    # Input layer (344 ICU features) -->\n",
    "    # Hidden Layer 1 (number of neurons) -->\n",
    "    # Hidden Layer 2 (number of neurons) -->\n",
    "    # Output (1 LOS)\n",
    "    def __init__(self, input_dim, h1=256, h2=128, h3=64, h4=32, out_features=1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.fc3 = nn.Linear(h2, h3)\n",
    "        self.fc4 = nn.Linear(h3, h4)\n",
    "        self.output = nn.Linear(h4, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940d87b-d98a-4cb3-9fb3-84b7f228a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a model\n",
    "input_dim = X_train.shape[1] # retrive input features\n",
    "model = Model(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f1eeaa-2bd8-41d7-ae4b-14071584dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the criterion of model to measure the error\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Choose Optimizer, set learning rate and epochs (model.parameters are the NN layers)\n",
    "optimizer =torch.optim.Adam(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c13403-d6db-4828-b21c-ef8abac953a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.parameters are the NN layers\n",
    "display(model.parameters)\n",
    "print(\"\\n-------------------\\n\")\n",
    "display(list(model.parameters()))\n",
    "print(\"\\n-------------------\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]}\")  # Display first 2 values for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b563e7-4c31-4036-845c-d39880009540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "epochs = 100\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    # Ensure shapes match\n",
    "    y_pred = y_pred.squeeze()  # Make y_pred a 1D tensor\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    \n",
    "    # Track losses\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Print every 10 epochs\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Epoch: {i}, Loss: {loss.item()}\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64353a2-bb03-47a0-baa4-39be2d06ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs),losses)\n",
    "plt.ylabel(\"loss/error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5a7d3-f9e2-413e-a36b-e861a0655b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb343f-378d-4406-b904-53e4babd7045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc2ee0-70a9-46a6-8f91-37f8a389fd70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988b4083-671e-488c-94d6-3f6b588cff88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5776c-1721-40f7-9852-7a577a7d7047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e518ce2f-934f-46fe-bcc7-adf2d86850df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Feed Forward ANN without HP\n",
    "## Two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadeadd1-df2f-4ae4-99c8-62f082d67a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Layers\n",
    "- Input 128 -> 64 -> 32 -> 1\n",
    "- Each layer use ReLU activation function.\n",
    "\"\"\"\n",
    "\n",
    "# ANN architecture\n",
    "class ANNModel(nn.Module):\n",
    "    # input_dim is the input features and are the same as the dataframe.\n",
    "    def __init__(self, input_dim): \n",
    "        super(ANNModel, self).__init__()\n",
    "        # Define layers. Full features as input and 128 outputs.\n",
    "        # Weight and biases are initialized automatically.\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        # Its techique that randomly set neurons to zero to avoid overfitting\n",
    "        # I must check it further. Propability 0.2 = 20%\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        # Next layer with 128 inputs and 64 outputs.\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # Its techique that randomly set neurons to zero to avoid overfitting\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        # Next layer with 64 inputs and 32 outputs.\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        # Output layer with 64 inputs 1 output.\n",
    "        self.output = nn.Linear(32, 1)\n",
    "\n",
    "# Feedforward Network\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) # Pass the x throught the first dense fc1 with relu function\n",
    "        x = self.dropout1(x) # turn off neurons to prevent overfitting\n",
    "        x = torch.relu(self.fc2(x)) # Pass the x throught the first dense fc2 with relu function\n",
    "        x = self.dropout2(x) # turn off neurons to prevent overfitting\n",
    "        x = torch.relu(self.fc3(x)) # Pass the x throught the first dense fc3 with relu function\n",
    "        x = self.output(x) # output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd8ca8-657d-4c62-8e81-428e4717d04e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Feed Forward ANN without HP\n",
    "## Three hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0746b18b-64fe-4a68-be00-cab1045c6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ANNModel, self).__init__()\n",
    "        # Increase the width of the first two layers and add more layers\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(64, 32)  # Additional layer\n",
    "        self.dropout4 = nn.Dropout(0.2)  # Additional dropout\n",
    "        self.output = nn.Linear(32, 1)  # Output layer remains the same\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))  # Pass through additional layer\n",
    "        x = self.dropout4(x)  # Apply dropout\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc3ef6-0e98-4931-95d3-a2a8130ec1f9",
   "metadata": {},
   "source": [
    "# HyperOpt Feed Forward ANN\n",
    "## Two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb7c154-aaed-4570-bb1c-47e6b13990ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the ANN model class\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim, layer1, layer2, dropout_rate):\n",
    "        super(ANNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, layer1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(layer1, layer2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(layer2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)  # No activation in the output layer for regression\n",
    "        return x\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    # Unpack parameters\n",
    "    layer1 = int(params['layer1'])\n",
    "    layer2 = int(params['layer2'])\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    learning_rate = params['learning_rate']\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ANNModel(input_dim=X_train.shape[1], layer1=layer1, layer2=layer2, dropout_rate=dropout_rate)\n",
    "    model.to(device)  # Move model to GPU if available\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 10  # Use a small number for quick evaluation\n",
    "    batch_size = 32\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[i:i + batch_size].to(device)\n",
    "            y_batch = y_train[i:i + batch_size].unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_validate.to(device))\n",
    "        val_loss = mean_squared_error(\n",
    "            y_validate.cpu().numpy(),\n",
    "            val_predictions.cpu().numpy()\n",
    "        )\n",
    "\n",
    "    # Return the loss as the optimization metric\n",
    "    return {'loss': val_loss, 'status': STATUS_OK}\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_space = {\n",
    "    'layer1': hp.quniform('layer1', 64, 256, 32),\n",
    "    'layer2': hp.quniform('layer2', 32, 128, 16),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.5),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -4, -2),  # log scale for small LR\n",
    "}\n",
    "\n",
    "# Initialize Trials object\n",
    "trials = Trials()\n",
    "\n",
    "# Perform hyperparameter search\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=param_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,  # Number of iterations\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best Hyperparameters:\", best)\n",
    "\n",
    "# Initialize the model with the best hyperparameters\n",
    "best_model = ANNModel(\n",
    "    input_dim=X_train.shape[1],\n",
    "    layer1=int(best['layer1']),\n",
    "    layer2=int(best['layer2']),\n",
    "    dropout_rate=best['dropout_rate']\n",
    ")\n",
    "best_model.to(device)\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca81800-791a-45d7-ad5b-5fd2924d367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with the best hyperparameters\n",
    "best_model = ANNModel(\n",
    "    input_dim=X_train.shape[1],\n",
    "    layer1=int(best['layer1']),\n",
    "    layer2=int(best['layer2']),\n",
    "    dropout_rate=best['dropout_rate']\n",
    ")\n",
    "best_model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best['learning_rate'])\n",
    "\n",
    "# Train the final model\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    best_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[i:i+batch_size].to(device)\n",
    "        y_batch = y_train[i:i+batch_size].unsqueeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = best_model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = best_model(X_validate.to(device))\n",
    "        val_loss = criterion(val_predictions, y_validate.unsqueeze(1).to(device)).item()\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Evaluate on test and external validation sets\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = best_model(X_test.to(device))\n",
    "    external_predictions = best_model(X_external.to(device))\n",
    "\n",
    "    # Convert predictions back to CPU for evaluation\n",
    "    test_predictions = test_predictions.cpu().numpy()\n",
    "    external_predictions = external_predictions.cpu().numpy()\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse_test = mean_squared_error(y_test.cpu(), test_predictions)\n",
    "    mae_test = mean_absolute_error(y_test.cpu(), test_predictions)\n",
    "    mse_external = mean_squared_error(y_external.cpu(), external_predictions)\n",
    "    mae_external = mean_absolute_error(y_external.cpu(), external_predictions)\n",
    "\n",
    "print(f\"Test Set - MSE: {mse_test:.4f}, MAE: {mae_test:.4f}\")\n",
    "print(f\"External Validation Set - MSE: {mse_external:.4f}, MAE: {mae_external:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753be50-02e9-47f3-93c8-5853958dd323",
   "metadata": {},
   "source": [
    "# Optuna HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e1d8c-1226-48fa-b64c-0d9734058a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-28 21:02:27,724] A new study created in memory with name: no-name-3f1b873e-c3fc-4f2e-ae49-7ae382a6f5ba\n",
      "[I 2024-11-28 21:04:02,303] Trial 0 finished with value: 1261.4462890625 and parameters: {'n_layers': 2, 'hidden_size': 96, 'dropout_rate': 0.4, 'lr': 0.00010583583537780022}. Best is trial 0 with value: 1261.4462890625.\n"
     ]
    }
   ],
   "source": [
    "# Define objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 3)  # Number of hidden layers\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 256, step=32)  # Neurons in hidden layers\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5, step=0.1)  # Dropout probability\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)  # Learning rate\n",
    "\n",
    "    class ANNModel(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(ANNModel, self).__init__()\n",
    "            layers = []\n",
    "            layers.append(nn.Linear(input_dim, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "            for _ in range(n_layers - 1):\n",
    "                layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "            layers.append(nn.Linear(hidden_size, 1))  # Output layer\n",
    "            self.model = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    # Prepare the model\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = ANNModel(input_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Prepare DataLoader\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(20):  # Fixed number of epochs for tuning\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_validate)\n",
    "        val_loss = criterion(val_predictions, y_validate.view(-1, 1))\n",
    "\n",
    "    return val_loss.item()\n",
    "\n",
    "# Run the hyperparameter optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d5640-14d6-46d1-9bef-e81e460e073c",
   "metadata": {},
   "source": [
    "# Feed forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663de2e5-fc4a-4b7a-b02a-9911c8d0c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "\n",
    "# prepare model to take inputs\n",
    "input_dim = X_train.shape[1] # retrive input features\n",
    "model = ANNModel(input_dim) # creates the model\n",
    "\n",
    "# computes how far from the true values are the predictions\n",
    "criterion = nn.MSELoss() # loss function MSE\n",
    "\n",
    "# update the weights to minimize loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # set Adam optimizer | lr = learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe1918-0f8f-4359-8ac5-f5016706aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping parameters\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "min_delta = 0.001  # Minimum change in validation loss to qualify as an improvement\n",
    "best_val_loss = float('inf')  # Initialize to a very large value\n",
    "patience_counter = 0  # Counter for epochs without improvement\n",
    "\n",
    "# Training the model with early stopping\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "early_stop = False\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "for epoch in range(epochs):\n",
    "    if early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size].unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_validate)\n",
    "        val_loss = criterion(val_predictions, y_validate.view(-1, 1))\n",
    "\n",
    "    # Append losses for plotting\n",
    "    train_losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss.item())\n",
    "    \n",
    "    #print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Logging progress\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logging.info(f\"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Check for improvement\n",
    "    if best_val_loss - val_loss.item() > min_delta:\n",
    "        best_val_loss = val_loss.item()\n",
    "        patience_counter = 0  # Reset the counter\n",
    "    else:\n",
    "        patience_counter += 1  # Increment the counter\n",
    "        if patience_counter >= patience:\n",
    "            early_stop = True\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs with Early Stopping')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c60895-0a6d-4ecc-956d-b742f5cb8f27",
   "metadata": {},
   "source": [
    "# Test & External Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1922c8-3480-4b34-a4d7-6a75d149f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and external validation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test).squeeze().numpy()\n",
    "    y_external_pred = model(X_external).squeeze().numpy()\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, y_test_pred)*100\n",
    "\n",
    "external_mse = mean_squared_error(y_external, y_external_pred)\n",
    "external_mae = mean_absolute_error(y_external, y_external_pred)\n",
    "external_rmse = np.sqrt(external_mse)\n",
    "external_r2 = r2_score(y_external, y_external_pred)*100\n",
    "\n",
    "print(f\"Test Set - MSE: {test_mse:.2f}, MAE: {test_mae:.2f}, RMSE: {test_rmse:.2f}, R2: {test_r2:.2f}\")\n",
    "print(f\"External Validation - MSE: {external_mse:.2f}, MAE: {external_mae:.2f}, RMSE: {external_rmse:.2f}, R2: {external_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba7784-37f6-4126-b512-c3e9b1a83b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics calculation\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "mae = mean_absolute_error(y_test, y_test_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_test_pred) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse:.4f}\")\n",
    "print(f\"Test Set MAE: {mae:.4f}\")\n",
    "print(f\"Test Set RMSE: {rmse:.4f}\")\n",
    "print(f\"Test Set R2: {r2:.4f}\")\n",
    "\n",
    "# Initialize error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [mse, mae, rmse]\n",
    "\n",
    "# Try to calculate MSLE\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_test_pred)\n",
    "    print(f\"Test Set MSLE: {msle:.4f}\")\n",
    "    \n",
    "    # Add MSLE to the list of metrics if applicable\n",
    "    error_metrics.append('MSLE')\n",
    "    values.append(msle)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")\n",
    "\n",
    "# Plot error metrics (with or without MSLE)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the test set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if r2 >= 0:\n",
    "    plt.pie([r2, 100 - r2], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Test Set Explained Variance by R-squared (R2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1530c7e1-73f1-4cad-8ce3-3bb218993da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for external validation set\n",
    "mse_external = mean_squared_error(y_external, y_external_pred)\n",
    "mae_external = mean_absolute_error(y_external, y_external_pred)\n",
    "rmse_external = np.sqrt(mse_external)\n",
    "r2_external = r2_score(y_external, y_external_pred) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external:.4f}\")\n",
    "print(f\"External Validation Set MAE: {mae_external:.4f}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external:.4f}\")\n",
    "print(f\"External Validation Set R2: {r2_external:.4f}\")\n",
    "\n",
    "# Initialize error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [mse_external, mae_external, rmse_external]\n",
    "\n",
    "# Try to calculate MSLE\n",
    "try:\n",
    "    msle_external = mean_squared_log_error(y_external, y_external_pred)\n",
    "    print(f\"External Validation Set MSLE: {msle_external:.4f}\")\n",
    "    \n",
    "    # Add MSLE to the list of metrics if applicable\n",
    "    error_metrics.append('MSLE')\n",
    "    values.append(msle_external)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")\n",
    "\n",
    "# Plot error metrics (with or without MSLE)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the external validation set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if r2_external >= 0:\n",
    "    plt.pie([r2_external, 100 - r2_external], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Validation Set Explained Variance by R-squared (R2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891ac33-2a3d-4dbe-a740-249a72defeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_test_pred, color='blue', label='Prediction')\n",
    "\n",
    "# Line for Perfect Prediction\n",
    "perfect_line = np.linspace(y_test.min(), y_test.max(), 100)\n",
    "plt.plot(perfect_line, perfect_line, color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Labels, legend, and grid\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Predicted LOS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Predicted vs. True LOS (Test Set)')\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "#plt.savefig(\"plots/02_Prediction_Plot/02_true_vs_pred/57_true_vs_pred_test_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# External Validation Set Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_external, y_external_pred, color='blue', label='Prediction')\n",
    "\n",
    "# Line for Perfect Prediction (y = x)\n",
    "perfect_line_ext = np.linspace(y_external.min(), y_external.max(), 100)\n",
    "plt.plot(perfect_line_ext, perfect_line_ext, color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Labels, legend, and grid\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Predicted LOS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Predicted vs. True LOS (External Validation Set)')\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "#plt.savefig(\"plots/02_Prediction_Plot/02_true_vs_pred/57_true_vs_pred_external_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d76b1-d9af-4790-88d0-752b252b32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_test to a 1D numpy array\n",
    "y_test = y_test.numpy().flatten()\n",
    "\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, residuals, color='blue', alpha=0.5, label=\"Residuals\")\n",
    "plt.axhline(y=0, color='red', linestyle='--', label=\"Zero Line\")\n",
    "plt.axhline(y=mae, color='green', linestyle='--', label=f\"MAE = {mae:.2f}\")\n",
    "plt.axhline(y=-mae, color='green', linestyle='--')\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Residuals (True - Predicted)')\n",
    "plt.title('Residuals Plot with MAE Bounds')\n",
    "plt.grid(True)\n",
    "\n",
    "# Place the legend outside of the plot\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "#plt.savefig(\"plots/02_Prediction_Plot/01_residuals/57_residuals_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdcb3a1-b975-4325-b2af-eaa3539f2b67",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4832b64-6291-4691-8919-ed8db53c19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file\n",
    "subfolder = \"o02_feed_forward_three_layers_256_128_64_32_1.pth\"\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), f\"models/{subfolder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fc07dc-610f-43da-bbc3-3f1951d15037",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262423c6-8c12-492e-b6cd-ab78ec3d7b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file\n",
    "subfolder = \"o01_feed_forward.pth\"\n",
    "\n",
    "# Reinitialize the model architecture\n",
    "input_dim = X_test.shape[1]  # Ensure this matches the original input dimension\n",
    "model = ANNModel(input_dim)\n",
    "\n",
    "# Load the saved model state\n",
    "model.load_state_dict(torch.load(f\"models/{subfolder}\"))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\"\"\"\n",
    "After that I must run the block with layers.\n",
    "Be careful, the layers must be exaclty the same. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9418b819-e643-4f8e-9268-c6e1e45cbb78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d389220-2d54-4e61-9331-c3ef98a4c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tensor_X = torch.tensor(X, dtype=torch.float32)\n",
    "        predictions = model(tensor_X).numpy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0defa-f61d-4841-831e-a65f62e163d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensors to Pandas DataFrames\n",
    "X_sample_df = pd.DataFrame(X_sample_np, columns=[f\"Feature_{i}\" for i in range(X_sample_np.shape[1])])\n",
    "X_validate_df = pd.DataFrame(X_validate_np, columns=[f\"Feature_{i}\" for i in range(X_validate_np.shape[1])])\n",
    "\n",
    "# Use KernelExplainer with Pandas DataFrame\n",
    "explainer = shap.KernelExplainer(model_predict, X_sample_df)\n",
    "shap_values = explainer.shap_values(X_validate_df)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_validate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d6a10-d882-427f-bae1-eaa908c80ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
