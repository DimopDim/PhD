{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e3b069-e7e7-4b55-9e4b-bd257e4e673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Iterative imputation\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# GAN imputation using PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7a7c9b-3629-4077-bcea-d58a9cc0472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"data_loading.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e6cfc3-fc60-45b2-af0b-46c147bfd09d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 19:05:39,477 - INFO - Loading... -> o1_X_external.csv\n",
      "2025-02-10 19:05:46,724 - INFO - Loading... -> o1_X_test.csv\n",
      "2025-02-10 19:05:47,221 - INFO - Loading... -> o1_X_train.csv\n",
      "2025-02-10 19:05:51,200 - INFO - Loading... -> o1_X_validate.csv\n",
      "2025-02-10 19:05:51,700 - INFO - Loading... -> o1_y_external_los.csv\n",
      "2025-02-10 19:05:51,747 - INFO - Loading... -> o1_y_external_mortality.csv\n",
      "2025-02-10 19:05:51,777 - INFO - Loading... -> o1_y_test_los.csv\n",
      "2025-02-10 19:05:51,787 - INFO - Loading... -> o1_y_test_mortality.csv\n",
      "2025-02-10 19:05:51,793 - INFO - Loading... -> o1_y_train_los.csv\n",
      "2025-02-10 19:05:51,839 - INFO - Loading... -> o1_y_train_mortality.csv\n",
      "2025-02-10 19:05:51,861 - INFO - Loading... -> o1_y_validate_los.csv\n",
      "2025-02-10 19:05:51,872 - INFO - Loading... -> o1_y_validate_mortality.csv\n",
      "2025-02-10 19:05:51,878 - INFO - Loading... -> o2_X_external.csv\n",
      "2025-02-10 19:05:55,462 - INFO - Loading... -> o2_X_test.csv\n",
      "2025-02-10 19:05:55,733 - INFO - Loading... -> o2_X_train.csv\n",
      "2025-02-10 19:05:57,766 - INFO - Loading... -> o2_X_validate.csv\n",
      "2025-02-10 19:05:58,044 - INFO - Loading... -> o2_y_external_los.csv\n",
      "2025-02-10 19:05:58,068 - INFO - Loading... -> o2_y_external_mortality.csv\n",
      "2025-02-10 19:05:58,087 - INFO - Loading... -> o2_y_test_los.csv\n",
      "2025-02-10 19:05:58,094 - INFO - Loading... -> o2_y_test_mortality.csv\n",
      "2025-02-10 19:05:58,098 - INFO - Loading... -> o2_y_train_los.csv\n",
      "2025-02-10 19:05:58,122 - INFO - Loading... -> o2_y_train_mortality.csv\n",
      "2025-02-10 19:05:58,133 - INFO - Loading... -> o2_y_validate_los.csv\n",
      "2025-02-10 19:05:58,139 - INFO - Loading... -> o2_y_validate_mortality.csv\n",
      "2025-02-10 19:05:58,144 - INFO - Loading... -> o3_X_external.csv\n",
      "2025-02-10 19:06:00,518 - INFO - Loading... -> o3_X_test.csv\n",
      "2025-02-10 19:06:00,709 - INFO - Loading... -> o3_X_train.csv\n",
      "2025-02-10 19:06:01,999 - INFO - Loading... -> o3_X_validate.csv\n",
      "2025-02-10 19:06:02,182 - INFO - Loading... -> o3_y_external_los.csv\n",
      "2025-02-10 19:06:02,199 - INFO - Loading... -> o3_y_external_mortality.csv\n",
      "2025-02-10 19:06:02,212 - INFO - Loading... -> o3_y_test_los.csv\n",
      "2025-02-10 19:06:02,216 - INFO - Loading... -> o3_y_test_mortality.csv\n",
      "2025-02-10 19:06:02,220 - INFO - Loading... -> o3_y_train_los.csv\n",
      "2025-02-10 19:06:02,236 - INFO - Loading... -> o3_y_train_mortality.csv\n",
      "2025-02-10 19:06:02,245 - INFO - Loading... -> o3_y_validate_los.csv\n",
      "2025-02-10 19:06:02,249 - INFO - Loading... -> o3_y_validate_mortality.csv\n",
      "2025-02-10 19:06:02,252 - INFO - Loading... -> o4_X_external.csv\n",
      "2025-02-10 19:06:04,076 - INFO - Loading... -> o4_X_test.csv\n",
      "2025-02-10 19:06:04,234 - INFO - Loading... -> o4_X_train.csv\n",
      "2025-02-10 19:06:05,237 - INFO - Loading... -> o4_X_validate.csv\n",
      "2025-02-10 19:06:05,386 - INFO - Loading... -> o4_y_external_los.csv\n",
      "2025-02-10 19:06:05,404 - INFO - Loading... -> o4_y_external_mortality.csv\n",
      "2025-02-10 19:06:05,414 - INFO - Loading... -> o4_y_test_los.csv\n",
      "2025-02-10 19:06:05,419 - INFO - Loading... -> o4_y_test_mortality.csv\n",
      "2025-02-10 19:06:05,423 - INFO - Loading... -> o4_y_train_los.csv\n",
      "2025-02-10 19:06:05,439 - INFO - Loading... -> o4_y_train_mortality.csv\n",
      "2025-02-10 19:06:05,447 - INFO - Loading... -> o4_y_validate_los.csv\n",
      "2025-02-10 19:06:05,452 - INFO - Loading... -> o4_y_validate_mortality.csv\n",
      "2025-02-10 19:06:05,458 - INFO - o1_X_external loaded successfully with shape (234720, 345)\n",
      "2025-02-10 19:06:05,459 - INFO - o1_X_test loaded successfully with shape (15312, 345)\n",
      "2025-02-10 19:06:05,459 - INFO - o1_X_train loaded successfully with shape (122496, 345)\n",
      "2025-02-10 19:06:05,460 - INFO - o1_X_validate loaded successfully with shape (15312, 345)\n",
      "2025-02-10 19:06:05,461 - INFO - o1_y_external_los loaded successfully with shape (234720, 1)\n",
      "2025-02-10 19:06:05,462 - INFO - o1_y_external_mortality loaded successfully with shape (234720, 1)\n",
      "2025-02-10 19:06:05,463 - INFO - o1_y_test_los loaded successfully with shape (15312, 1)\n",
      "2025-02-10 19:06:05,464 - INFO - o1_y_test_mortality loaded successfully with shape (15312, 1)\n",
      "2025-02-10 19:06:05,465 - INFO - o1_y_train_los loaded successfully with shape (122496, 1)\n",
      "2025-02-10 19:06:05,466 - INFO - o1_y_train_mortality loaded successfully with shape (122496, 1)\n",
      "2025-02-10 19:06:05,467 - INFO - o1_y_validate_los loaded successfully with shape (15312, 1)\n",
      "2025-02-10 19:06:05,469 - INFO - o1_y_validate_mortality loaded successfully with shape (15312, 1)\n",
      "2025-02-10 19:06:05,469 - INFO - o2_X_external loaded successfully with shape (117360, 345)\n",
      "2025-02-10 19:06:05,472 - INFO - o2_X_test loaded successfully with shape (7656, 345)\n",
      "2025-02-10 19:06:05,473 - INFO - o2_X_train loaded successfully with shape (61248, 345)\n",
      "2025-02-10 19:06:05,474 - INFO - o2_X_validate loaded successfully with shape (7656, 345)\n",
      "2025-02-10 19:06:05,474 - INFO - o2_y_external_los loaded successfully with shape (117360, 1)\n",
      "2025-02-10 19:06:05,475 - INFO - o2_y_external_mortality loaded successfully with shape (117360, 1)\n",
      "2025-02-10 19:06:05,477 - INFO - o2_y_test_los loaded successfully with shape (7656, 1)\n",
      "2025-02-10 19:06:05,477 - INFO - o2_y_test_mortality loaded successfully with shape (7656, 1)\n",
      "2025-02-10 19:06:05,478 - INFO - o2_y_train_los loaded successfully with shape (61248, 1)\n",
      "2025-02-10 19:06:05,479 - INFO - o2_y_train_mortality loaded successfully with shape (61248, 1)\n",
      "2025-02-10 19:06:05,479 - INFO - o2_y_validate_los loaded successfully with shape (7656, 1)\n",
      "2025-02-10 19:06:05,480 - INFO - o2_y_validate_mortality loaded successfully with shape (7656, 1)\n",
      "2025-02-10 19:06:05,481 - INFO - o3_X_external loaded successfully with shape (78240, 345)\n",
      "2025-02-10 19:06:05,483 - INFO - o3_X_test loaded successfully with shape (5104, 345)\n",
      "2025-02-10 19:06:05,484 - INFO - o3_X_train loaded successfully with shape (40832, 345)\n",
      "2025-02-10 19:06:05,485 - INFO - o3_X_validate loaded successfully with shape (5104, 345)\n",
      "2025-02-10 19:06:05,486 - INFO - o3_y_external_los loaded successfully with shape (78240, 1)\n",
      "2025-02-10 19:06:05,488 - INFO - o3_y_external_mortality loaded successfully with shape (78240, 1)\n",
      "2025-02-10 19:06:05,489 - INFO - o3_y_test_los loaded successfully with shape (5104, 1)\n",
      "2025-02-10 19:06:05,490 - INFO - o3_y_test_mortality loaded successfully with shape (5104, 1)\n",
      "2025-02-10 19:06:05,491 - INFO - o3_y_train_los loaded successfully with shape (40832, 1)\n",
      "2025-02-10 19:06:05,492 - INFO - o3_y_train_mortality loaded successfully with shape (40832, 1)\n",
      "2025-02-10 19:06:05,493 - INFO - o3_y_validate_los loaded successfully with shape (5104, 1)\n",
      "2025-02-10 19:06:05,494 - INFO - o3_y_validate_mortality loaded successfully with shape (5104, 1)\n",
      "2025-02-10 19:06:05,495 - INFO - o4_X_external loaded successfully with shape (58680, 345)\n",
      "2025-02-10 19:06:05,495 - INFO - o4_X_test loaded successfully with shape (3828, 345)\n",
      "2025-02-10 19:06:05,496 - INFO - o4_X_train loaded successfully with shape (30624, 345)\n",
      "2025-02-10 19:06:05,497 - INFO - o4_X_validate loaded successfully with shape (3828, 345)\n",
      "2025-02-10 19:06:05,498 - INFO - o4_y_external_los loaded successfully with shape (58680, 1)\n",
      "2025-02-10 19:06:05,499 - INFO - o4_y_external_mortality loaded successfully with shape (58680, 1)\n",
      "2025-02-10 19:06:05,499 - INFO - o4_y_test_los loaded successfully with shape (3828, 1)\n",
      "2025-02-10 19:06:05,500 - INFO - o4_y_test_mortality loaded successfully with shape (3828, 1)\n",
      "2025-02-10 19:06:05,501 - INFO - o4_y_train_los loaded successfully with shape (30624, 1)\n",
      "2025-02-10 19:06:05,502 - INFO - o4_y_train_mortality loaded successfully with shape (30624, 1)\n",
      "2025-02-10 19:06:05,502 - INFO - o4_y_validate_los loaded successfully with shape (3828, 1)\n",
      "2025-02-10 19:06:05,503 - INFO - o4_y_validate_mortality loaded successfully with shape (3828, 1)\n",
      "2025-02-10 19:06:05,504 - INFO - Load Complete.\n"
     ]
    }
   ],
   "source": [
    "# CSVs Directory \n",
    "data_path = \"../CSV/exports/split_set/without_multiple_rows\"\n",
    "all_files = os.listdir(data_path)\n",
    "\n",
    "# Load CSVs into a dictionary of dataframes\n",
    "dataframes = {}\n",
    "for file in all_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        var_name = file.replace(\".csv\", \"\").replace(\"-\", \"_\")\n",
    "        logging.info(f\"Loading... -> {file}\")\n",
    "        dataframes[var_name] = pd.read_csv(os.path.join(data_path, file)).astype('float32')\n",
    "\n",
    "# Log loaded datasets\n",
    "for var_name, df in dataframes.items():\n",
    "    globals()[var_name] = df\n",
    "    logging.info(f\"{var_name} loaded successfully with shape {df.shape}\")\n",
    "logging.info(\"Load Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67278f2-4393-4cd8-9abf-a34a50c26995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 40832, Total Columns: 345\n",
      "Number of rows with missing values up to 30 %: 10912\n",
      "The percentage between total and missing values sets are 26.72%\n"
     ]
    }
   ],
   "source": [
    "percent = 30\n",
    "\n",
    "# Calculate the percentage of missing values per row\n",
    "missing_percentage_per_row = o3_X_train.isnull().mean(axis=1) * 100\n",
    "\n",
    "# Count the number of rows with missing values up to percent\n",
    "missing_rows = (missing_percentage_per_row <= percent).sum()\n",
    "\n",
    "# Get the total number of rows and columns\n",
    "total_rows, total_columns = o3_X_train.shape\n",
    "\n",
    "percent_between = missing_rows*100/total_rows\n",
    "\n",
    "# Display the results\n",
    "print(f\"Total Rows: {total_rows}, Total Columns: {total_columns}\")\n",
    "print(f\"Number of rows with missing values up to {percent} %: {missing_rows}\")\n",
    "print (f\"The percentage between total and missing values sets are {percent_between:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f89d87-6062-4362-8b2d-61f9bff2c2a3",
   "metadata": {},
   "source": [
    "# GAN Imputation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded01e32-e260-4c16-9fd4-a2fefcde62c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for PyTorch\n",
    "def prepare_dataset(data):\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, data):\n",
    "            # Convert to float32 tensor\n",
    "            self.data = torch.tensor(data.astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx]\n",
    "    \n",
    "    return CustomDataset(data)\n",
    "\n",
    "# Define Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# Define Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc2ae1d-18f8-4242-b98f-8cfb68d7f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN Imputation Function\n",
    "def gan_impute_method(X, num_passes=3, epochs=1000, batch_size=64, learning_rate=0.0002, patience=10):\n",
    "\n",
    "    X_imputed = X.copy()\n",
    "    # Create a mask of missing values\n",
    "    mask = X_imputed.isna()\n",
    "    # Fill missing values with 0 as a starting point\n",
    "    X_imputed.fillna(0, inplace=True)\n",
    "    \n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    for pass_num in range(num_passes):\n",
    "        logging.info(f\"GAN Imputation Pass {pass_num + 1}/{num_passes}\")\n",
    "        \n",
    "        # Prepare dataset and dataloader\n",
    "        dataset = prepare_dataset(X_imputed.values)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Initialize Generator and Discriminator\n",
    "        generator = Generator(input_dim)\n",
    "        discriminator = Discriminator(input_dim)\n",
    "        \n",
    "        optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "        optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "        adversarial_loss = nn.BCELoss()\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for i, real_data in enumerate(dataloader):\n",
    "                # Prepare labels\n",
    "                valid = torch.ones(real_data.size(0), 1)\n",
    "                fake = torch.zeros(real_data.size(0), 1)\n",
    "                \n",
    "                # Train Generator\n",
    "                optimizer_G.zero_grad()\n",
    "                gen_data = generator(real_data)\n",
    "                \n",
    "                # Get the corresponding mask for this batch.\n",
    "                # Assume the DataLoader returns rows in order.\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, len(X_imputed))\n",
    "                batch_mask = torch.tensor(mask.iloc[start_idx:end_idx].values, dtype=torch.bool)\n",
    "                \n",
    "                # Do not update the values that were originally observed.\n",
    "                gen_data[batch_mask] = real_data[batch_mask]\n",
    "                \n",
    "                g_loss = adversarial_loss(discriminator(gen_data), valid)\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "                \n",
    "                # Train Discriminator\n",
    "                optimizer_D.zero_grad()\n",
    "                real_loss = adversarial_loss(discriminator(real_data), valid)\n",
    "                fake_loss = adversarial_loss(discriminator(gen_data.detach()), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "            \n",
    "            logging.info(f\"Epoch {epoch + 1}/{epochs} - G_loss: {g_loss.item():.4f}, D_loss: {d_loss.item():.4f}\")\n",
    "            \n",
    "            # Early stopping based on generator loss\n",
    "            if g_loss.item() < best_loss:\n",
    "                best_loss = g_loss.item()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                logging.info(f\"Early stopping at epoch {epoch + 1} with G_loss: {g_loss.item():.4f}\")\n",
    "                break\n",
    "        \n",
    "        # After training, update the missing values\n",
    "        X_imputed_tensor = torch.tensor(X_imputed.values.astype(np.float32), dtype=torch.float32)\n",
    "        refined_data = generator(X_imputed_tensor).detach().numpy()\n",
    "        # Only update the positions where data was missing\n",
    "        X_imputed.values[mask.values] = refined_data[mask.values]\n",
    "    \n",
    "    return pd.DataFrame(X_imputed, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e613b824-6927-4ed3-a530-93fd21f9cac8",
   "metadata": {},
   "source": [
    "# Iterative Imputation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9297594-c420-4ac7-90d3-025cdca26942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Impute using IterativeImputer with an ExtraTreesRegressor.\n",
    "Returns a DataFrame with imputed values.\n",
    "\"\"\"\n",
    "def iterative_impute_method(data, random_state=0):\n",
    "    base_estimator = ExtraTreesRegressor(n_estimators=10, random_state=random_state)\n",
    "    imputer = IterativeImputer(random_state=random_state, estimator=base_estimator)\n",
    "    imputed = pd.DataFrame(imputer.fit_transform(data),\n",
    "                           index=data.index,\n",
    "                           columns=data.columns)\n",
    "    return imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509df751-f446-44fd-a1c4-0ec9ca3ce300",
   "metadata": {},
   "source": [
    "# Hierarchical Imputation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf398949-d5b3-4766-9a0e-11c915fd6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic hierarchical imputation function\n",
    "\n",
    "\"\"\"\n",
    "Perform hierarchical imputation by splitting rows into groups based on a dynamic set of thresholds.\n",
    "    \n",
    "Parameters:\n",
    "\n",
    "      thresholds : List of thresholds that specify group widths (e.g., [0.1, 0.1, 0.8] for three groups),\n",
    "                      or [0.05, 0.05, 0.1, 0.1, 0.7]\n",
    "                      or [0.05]*20.\n",
    "                      These will be converted to cumulative thresholds. No matter what the sum of the\n",
    "                      thresholds must be 1\n",
    "      estimator  : Optional. An estimator for IterativeImputer. If None, defaults to ExtraTreesRegressor.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def hierarchical_impute_dynamic(df, thresholds, methods, random_state=0):\n",
    "    if len(thresholds) != len(methods):\n",
    "        raise ValueError(\"The number of thresholds must equal the number of methods provided.\")\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    # Compute the missing percentage for each row.\n",
    "    df_copy['missing_pct'] = df_copy.isnull().mean(axis=1)\n",
    "    cols = df_copy.columns.drop('missing_pct')\n",
    "    \n",
    "    # Compute global means for each column.\n",
    "    global_means = df_copy[cols].mean().fillna(0)\n",
    "    \n",
    "    # Prepare an empty DataFrame for imputed results.\n",
    "    imputed_df = pd.DataFrame(index=df_copy.index, columns=cols)\n",
    "    \n",
    "    # Compute cumulative thresholds.\n",
    "    cum_thresholds = np.cumsum(thresholds)\n",
    "    if not np.isclose(cum_thresholds[-1], 1.0):\n",
    "        raise ValueError(\"The sum of thresholds must be 1.0 (or very close to it).\")\n",
    "    \n",
    "    previous_imputed = None\n",
    "    \n",
    "    # Process each group in sequence.\n",
    "    for i, upper_bound in enumerate(cum_thresholds):\n",
    "        if i == 0:\n",
    "            lower_bound = 0.0\n",
    "            idx = df_copy.index[df_copy['missing_pct'] <= upper_bound]\n",
    "        else:\n",
    "            lower_bound = cum_thresholds[i - 1]\n",
    "            idx = df_copy.index[(df_copy['missing_pct'] > lower_bound) & (df_copy['missing_pct'] <= upper_bound)]\n",
    "        \n",
    "        group_data = df_copy.loc[idx, cols].copy()\n",
    "        # For any column completely missing in this group, fill with the global mean.\n",
    "        for col in group_data.columns:\n",
    "            if group_data[col].isnull().all():\n",
    "                group_data[col] = global_means[col]\n",
    "        \n",
    "        # Count the total missing values in the current group.\n",
    "        missing_count = group_data.isnull().sum().sum()\n",
    "        \n",
    "        logging.info(f\"Group {i+1} (missing_pct in ({lower_bound:.2f}, {upper_bound:.2f}]): \"\n",
    "                     f\"{group_data.shape[0]} rows, {missing_count} missing values\")\n",
    "        \n",
    "        if group_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # Combine previously imputed rows (if any) with current group data.\n",
    "        if previous_imputed is None:\n",
    "            combined = group_data\n",
    "        else:\n",
    "            combined = pd.concat([previous_imputed, group_data])\n",
    "        \n",
    "        # Apply the selected imputation method for this group.\n",
    "        current_method = methods[i]\n",
    "        try:\n",
    "            combined_imputed = current_method(combined, random_state=random_state)\n",
    "        except TypeError:\n",
    "            combined_imputed = current_method(combined)\n",
    "        \n",
    "        # Extract imputed values corresponding to the current group.\n",
    "        group_imputed = combined_imputed.loc[idx]\n",
    "        imputed_df.loc[idx] = group_imputed\n",
    "        \n",
    "        # Update previous_imputed by combining with the newly imputed rows.\n",
    "        if previous_imputed is None:\n",
    "            previous_imputed = group_imputed.copy()\n",
    "        else:\n",
    "            previous_imputed = pd.concat([previous_imputed, group_imputed])\n",
    "    \n",
    "    if imputed_df.isnull().values.any():\n",
    "        raise ValueError(\"NaN values remain after hierarchical imputation!\")\n",
    "    \n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6696c-4003-4828-90cc-ecd4b8c224ec",
   "metadata": {},
   "source": [
    "# Set Up Thresholds and Imputation Methods we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee62afd-4fdd-4bd0-96a2-95336cae56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dynamic_thresholds : List of thresholds that specify group widths\n",
    "                    e.g. [0.1, 0.1, 0.8]\n",
    "                    or   [0.05, 0.05, 0.1, 0.1, 0.7]\n",
    "                    or   [0.05]*20.\n",
    "                    These will be converted to cumulative thresholds.\n",
    "                    No matter what the sum of the thresholds must be 1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Set up threshold\n",
    "dynamic_thresholds = [0.04] * 25\n",
    "\n",
    "# Setting Up which methon we are going to use for every threshold.\n",
    "# The number of methods must be the same as the number of thresholds.\n",
    "dynamic_methods = [iterative_impute_method] * 7 + [gan_impute_method] * 18\n",
    "\n",
    "\"\"\"\n",
    "# List of dataset names\n",
    "datasets_to_impute = [\n",
    "    \"o1_X_train\", \"o2_X_train\", \"o3_X_train\", \"o4_X_train\",\n",
    "    \"o1_X_test\", \"o2_X_test\", \"o3_X_test\", \"o4_X_test\",\n",
    "    \"o1_X_validate\", \"o2_X_validate\", \"o3_X_validate\", \"o4_X_validate\",\n",
    "    \"o1_X_external\", \"o2_X_external\", \"o3_X_external\", \"o4_X_external\"\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# List of dataset names\n",
    "datasets_to_impute = [\n",
    "    \"o3_X_train\",\n",
    "    \"o3_X_test\",\n",
    "    \"o3_X_validate\",\n",
    "    \"o3_X_external\"\n",
    "]\n",
    "\n",
    "# Create the save path.\n",
    "output_path = '../CSV/exports/impute/o4_Hierarchical/'\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265e543-37a8-44fa-b5ac-4e07bb0e7807",
   "metadata": {},
   "source": [
    "# Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cde5cf-756b-4b4d-a274-bead25a98673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 19:06:34,766 - INFO - Processing dataset: o3_X_train\n",
      "2025-02-10 19:06:35,309 - INFO - Group 1 (missing_pct in (0.00, 0.04]): 0 rows, 0 missing values\n",
      "2025-02-10 19:06:35,383 - INFO - Group 2 (missing_pct in (0.04, 0.08]): 16 rows, 0 missing values\n",
      "2025-02-10 19:06:39,566 - INFO - Group 3 (missing_pct in (0.08, 0.12]): 160 rows, 5440 missing values\n",
      "C:\\Users\\Dimopoulos\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "2025-02-10 19:08:47,644 - INFO - Group 4 (missing_pct in (0.12, 0.16]): 560 rows, 27584 missing values\n",
      "C:\\Users\\Dimopoulos\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "2025-02-10 19:17:43,257 - INFO - Group 5 (missing_pct in (0.16, 0.20]): 2112 rows, 131712 missing values\n",
      "C:\\Users\\Dimopoulos\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "2025-02-10 19:57:53,701 - INFO - Group 6 (missing_pct in (0.20, 0.24]): 2688 rows, 205824 missing values\n",
      "C:\\Users\\Dimopoulos\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "2025-02-10 21:41:34,201 - INFO - Group 7 (missing_pct in (0.24, 0.28]): 3840 rows, 346816 missing values\n"
     ]
    }
   ],
   "source": [
    "imputed_datasets = {}\n",
    "# Assuming your CSVs have already been loaded into a dictionary called 'dataframes'\n",
    "for dataset_name in datasets_to_impute:\n",
    "    if dataset_name in globals().get(\"dataframes\", {}):\n",
    "        logging.info(f\"Processing dataset: {dataset_name}\")\n",
    "        df = dataframes[dataset_name]\n",
    "        try:\n",
    "            imputed_df = hierarchical_impute_dynamic(df, thresholds=dynamic_thresholds, methods=dynamic_methods, random_state=0)\n",
    "            if imputed_df.isnull().values.any():\n",
    "                raise ValueError(f\"NaN values found in dataset {dataset_name} after imputation.\")\n",
    "            imputed_datasets[dataset_name] = imputed_df\n",
    "            output_file = os.path.join(output_path, f\"{dataset_name}.csv\")\n",
    "            imputed_df.to_csv(output_file, index=False)\n",
    "            logging.info(f\"Imputed dataset saved as {output_file}\")\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"Error in {dataset_name}: {e}\")\n",
    "    else:\n",
    "        logging.warning(f\"Dataset {dataset_name} not found!\")\n",
    "\n",
    "logging.info(\"All datasets have been imputed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b14e6-d152-4fe4-bfb9-cd9c7288623c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
