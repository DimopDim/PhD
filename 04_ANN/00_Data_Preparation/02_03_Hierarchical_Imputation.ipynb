{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e3b069-e7e7-4b55-9e4b-bd257e4e673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7a7c9b-3629-4077-bcea-d58a9cc0472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"data_loading.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e6cfc3-fc60-45b2-af0b-46c147bfd09d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 11:33:59,055 - INFO - Loading... -> o1_X_external.csv\n",
      "2025-02-01 11:34:06,469 - INFO - Loading... -> o1_X_test.csv\n",
      "2025-02-01 11:34:07,013 - INFO - Loading... -> o1_X_train.csv\n",
      "2025-02-01 11:34:11,347 - INFO - Loading... -> o1_X_validate.csv\n",
      "2025-02-01 11:34:11,903 - INFO - Loading... -> o1_y_external_los.csv\n",
      "2025-02-01 11:34:11,945 - INFO - Loading... -> o1_y_external_mortality.csv\n",
      "2025-02-01 11:34:11,976 - INFO - Loading... -> o1_y_test_los.csv\n",
      "2025-02-01 11:34:11,986 - INFO - Loading... -> o1_y_test_mortality.csv\n",
      "2025-02-01 11:34:11,991 - INFO - Loading... -> o1_y_train_los.csv\n",
      "2025-02-01 11:34:12,031 - INFO - Loading... -> o1_y_train_mortality.csv\n",
      "2025-02-01 11:34:12,049 - INFO - Loading... -> o1_y_validate_los.csv\n",
      "2025-02-01 11:34:12,059 - INFO - Loading... -> o1_y_validate_mortality.csv\n",
      "2025-02-01 11:34:12,064 - INFO - Loading... -> o2_X_external.csv\n",
      "2025-02-01 11:34:15,749 - INFO - Loading... -> o2_X_test.csv\n",
      "2025-02-01 11:34:16,030 - INFO - Loading... -> o2_X_train.csv\n",
      "2025-02-01 11:34:18,241 - INFO - Loading... -> o2_X_validate.csv\n",
      "2025-02-01 11:34:18,512 - INFO - Loading... -> o2_y_external_los.csv\n",
      "2025-02-01 11:34:18,534 - INFO - Loading... -> o2_y_external_mortality.csv\n",
      "2025-02-01 11:34:18,550 - INFO - Loading... -> o2_y_test_los.csv\n",
      "2025-02-01 11:34:18,556 - INFO - Loading... -> o2_y_test_mortality.csv\n",
      "2025-02-01 11:34:18,562 - INFO - Loading... -> o2_y_train_los.csv\n",
      "2025-02-01 11:34:18,585 - INFO - Loading... -> o2_y_train_mortality.csv\n",
      "2025-02-01 11:34:18,595 - INFO - Loading... -> o2_y_validate_los.csv\n",
      "2025-02-01 11:34:18,601 - INFO - Loading... -> o2_y_validate_mortality.csv\n",
      "2025-02-01 11:34:18,604 - INFO - Loading... -> o3_X_external.csv\n",
      "2025-02-01 11:34:21,031 - INFO - Loading... -> o3_X_test.csv\n",
      "2025-02-01 11:34:21,240 - INFO - Loading... -> o3_X_train.csv\n",
      "2025-02-01 11:34:22,680 - INFO - Loading... -> o3_X_validate.csv\n",
      "2025-02-01 11:34:22,874 - INFO - Loading... -> o3_y_external_los.csv\n",
      "2025-02-01 11:34:22,889 - INFO - Loading... -> o3_y_external_mortality.csv\n",
      "2025-02-01 11:34:22,902 - INFO - Loading... -> o3_y_test_los.csv\n",
      "2025-02-01 11:34:22,906 - INFO - Loading... -> o3_y_test_mortality.csv\n",
      "2025-02-01 11:34:22,909 - INFO - Loading... -> o3_y_train_los.csv\n",
      "2025-02-01 11:34:22,925 - INFO - Loading... -> o3_y_train_mortality.csv\n",
      "2025-02-01 11:34:22,932 - INFO - Loading... -> o3_y_validate_los.csv\n",
      "2025-02-01 11:34:22,937 - INFO - Loading... -> o3_y_validate_mortality.csv\n",
      "2025-02-01 11:34:22,940 - INFO - Loading... -> o4_X_external.csv\n",
      "2025-02-01 11:34:24,810 - INFO - Loading... -> o4_X_test.csv\n",
      "2025-02-01 11:34:24,965 - INFO - Loading... -> o4_X_train.csv\n",
      "2025-02-01 11:34:26,034 - INFO - Loading... -> o4_X_validate.csv\n",
      "2025-02-01 11:34:26,198 - INFO - Loading... -> o4_y_external_los.csv\n",
      "2025-02-01 11:34:26,213 - INFO - Loading... -> o4_y_external_mortality.csv\n",
      "2025-02-01 11:34:26,222 - INFO - Loading... -> o4_y_test_los.csv\n",
      "2025-02-01 11:34:26,227 - INFO - Loading... -> o4_y_test_mortality.csv\n",
      "2025-02-01 11:34:26,230 - INFO - Loading... -> o4_y_train_los.csv\n",
      "2025-02-01 11:34:26,244 - INFO - Loading... -> o4_y_train_mortality.csv\n",
      "2025-02-01 11:34:26,251 - INFO - Loading... -> o4_y_validate_los.csv\n",
      "2025-02-01 11:34:26,255 - INFO - Loading... -> o4_y_validate_mortality.csv\n",
      "2025-02-01 11:34:26,258 - INFO - o1_X_external loaded successfully with shape (234720, 345)\n",
      "2025-02-01 11:34:26,259 - INFO - o1_X_test loaded successfully with shape (15312, 345)\n",
      "2025-02-01 11:34:26,260 - INFO - o1_X_train loaded successfully with shape (122496, 345)\n",
      "2025-02-01 11:34:26,261 - INFO - o1_X_validate loaded successfully with shape (15312, 345)\n",
      "2025-02-01 11:34:26,262 - INFO - o1_y_external_los loaded successfully with shape (234720, 1)\n",
      "2025-02-01 11:34:26,263 - INFO - o1_y_external_mortality loaded successfully with shape (234720, 1)\n",
      "2025-02-01 11:34:26,264 - INFO - o1_y_test_los loaded successfully with shape (15312, 1)\n",
      "2025-02-01 11:34:26,264 - INFO - o1_y_test_mortality loaded successfully with shape (15312, 1)\n",
      "2025-02-01 11:34:26,265 - INFO - o1_y_train_los loaded successfully with shape (122496, 1)\n",
      "2025-02-01 11:34:26,266 - INFO - o1_y_train_mortality loaded successfully with shape (122496, 1)\n",
      "2025-02-01 11:34:26,267 - INFO - o1_y_validate_los loaded successfully with shape (15312, 1)\n",
      "2025-02-01 11:34:26,267 - INFO - o1_y_validate_mortality loaded successfully with shape (15312, 1)\n",
      "2025-02-01 11:34:26,268 - INFO - o2_X_external loaded successfully with shape (117360, 345)\n",
      "2025-02-01 11:34:26,269 - INFO - o2_X_test loaded successfully with shape (7656, 345)\n",
      "2025-02-01 11:34:26,270 - INFO - o2_X_train loaded successfully with shape (61248, 345)\n",
      "2025-02-01 11:34:26,270 - INFO - o2_X_validate loaded successfully with shape (7656, 345)\n",
      "2025-02-01 11:34:26,272 - INFO - o2_y_external_los loaded successfully with shape (117360, 1)\n",
      "2025-02-01 11:34:26,273 - INFO - o2_y_external_mortality loaded successfully with shape (117360, 1)\n",
      "2025-02-01 11:34:26,273 - INFO - o2_y_test_los loaded successfully with shape (7656, 1)\n",
      "2025-02-01 11:34:26,274 - INFO - o2_y_test_mortality loaded successfully with shape (7656, 1)\n",
      "2025-02-01 11:34:26,275 - INFO - o2_y_train_los loaded successfully with shape (61248, 1)\n",
      "2025-02-01 11:34:26,276 - INFO - o2_y_train_mortality loaded successfully with shape (61248, 1)\n",
      "2025-02-01 11:34:26,277 - INFO - o2_y_validate_los loaded successfully with shape (7656, 1)\n",
      "2025-02-01 11:34:26,279 - INFO - o2_y_validate_mortality loaded successfully with shape (7656, 1)\n",
      "2025-02-01 11:34:26,280 - INFO - o3_X_external loaded successfully with shape (78240, 345)\n",
      "2025-02-01 11:34:26,281 - INFO - o3_X_test loaded successfully with shape (5104, 345)\n",
      "2025-02-01 11:34:26,282 - INFO - o3_X_train loaded successfully with shape (40832, 345)\n",
      "2025-02-01 11:34:26,283 - INFO - o3_X_validate loaded successfully with shape (5104, 345)\n",
      "2025-02-01 11:34:26,283 - INFO - o3_y_external_los loaded successfully with shape (78240, 1)\n",
      "2025-02-01 11:34:26,284 - INFO - o3_y_external_mortality loaded successfully with shape (78240, 1)\n",
      "2025-02-01 11:34:26,285 - INFO - o3_y_test_los loaded successfully with shape (5104, 1)\n",
      "2025-02-01 11:34:26,286 - INFO - o3_y_test_mortality loaded successfully with shape (5104, 1)\n",
      "2025-02-01 11:34:26,287 - INFO - o3_y_train_los loaded successfully with shape (40832, 1)\n",
      "2025-02-01 11:34:26,288 - INFO - o3_y_train_mortality loaded successfully with shape (40832, 1)\n",
      "2025-02-01 11:34:26,288 - INFO - o3_y_validate_los loaded successfully with shape (5104, 1)\n",
      "2025-02-01 11:34:26,290 - INFO - o3_y_validate_mortality loaded successfully with shape (5104, 1)\n",
      "2025-02-01 11:34:26,290 - INFO - o4_X_external loaded successfully with shape (58680, 345)\n",
      "2025-02-01 11:34:26,291 - INFO - o4_X_test loaded successfully with shape (3828, 345)\n",
      "2025-02-01 11:34:26,292 - INFO - o4_X_train loaded successfully with shape (30624, 345)\n",
      "2025-02-01 11:34:26,294 - INFO - o4_X_validate loaded successfully with shape (3828, 345)\n",
      "2025-02-01 11:34:26,295 - INFO - o4_y_external_los loaded successfully with shape (58680, 1)\n",
      "2025-02-01 11:34:26,296 - INFO - o4_y_external_mortality loaded successfully with shape (58680, 1)\n",
      "2025-02-01 11:34:26,297 - INFO - o4_y_test_los loaded successfully with shape (3828, 1)\n",
      "2025-02-01 11:34:26,298 - INFO - o4_y_test_mortality loaded successfully with shape (3828, 1)\n",
      "2025-02-01 11:34:26,298 - INFO - o4_y_train_los loaded successfully with shape (30624, 1)\n",
      "2025-02-01 11:34:26,299 - INFO - o4_y_train_mortality loaded successfully with shape (30624, 1)\n",
      "2025-02-01 11:34:26,300 - INFO - o4_y_validate_los loaded successfully with shape (3828, 1)\n",
      "2025-02-01 11:34:26,301 - INFO - o4_y_validate_mortality loaded successfully with shape (3828, 1)\n",
      "2025-02-01 11:34:26,302 - INFO - Load Complete.\n"
     ]
    }
   ],
   "source": [
    "# CSVs Directory \n",
    "data_path = \"../CSV/exports/split_set/without_multiple_rows\"\n",
    "all_files = os.listdir(data_path)\n",
    "\n",
    "# Load CSVs into a dictionary of dataframes\n",
    "dataframes = {}\n",
    "for file in all_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        var_name = file.replace(\".csv\", \"\").replace(\"-\", \"_\")\n",
    "        logging.info(f\"Loading... -> {file}\")\n",
    "        dataframes[var_name] = pd.read_csv(os.path.join(data_path, file)).astype('float32')\n",
    "\n",
    "# Log loaded datasets\n",
    "for var_name, df in dataframes.items():\n",
    "    globals()[var_name] = df  # if you need them as global variables\n",
    "    logging.info(f\"{var_name} loaded successfully with shape {df.shape}\")\n",
    "logging.info(\"Load Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e06054-8450-486c-90ad-c4bf74bf6202",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(o1_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf398949-d5b3-4766-9a0e-11c915fd6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform hierarchical imputation on a dataframe by splitting rows into groups based on missingness.\n",
    "    \n",
    "Groups:\n",
    "    - Group 1: missing percentage <= thresholds[0]\n",
    "    - Group 2: thresholds[0] < missing percentage <= thresholds[1]\n",
    "    - Group 3: missing percentage > thresholds[1]\n",
    "       \n",
    "Returns:\n",
    "    df_imputed: DataFrame with imputed values.\n",
    "\"\"\"\n",
    "\n",
    "def hierarchical_impute(df, thresholds=[0.10, 0.20, 1.00]):\n",
    "    df_copy = df.copy()\n",
    "    # Compute the percentage of missing values for each row\n",
    "    df_copy['missing_pct'] = df_copy.isnull().mean(axis=1)\n",
    "    # Get the original columns (excluding our helper column)\n",
    "    cols = df_copy.columns.drop('missing_pct')\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute global means for each column (from the entire dataframe)\n",
    "    We want that because there are columns beween sets that are all NaN\n",
    "    and if so we replace them with 0.\n",
    "    \"\"\"\n",
    "    global_means = df_copy[cols].mean()\n",
    "    # In case any global mean is still NaN (column entirely missing in the full dataset), replace with 0\n",
    "    global_means = global_means.fillna(0)\n",
    "    \n",
    "    # Create an empty DataFrame to store imputed results\n",
    "    imputed_df = pd.DataFrame(index=df_copy.index, columns=cols)\n",
    "    \n",
    "    # We'll use ExtraTreesRegressor in IterativeImputer to improve numerical stability.\n",
    "    base_estimator = ExtraTreesRegressor(n_estimators=10, random_state=0)\n",
    "    \n",
    "    # ----- Group 1: Rows with <= thresholds[0] missing values -----\n",
    "    idx_group1 = df_copy.index[df_copy['missing_pct'] <= thresholds[0]]\n",
    "    group1 = df_copy.loc[idx_group1, cols].copy()\n",
    "    \n",
    "    # For any column that is completely missing in group1, we fill it with the global mean\n",
    "    for col in group1.columns:\n",
    "        if group1[col].isnull().all():\n",
    "            group1.loc[:, col] = global_means[col]\n",
    "    if not group1.empty:\n",
    "        logging.info(f\"Group 1 (<= {thresholds[0]*100:.0f}% missing): {group1.shape[0]} rows\")\n",
    "        imputer1 = IterativeImputer(random_state=0, estimator=base_estimator)\n",
    "        group1_imputed = pd.DataFrame(imputer1.fit_transform(group1),\n",
    "                                      index=group1.index,\n",
    "                                      columns=group1.columns)\n",
    "        imputed_df.loc[idx_group1] = group1_imputed\n",
    "    else:\n",
    "        group1_imputed = pd.DataFrame()\n",
    "    \n",
    "    # ----- Group 2: Rows with > thresholds[0] and <= thresholds[1] missing values -----\n",
    "    idx_group2 = df_copy.index[(df_copy['missing_pct'] > thresholds[0]) & (df_copy['missing_pct'] <= thresholds[1])]\n",
    "    group2 = df_copy.loc[idx_group2, cols].copy()\n",
    "    for col in group2.columns:\n",
    "        if group2[col].isnull().all():\n",
    "            group2.loc[:, col] = global_means[col]\n",
    "    if not group2.empty:\n",
    "        logging.info(f\"Group 2 (> {thresholds[0]*100:.0f}% and <= {thresholds[1]*100:.0f}% missing): {group2.shape[0]} rows\")\n",
    "        # Combine already imputed group1 rows with group2 rows\n",
    "        combined = pd.concat([group1_imputed, group2])\n",
    "        imputer2 = IterativeImputer(random_state=0, estimator=base_estimator)\n",
    "        combined_imputed = pd.DataFrame(imputer2.fit_transform(combined),\n",
    "                                        index=combined.index,\n",
    "                                        columns=combined.columns)\n",
    "        # Extract the imputed values for group2\n",
    "        group2_imputed = combined_imputed.loc[idx_group2]\n",
    "        imputed_df.loc[idx_group2] = group2_imputed\n",
    "    else:\n",
    "        logging.info(\"No rows found for Group 2.\")\n",
    "    \n",
    "    # ----- Group 3: Rows with > thresholds[1] missing values -----\n",
    "    idx_group3 = df_copy.index[df_copy['missing_pct'] > thresholds[1]]\n",
    "    group3 = df_copy.loc[idx_group3, cols].copy()\n",
    "    for col in group3.columns:\n",
    "        if group3[col].isnull().all():\n",
    "            group3.loc[:, col] = global_means[col]\n",
    "    if not group3.empty:\n",
    "        logging.info(f\"Group 3 (> {thresholds[1]*100:.0f}% missing): {group3.shape[0]} rows\")\n",
    "        # Combine already imputed rows (Group 1 & 2) with Group 3 rows\n",
    "        imputed_so_far = imputed_df.loc[idx_group1.union(idx_group2)]\n",
    "        combined2 = pd.concat([imputed_so_far, group3])\n",
    "        imputer3 = IterativeImputer(random_state=0, estimator=base_estimator)\n",
    "        combined2_imputed = pd.DataFrame(imputer3.fit_transform(combined2),\n",
    "                                         index=combined2.index,\n",
    "                                         columns=combined2.columns)\n",
    "        # Extract imputed rows for group3\n",
    "        group3_imputed = combined2_imputed.loc[idx_group3]\n",
    "        imputed_df.loc[idx_group3] = group3_imputed\n",
    "    else:\n",
    "        logging.info(\"No rows found for Group 3.\")\n",
    "    \n",
    "    # Final check: if any NaN remains, raise an error\n",
    "    if imputed_df.isnull().values.any():\n",
    "        raise ValueError(\"NaN values remain after hierarchical imputation!\")\n",
    "    \n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb977ca-e156-4b48-9ddc-6a9ac5885b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 11:35:01,855 - INFO - Processing dataset: o1_X_train\n",
      "2025-02-01 11:35:03,027 - INFO - Group 1 (<= 10% missing): 48 rows\n",
      "2025-02-01 11:35:07,623 - INFO - Group 2 (> 10% and <= 20% missing): 8496 rows\n",
      "C:\\Users\\Dimopoulos\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "2025-02-01 13:43:03,593 - INFO - Group 3 (> 20% missing): 113952 rows\n"
     ]
    }
   ],
   "source": [
    "# List of datasets\n",
    "datasets_to_impute = [\n",
    "    \"o1_X_train\", \"o2_X_train\", \"o3_X_train\", \"o4_X_train\",\n",
    "    \"o1_X_test\", \"o2_X_test\", \"o3_X_test\", \"o4_X_test\",\n",
    "    \"o1_X_validate\", \"o2_X_validate\", \"o3_X_validate\", \"o4_X_validate\",\n",
    "    \"o1_X_external\", \"o2_X_external\", \"o3_X_external\", \"o4_X_external\"\n",
    "]\n",
    "\n",
    "# Set the output path for imputed CSVs\n",
    "output_path = '../CSV/exports/impute/hierarchical/'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Process and impute each dataset using the hierarchical strategy\n",
    "imputed_datasets = {}\n",
    "for dataset_name in datasets_to_impute:\n",
    "    if dataset_name in dataframes:\n",
    "        logging.info(f\"Processing dataset: {dataset_name}\")\n",
    "        df = dataframes[dataset_name]\n",
    "        try:\n",
    "            imputed_df = hierarchical_impute(df, thresholds=[0.10, 0.20, 1.00])\n",
    "            \n",
    "            # Validate no NaN values remain\n",
    "            if imputed_df.isnull().values.any():\n",
    "                raise ValueError(f\"NaN values found in dataset {dataset_name} after imputation.\")\n",
    "            \n",
    "            imputed_datasets[dataset_name] = imputed_df\n",
    "            \n",
    "            # Save the imputed dataset\n",
    "            output_file = os.path.join(output_path, f\"{dataset_name}.csv\")\n",
    "            imputed_df.to_csv(output_file, index=False)\n",
    "            logging.info(f\"Imputed dataset saved as {output_file}\")\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"Error in {dataset_name}: {e}\")\n",
    "    else:\n",
    "        logging.warning(f\"Dataset {dataset_name} not found!\")\n",
    "\n",
    "logging.info(\"All datasets have been imputed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
