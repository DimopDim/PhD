{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6243b02-165d-496a-a198-f00704a71eb6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6671463d-f9c0-4542-b28c-eec0dfb36e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43925465-ed65-4369-80e9-24640b9d0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"data_loading.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25beddeb-cbdd-4fc6-bcd2-dc740997c4da",
   "metadata": {},
   "source": [
    "# Reads | Filter Patients (Phase 01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a182eba9-001e-43e2-b928-7bdd96cf667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 16:29:15,349 - INFO - Successfully read ../CSV/exports/whole_set/o1_hour_overlap_window_mimic.csv into variable o1_mimic\n",
      "2024-12-15 16:29:22,483 - INFO - Successfully read ../CSV/exports/whole_set/o1_hour_overlap_window_eicu.csv into variable o1_eicu\n",
      "2024-12-15 16:29:25,074 - INFO - Successfully read ../CSV/exports/whole_set/o2_hour_overlap_window_mimic.csv into variable o2_mimic\n",
      "2024-12-15 16:29:28,630 - INFO - Successfully read ../CSV/exports/whole_set/o2_hour_overlap_window_eicu.csv into variable o2_eicu\n",
      "2024-12-15 16:29:30,373 - INFO - Successfully read ../CSV/exports/whole_set/o3_hour_overlap_window_mimic.csv into variable o3_mimic\n",
      "2024-12-15 16:29:32,711 - INFO - Successfully read ../CSV/exports/whole_set/o3_hour_overlap_window_eicu.csv into variable o3_eicu\n",
      "2024-12-15 16:29:34,070 - INFO - Successfully read ../CSV/exports/whole_set/o4_hour_overlap_window_mimic.csv into variable o4_mimic\n",
      "2024-12-15 16:29:35,870 - INFO - Successfully read ../CSV/exports/whole_set/o4_hour_overlap_window_eicu.csv into variable o4_eicu\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    # File paths\n",
    "    eicu_file = f\"../CSV/exports/whole_set/o{i}_hour_overlap_window_eicu.csv\"\n",
    "    mimic_file = f\"../CSV/exports/whole_set/o{i}_hour_overlap_window_mimic.csv\"\n",
    "    \n",
    "    # Variable names\n",
    "    eicu_var_name = f\"o{i}_eicu\"\n",
    "    mimic_var_name = f\"o{i}_mimic\"\n",
    "    \n",
    "    try:\n",
    "        # Read MIMIC file and assign to a variable\n",
    "        globals()[mimic_var_name] = pd.read_csv(mimic_file)\n",
    "        logging.info(f\"Successfully read {mimic_file} into variable {mimic_var_name}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.info(f\"{mimic_file} not found.\")\n",
    "    except Exception as e:\n",
    "        logging.info(f\"An error occurred while reading {mimic_file}: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Read eICU file and assign to a variable\n",
    "        globals()[eicu_var_name] = pd.read_csv(eicu_file)\n",
    "        logging.info(f\"Successfully read {eicu_file} into variable {eicu_var_name}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.info(f\"{eicu_file} not found.\")\n",
    "    except Exception as e:\n",
    "        logging.info(f\"An error occurred while reading {eicu_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f2f17d-c77d-4f99-bdfd-50e44a076ded",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making all the dataset to have the same rows by multiplication.\n",
    "logging.info(\"Starting row multiplication for all mimic and eicu datasets.\")\n",
    "\n",
    "# Store mimic and eicu dataframes in separate dictionaries\n",
    "mimic_dataframes = {\n",
    "    \"o2_mimic\": o2_mimic,\n",
    "    \"o3_mimic\": o3_mimic,\n",
    "    \"o4_mimic\": o4_mimic,\n",
    "}\n",
    "\n",
    "eicu_dataframes = {\n",
    "    \"o2_eicu\": o2_eicu,\n",
    "    \"o3_eicu\": o3_eicu,\n",
    "    \"o4_eicu\": o4_eicu,\n",
    "}\n",
    "\n",
    "# Multiply rows for mimic datasets\n",
    "for i in range(2, 5):\n",
    "    df_name = f\"o{i}_mimic\"\n",
    "    logging.info(f\"Processing mimic dataframe: {df_name}, multiplying rows by {i}.\")\n",
    "    mimic_dataframes[df_name] = mimic_dataframes[df_name].loc[mimic_dataframes[df_name].index.repeat(i)].reset_index(drop=True)\n",
    "    logging.info(f\"Completed multiplication for {df_name}. New row count: {len(mimic_dataframes[df_name])}.\")\n",
    "\n",
    "# Multiply rows for eicu datasets\n",
    "for i in range(2, 5):\n",
    "    df_name = f\"o{i}_eicu\"\n",
    "    logging.info(f\"Processing eicu dataframe: {df_name}, multiplying rows by {i}.\")\n",
    "    eicu_dataframes[df_name] = eicu_dataframes[df_name].loc[eicu_dataframes[df_name].index.repeat(i)].reset_index(drop=True)\n",
    "    logging.info(f\"Completed multiplication for {df_name}. New row count: {len(eicu_dataframes[df_name])}.\")\n",
    "\n",
    "# Access the modified mimic and eicu dataframes\n",
    "o2_mimic = mimic_dataframes[\"o2_mimic\"]\n",
    "o3_mimic = mimic_dataframes[\"o3_mimic\"]\n",
    "o4_mimic = mimic_dataframes[\"o4_mimic\"]\n",
    "\n",
    "o2_eicu = eicu_dataframes[\"o2_eicu\"]\n",
    "o3_eicu = eicu_dataframes[\"o3_eicu\"]\n",
    "o4_eicu = eicu_dataframes[\"o4_eicu\"]\n",
    "\n",
    "# Logging the end of the process\n",
    "logging.info(\"Row multiplication for all mimic and eicu datasets is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc6b27a-0833-4272-8260-1cb5e835b122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 16:30:09,954 - INFO - Processing datasets: o1_mimic and o1_eicu\n",
      "2024-12-15 16:30:09,955 - INFO - Row count of o1_mimic: 174432\n",
      "2024-12-15 16:30:10,304 - INFO - Combined dataset for o1_mimic and o1_eicu created.\n",
      "2024-12-15 16:30:11,900 - INFO - One-hot encoding applied to categorical columns for o1_mimic and o1_eicu.\n",
      "2024-12-15 16:30:11,901 - INFO - Splitting completed for o1_mimic and o1_eicu.\n",
      "2024-12-15 16:30:11,902 - INFO - Processing datasets: o2_mimic and o2_eicu\n",
      "2024-12-15 16:30:11,903 - INFO - Row count of o2_mimic: 87216\n",
      "2024-12-15 16:30:12,166 - INFO - Combined dataset for o2_mimic and o2_eicu created.\n",
      "2024-12-15 16:30:12,997 - INFO - One-hot encoding applied to categorical columns for o2_mimic and o2_eicu.\n",
      "2024-12-15 16:30:12,998 - INFO - Splitting completed for o2_mimic and o2_eicu.\n",
      "2024-12-15 16:30:12,999 - INFO - Processing datasets: o3_mimic and o3_eicu\n",
      "2024-12-15 16:30:13,000 - INFO - Row count of o3_mimic: 58144\n",
      "2024-12-15 16:30:13,187 - INFO - Combined dataset for o3_mimic and o3_eicu created.\n",
      "2024-12-15 16:30:13,751 - INFO - One-hot encoding applied to categorical columns for o3_mimic and o3_eicu.\n",
      "2024-12-15 16:30:13,752 - INFO - Splitting completed for o3_mimic and o3_eicu.\n",
      "2024-12-15 16:30:13,753 - INFO - Processing datasets: o4_mimic and o4_eicu\n",
      "2024-12-15 16:30:13,754 - INFO - Row count of o4_mimic: 43608\n",
      "2024-12-15 16:30:13,932 - INFO - Combined dataset for o4_mimic and o4_eicu created.\n",
      "2024-12-15 16:30:14,489 - INFO - One-hot encoding applied to categorical columns for o4_mimic and o4_eicu.\n",
      "2024-12-15 16:30:14,490 - INFO - Splitting completed for o4_mimic and o4_eicu.\n",
      "2024-12-15 16:30:14,491 - INFO - All datasets have been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I'm gonna concat and split the mimic and icu\n",
    "at this point. I must create the same columns\n",
    "from the tranformation of categorical data.\n",
    "\"\"\"\n",
    "# Store mimic and eicu datasets in dictionaries\n",
    "mimic_dataframes = {\n",
    "    \"o1_mimic\": o1_mimic,\n",
    "    \"o2_mimic\": o2_mimic,\n",
    "    \"o3_mimic\": o3_mimic,\n",
    "    \"o4_mimic\": o4_mimic,\n",
    "}\n",
    "\n",
    "eicu_dataframes = {\n",
    "    \"o1_eicu\": o1_eicu,\n",
    "    \"o2_eicu\": o2_eicu,\n",
    "    \"o3_eicu\": o3_eicu,\n",
    "    \"o4_eicu\": o4_eicu,\n",
    "}\n",
    "\n",
    "# Loop through datasets to concatenate and split\n",
    "combined_results = {}\n",
    "for i in range(1, 5):\n",
    "    mimic_df_name = f\"o{i}_mimic\"\n",
    "    eicu_df_name = f\"o{i}_eicu\"\n",
    "    \n",
    "    logging.info(f\"Processing datasets: {mimic_df_name} and {eicu_df_name}\")\n",
    "    \n",
    "    # Get the mimic and eicu datasets\n",
    "    mimic_df = mimic_dataframes[mimic_df_name]\n",
    "    eicu_df = eicu_dataframes[eicu_df_name]\n",
    "    \n",
    "    # Get the row count of mimic dataset\n",
    "    row_count = mimic_df.shape[0]\n",
    "    logging.info(f\"Row count of {mimic_df_name}: {row_count}\")\n",
    "    \n",
    "    # Concatenate mimic and eicu datasets\n",
    "    df_combined = pd.concat([mimic_df, eicu_df], ignore_index=True)\n",
    "    logging.info(f\"Combined dataset for {mimic_df_name} and {eicu_df_name} created.\")\n",
    "    \n",
    "    # Find categorical columns and apply one-hot encoding\n",
    "    categorical_columns = df_combined.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    df_encoded = pd.get_dummies(df_combined, columns=categorical_columns)\n",
    "    logging.info(f\"One-hot encoding applied to categorical columns for {mimic_df_name} and {eicu_df_name}.\")\n",
    "    \n",
    "    # Split the encoded dataframe back into mimic and eicu datasets\n",
    "    mimic_encoded = df_encoded.iloc[:row_count, :]\n",
    "    eicu_encoded = df_encoded.iloc[row_count:, :]\n",
    "    \n",
    "    # Store the split results\n",
    "    combined_results[mimic_df_name] = mimic_encoded\n",
    "    combined_results[eicu_df_name] = eicu_encoded\n",
    "    \n",
    "    logging.info(f\"Splitting completed for {mimic_df_name} and {eicu_df_name}.\")\n",
    "\n",
    "# Access the modified mimic and eicu dataframes\n",
    "o1_mimic = combined_results[\"o1_mimic\"]\n",
    "o1_eicu = combined_results[\"o1_eicu\"]\n",
    "o2_mimic = combined_results[\"o2_mimic\"]\n",
    "o2_eicu = combined_results[\"o2_eicu\"]\n",
    "o3_mimic = combined_results[\"o3_mimic\"]\n",
    "o3_eicu = combined_results[\"o3_eicu\"]\n",
    "o4_mimic = combined_results[\"o4_mimic\"]\n",
    "o4_eicu = combined_results[\"o4_eicu\"]\n",
    "\n",
    "# Logging the end of the process\n",
    "logging.info(\"All datasets have been processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abae581b-694b-43c7-9805-90856381db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 10\n",
    "\n",
    "# Filter icu stay less than 10 days\n",
    "o1_mimic = o1_mimic[o1_mimic['los'] < day]\n",
    "o2_mimic = o2_mimic[o2_mimic['los'] < day]\n",
    "o3_mimic = o3_mimic[o3_mimic['los'] < day]\n",
    "o4_mimic = o4_mimic[o4_mimic['los'] < day]\n",
    "\n",
    "# Filter icu stay less than 10 days\n",
    "o1_eicu = o1_eicu[o1_eicu['los'] < day]\n",
    "o2_eicu = o2_eicu[o2_eicu['los'] < day]\n",
    "o3_eicu = o3_eicu[o3_eicu['los'] < day]\n",
    "o4_eicu = o4_eicu[o4_eicu['los'] < day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51bc6cae-2821-43c1-a6b1-04c79b38273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Time Zone\n",
    "\n",
    "#time_zone = 16\n",
    "#mimic_df = mimic_df[mimic_df['Time_Zone'] == time_zone]\n",
    "#eicu_df = eicu_df[eicu_df['Time_Zone'] == time_zone]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1786ba0-60a9-4593-87be-689ee23fffe2",
   "metadata": {},
   "source": [
    "# Split Training - Validation - Test Set (Phase 02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4565d216-91bb-40f0-b998-c5d8b52d1a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 16:30:41,291 - INFO - o1_mimic:\n",
      "2024-12-15 16:30:41,292 - INFO -   Training set size: 122496\n",
      "2024-12-15 16:30:41,292 - INFO -   Validation set size: 15312\n",
      "2024-12-15 16:30:41,294 - INFO -   Test set size: 15312\n",
      "2024-12-15 16:30:41,513 - INFO - o2_mimic:\n",
      "2024-12-15 16:30:41,514 - INFO -   Training set size: 61248\n",
      "2024-12-15 16:30:41,515 - INFO -   Validation set size: 7656\n",
      "2024-12-15 16:30:41,516 - INFO -   Test set size: 7656\n",
      "2024-12-15 16:30:41,658 - INFO - o3_mimic:\n",
      "2024-12-15 16:30:41,658 - INFO -   Training set size: 40832\n",
      "2024-12-15 16:30:41,659 - INFO -   Validation set size: 5104\n",
      "2024-12-15 16:30:41,660 - INFO -   Test set size: 5104\n",
      "2024-12-15 16:30:41,767 - INFO - o4_mimic:\n",
      "2024-12-15 16:30:41,768 - INFO -   Training set size: 30624\n",
      "2024-12-15 16:30:41,769 - INFO -   Validation set size: 3828\n",
      "2024-12-15 16:30:41,770 - INFO -   Test set size: 3828\n"
     ]
    }
   ],
   "source": [
    "# Mimic datasets dictionary\n",
    "mimic_dataframes = {\n",
    "    \"o1_mimic\": o1_mimic,\n",
    "    \"o2_mimic\": o2_mimic,\n",
    "    \"o3_mimic\": o3_mimic,\n",
    "    \"o4_mimic\": o4_mimic,\n",
    "}\n",
    "\n",
    "# Parameters for splitting\n",
    "total_test_val_perc = 0.2  # Total percentage for validation and test sets\n",
    "split_between_test_val_perc = 0.5  # Percentage split between validation and test sets\n",
    "\n",
    "# Splitting function\n",
    "def split_mimic_data(mimic_df, total_test_val_perc, split_between_test_val_perc):\n",
    "    # Step 1: Group by subject_id and hadm_id\n",
    "    grouped_df = mimic_df.groupby(['subject_id', 'hadm_id'])\n",
    "    patient_df = grouped_df['hospital_expire_flag'].first().reset_index()\n",
    "\n",
    "    # Step 2: Perform stratified split\n",
    "    train, temp = train_test_split(\n",
    "        patient_df,\n",
    "        test_size=total_test_val_perc,\n",
    "        stratify=patient_df['hospital_expire_flag'],\n",
    "        random_state=42\n",
    "    )\n",
    "    val, test = train_test_split(\n",
    "        temp,\n",
    "        test_size=split_between_test_val_perc,\n",
    "        stratify=temp['hospital_expire_flag'],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Step 3: Merge back to original mimic_df\n",
    "    train_df = mimic_df.merge(train[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "    val_df = mimic_df.merge(val[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "    test_df = mimic_df.merge(test[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Loop through mimic datasets and apply the split\n",
    "split_results = {}\n",
    "for name, df in mimic_dataframes.items():\n",
    "    train_df, val_df, test_df = split_mimic_data(df, total_test_val_perc, split_between_test_val_perc)\n",
    "    split_results[name] = {\n",
    "        \"train\": train_df,\n",
    "        \"val\": val_df,\n",
    "        \"test\": test_df\n",
    "    }\n",
    "    # Print the sizes of splits\n",
    "    logging.info(f\"{name}:\")\n",
    "    logging.info(f\"  Training set size: {train_df.shape[0]}\")\n",
    "    logging.info(f\"  Validation set size: {val_df.shape[0]}\")\n",
    "    logging.info(f\"  Test set size: {test_df.shape[0]}\")\n",
    "\n",
    "# Access the splits\n",
    "o1_train = split_results[\"o1_mimic\"][\"train\"]\n",
    "o1_val = split_results[\"o1_mimic\"][\"val\"]\n",
    "o1_test = split_results[\"o1_mimic\"][\"test\"]\n",
    "\n",
    "o2_train = split_results[\"o2_mimic\"][\"train\"]\n",
    "o2_val = split_results[\"o2_mimic\"][\"val\"]\n",
    "o2_test = split_results[\"o2_mimic\"][\"test\"]\n",
    "\n",
    "o3_train = split_results[\"o3_mimic\"][\"train\"]\n",
    "o3_val = split_results[\"o3_mimic\"][\"val\"]\n",
    "o3_test = split_results[\"o3_mimic\"][\"test\"]\n",
    "\n",
    "o4_train = split_results[\"o4_mimic\"][\"train\"]\n",
    "o4_val = split_results[\"o4_mimic\"][\"val\"]\n",
    "o4_test = split_results[\"o4_mimic\"][\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47462f19-6295-4cb1-a8f7-64b8bd104609",
   "metadata": {},
   "source": [
    "# Check ratio and unique patients between sets (Phase 03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "729ed3bc-b7ae-48fe-892f-66b6f814c5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "Survive: 1014.0\n",
      "Non-survive: 262.0\n",
      "Ratio Train Set: 3.87:1\n",
      "\n",
      "Validation Set\n",
      "Survive: 126.5\n",
      "Non-survive: 33.0\n",
      "Ratio Train Set: 3.83:1\n",
      "\n",
      "Test Set\n",
      "Survive: 127.0\n",
      "Non-survive: 32.5\n",
      "Ratio Train Set: 3.91:1\n"
     ]
    }
   ],
   "source": [
    "# Count on Training set survive and non-survive\n",
    "survival_counts = o2_train['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/48\n",
    "temp_non_survive = survival_counts.get(1, 0)/48\n",
    "\n",
    "# Display the results\n",
    "print(f'Train Set')\n",
    "print(f'Survive: {temp_survive}')\n",
    "print(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "print(f'Ratio Train Set: {ratio:.2f}:1')\n",
    "\n",
    "\"\"\"----------------------------\"\"\"\n",
    "\n",
    "# Count on validation set survive and non-survive\n",
    "survival_counts = o2_val['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/48\n",
    "temp_non_survive = survival_counts.get(1, 0)/48\n",
    "\n",
    "# Display the results\n",
    "print(f'\\nValidation Set')\n",
    "print(f'Survive: {temp_survive}')\n",
    "print(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "print(f'Ratio Train Set: {ratio:.2f}:1')\n",
    "\n",
    "\"\"\"----------------------------\"\"\"\n",
    "\n",
    "# Count on validation set survive and non-survive\n",
    "survival_counts = o2_test['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/48\n",
    "temp_non_survive = survival_counts.get(1, 0)/48\n",
    "\n",
    "# Display the results\n",
    "print(f'\\nTest Set')\n",
    "print(f'Survive: {temp_survive}')\n",
    "print(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "print(f'Ratio Train Set: {ratio:.2f}:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73205561-9bcc-42fe-acc4-8bcd1616d9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between training and validation sets: 0\n",
      "Overlap between training and test sets: 0\n",
      "Overlap between validation and test sets: 0\n"
     ]
    }
   ],
   "source": [
    "# Mine unique subject_id from sets\n",
    "train_subjects = set(o2_train['subject_id'].unique())\n",
    "val_subjects = set(o2_val['subject_id'].unique())\n",
    "test_subjects = set(o2_test['subject_id'].unique())\n",
    "\n",
    "# Check if there are overlaping subject_id\n",
    "train_val_overlap = train_subjects.intersection(val_subjects)\n",
    "train_test_overlap = train_subjects.intersection(test_subjects)\n",
    "val_test_overlap = val_subjects.intersection(test_subjects)\n",
    "\n",
    "# Display the results\n",
    "print(f'Overlap between training and validation sets: {len(train_val_overlap)}')\n",
    "print(f'Overlap between training and test sets: {len(train_test_overlap)}')\n",
    "print(f'Overlap between validation and test sets: {len(val_test_overlap)}')\n",
    "\n",
    "# print overlaping\n",
    "if train_val_overlap:\n",
    "    print(f'Subjects in both training and validation: {train_val_overlap}')\n",
    "if train_test_overlap:\n",
    "    print(f'Subjects in both training and test: {train_test_overlap}')\n",
    "if val_test_overlap:\n",
    "    print(f'Subjects in both validation and test: {val_test_overlap}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d473a75a-86eb-4dac-be9a-b73cf80b111b",
   "metadata": {},
   "source": [
    "# Split label from Train - Validation - Test Sets (Phase 04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee62e244-e475-4222-802b-57d374b63c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External validation from eICU\n",
    "o1_X_external = o1_eicu.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o1_y_external_los = o1_eicu['los']\n",
    "o1_y_external_mortality = o1_eicu['hospital_expire_flag']\n",
    "\n",
    "o2_X_external = o2_eicu.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o2_y_external_los = o2_eicu['los']\n",
    "o2_y_external_mortality = o2_eicu['hospital_expire_flag']\n",
    "\n",
    "o3_X_external = o3_eicu.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o3_y_external_los = o3_eicu['los']\n",
    "o3_y_external_mortality = o3_eicu['hospital_expire_flag']\n",
    "\n",
    "o4_X_external = o4_eicu.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o4_y_external_los = o4_eicu['los']\n",
    "o4_y_external_mortality = o4_eicu['hospital_expire_flag']\n",
    "\n",
    "\n",
    "# Separate features and target for the training, validation, and test sets\n",
    "# Train\n",
    "o1_X_train = o1_train.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o1_y_train_los = o1_train['los']\n",
    "o1_y_train_mortality = o1_train['hospital_expire_flag']\n",
    "\n",
    "o2_X_train = o2_train.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o2_y_train_los = o2_train['los']\n",
    "o2_y_train_mortality = o2_train['hospital_expire_flag']\n",
    "\n",
    "o3_X_train = o3_train.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o3_y_train_los = o3_train['los']\n",
    "o3_y_train_mortality = o3_train['hospital_expire_flag']\n",
    "\n",
    "o4_X_train = o4_train.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o4_y_train_los = o4_train['los']\n",
    "o4_y_train_mortality = o4_train['hospital_expire_flag']\n",
    "\n",
    "# Validation\n",
    "o1_X_validate = o1_val.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o1_y_validate_los = o1_val['los']\n",
    "o1_y_validate_mortality = o1_val['hospital_expire_flag']\n",
    "\n",
    "o2_X_validate = o2_val.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o2_y_validate_los = o2_val['los']\n",
    "o2_y_validate_mortality = o2_val['hospital_expire_flag']\n",
    "\n",
    "o3_X_validate = o3_val.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o3_y_validate_los = o3_val['los']\n",
    "o3_y_validate_mortality = o3_val['hospital_expire_flag']\n",
    "\n",
    "o4_X_validate = o4_val.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o4_y_validate_los = o4_val['los']\n",
    "o4_y_validate_mortality = o4_val['hospital_expire_flag']\n",
    "\n",
    "# Test\n",
    "o1_X_test = o1_test.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o1_y_test_los = o1_test['los']\n",
    "o1_y_test_mortality = o1_test['hospital_expire_flag']\n",
    "\n",
    "o2_X_test = o2_test.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o2_y_test_los = o2_test['los']\n",
    "o2_y_test_mortality = o2_test['hospital_expire_flag']\n",
    "\n",
    "o3_X_test = o3_test.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o3_y_test_los = o3_test['los']\n",
    "o3_y_test_mortality = o3_test['hospital_expire_flag']\n",
    "\n",
    "o4_X_test = o4_test.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "o4_y_test_los = o4_test['los']\n",
    "o4_y_test_mortality = o4_test['hospital_expire_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca998b29-a24f-449a-b37f-4533a295b61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframes do NOT have the same order in the columns 'subject_id' and'hadm_id'.\n"
     ]
    }
   ],
   "source": [
    "# Check if the specified columns have the same order across all dataframes\n",
    "columns_to_check = ['subject_id', 'hadm_id']\n",
    "\n",
    "# Extract the relevant columns from each dataframe\n",
    "o1_subset = o1_train[columns_to_check]\n",
    "o2_subset = o2_train[columns_to_check]\n",
    "o3_subset = o3_train[columns_to_check]\n",
    "o4_subset = o4_train[columns_to_check]\n",
    "\n",
    "# Compare the order and content\n",
    "same_order = (\n",
    "    o1_subset.equals(o2_subset) and\n",
    "    o1_subset.equals(o3_subset) and\n",
    "    o1_subset.equals(o4_subset)\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "if same_order:\n",
    "    print(\"The dataframes have the same order in the columns 'subject_id' and 'hadm_id'.\")\n",
    "else:\n",
    "    print(\"The dataframes do NOT have the same order in the columns 'subject_id' and'hadm_id'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbc9eff3-d99d-4fed-b2f6-a0986eba0049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 16:32:01,835 - INFO - Output directory set to: ../CSV/exports/split_set\n",
      "2024-12-15 16:32:45,897 - INFO - Saved o1_X_external.csv to ../CSV/exports/split_set\\o1_X_external.csv\n",
      "2024-12-15 16:32:46,110 - INFO - Saved o1_y_external_los.csv to ../CSV/exports/split_set\\o1_y_external_los.csv\n",
      "2024-12-15 16:32:46,225 - INFO - Saved o1_y_external_mortality.csv to ../CSV/exports/split_set\\o1_y_external_mortality.csv\n",
      "2024-12-15 16:33:07,853 - INFO - Saved o2_X_external.csv to ../CSV/exports/split_set\\o2_X_external.csv\n",
      "2024-12-15 16:33:07,959 - INFO - Saved o2_y_external_los.csv to ../CSV/exports/split_set\\o2_y_external_los.csv\n",
      "2024-12-15 16:33:08,014 - INFO - Saved o2_y_external_mortality.csv to ../CSV/exports/split_set\\o2_y_external_mortality.csv\n",
      "2024-12-15 16:33:22,090 - INFO - Saved o3_X_external.csv to ../CSV/exports/split_set\\o3_X_external.csv\n",
      "2024-12-15 16:33:22,177 - INFO - Saved o3_y_external_los.csv to ../CSV/exports/split_set\\o3_y_external_los.csv\n",
      "2024-12-15 16:33:22,216 - INFO - Saved o3_y_external_mortality.csv to ../CSV/exports/split_set\\o3_y_external_mortality.csv\n",
      "2024-12-15 16:33:32,769 - INFO - Saved o4_X_external.csv to ../CSV/exports/split_set\\o4_X_external.csv\n",
      "2024-12-15 16:33:32,829 - INFO - Saved o4_y_external_los.csv to ../CSV/exports/split_set\\o4_y_external_los.csv\n",
      "2024-12-15 16:33:32,864 - INFO - Saved o4_y_external_mortality.csv to ../CSV/exports/split_set\\o4_y_external_mortality.csv\n",
      "2024-12-15 16:33:56,733 - INFO - Saved o1_X_train.csv to ../CSV/exports/split_set\\o1_X_train.csv\n",
      "2024-12-15 16:33:56,932 - INFO - Saved o1_y_train_los.csv to ../CSV/exports/split_set\\o1_y_train_los.csv\n",
      "2024-12-15 16:33:56,991 - INFO - Saved o1_y_train_mortality.csv to ../CSV/exports/split_set\\o1_y_train_mortality.csv\n",
      "2024-12-15 16:34:09,111 - INFO - Saved o2_X_train.csv to ../CSV/exports/split_set\\o2_X_train.csv\n",
      "2024-12-15 16:34:09,241 - INFO - Saved o2_y_train_los.csv to ../CSV/exports/split_set\\o2_y_train_los.csv\n",
      "2024-12-15 16:34:09,271 - INFO - Saved o2_y_train_mortality.csv to ../CSV/exports/split_set\\o2_y_train_mortality.csv\n",
      "2024-12-15 16:34:17,474 - INFO - Saved o3_X_train.csv to ../CSV/exports/split_set\\o3_X_train.csv\n",
      "2024-12-15 16:34:17,552 - INFO - Saved o3_y_train_los.csv to ../CSV/exports/split_set\\o3_y_train_los.csv\n",
      "2024-12-15 16:34:17,579 - INFO - Saved o3_y_train_mortality.csv to ../CSV/exports/split_set\\o3_y_train_mortality.csv\n",
      "2024-12-15 16:34:23,881 - INFO - Saved o4_X_train.csv to ../CSV/exports/split_set\\o4_X_train.csv\n",
      "2024-12-15 16:34:23,952 - INFO - Saved o4_y_train_los.csv to ../CSV/exports/split_set\\o4_y_train_los.csv\n",
      "2024-12-15 16:34:23,975 - INFO - Saved o4_y_train_mortality.csv to ../CSV/exports/split_set\\o4_y_train_mortality.csv\n",
      "2024-12-15 16:34:27,130 - INFO - Saved o1_X_validate.csv to ../CSV/exports/split_set\\o1_X_validate.csv\n",
      "2024-12-15 16:34:27,171 - INFO - Saved o1_y_validate_los.csv to ../CSV/exports/split_set\\o1_y_validate_los.csv\n",
      "2024-12-15 16:34:27,183 - INFO - Saved o1_y_validate_mortality.csv to ../CSV/exports/split_set\\o1_y_validate_mortality.csv\n",
      "2024-12-15 16:34:28,753 - INFO - Saved o2_X_validate.csv to ../CSV/exports/split_set\\o2_X_validate.csv\n",
      "2024-12-15 16:34:28,768 - INFO - Saved o2_y_validate_los.csv to ../CSV/exports/split_set\\o2_y_validate_los.csv\n",
      "2024-12-15 16:34:28,776 - INFO - Saved o2_y_validate_mortality.csv to ../CSV/exports/split_set\\o2_y_validate_mortality.csv\n",
      "2024-12-15 16:34:29,863 - INFO - Saved o3_X_validate.csv to ../CSV/exports/split_set\\o3_X_validate.csv\n",
      "2024-12-15 16:34:29,876 - INFO - Saved o3_y_validate_los.csv to ../CSV/exports/split_set\\o3_y_validate_los.csv\n",
      "2024-12-15 16:34:29,881 - INFO - Saved o3_y_validate_mortality.csv to ../CSV/exports/split_set\\o3_y_validate_mortality.csv\n",
      "2024-12-15 16:34:30,687 - INFO - Saved o4_X_validate.csv to ../CSV/exports/split_set\\o4_X_validate.csv\n",
      "2024-12-15 16:34:30,697 - INFO - Saved o4_y_validate_los.csv to ../CSV/exports/split_set\\o4_y_validate_los.csv\n",
      "2024-12-15 16:34:30,702 - INFO - Saved o4_y_validate_mortality.csv to ../CSV/exports/split_set\\o4_y_validate_mortality.csv\n",
      "2024-12-15 16:34:33,750 - INFO - Saved o1_X_test.csv to ../CSV/exports/split_set\\o1_X_test.csv\n",
      "2024-12-15 16:34:33,784 - INFO - Saved o1_y_test_los.csv to ../CSV/exports/split_set\\o1_y_test_los.csv\n",
      "2024-12-15 16:34:33,795 - INFO - Saved o1_y_test_mortality.csv to ../CSV/exports/split_set\\o1_y_test_mortality.csv\n",
      "2024-12-15 16:34:35,340 - INFO - Saved o2_X_test.csv to ../CSV/exports/split_set\\o2_X_test.csv\n",
      "2024-12-15 16:34:35,356 - INFO - Saved o2_y_test_los.csv to ../CSV/exports/split_set\\o2_y_test_los.csv\n",
      "2024-12-15 16:34:35,364 - INFO - Saved o2_y_test_mortality.csv to ../CSV/exports/split_set\\o2_y_test_mortality.csv\n",
      "2024-12-15 16:34:36,505 - INFO - Saved o3_X_test.csv to ../CSV/exports/split_set\\o3_X_test.csv\n",
      "2024-12-15 16:34:36,516 - INFO - Saved o3_y_test_los.csv to ../CSV/exports/split_set\\o3_y_test_los.csv\n",
      "2024-12-15 16:34:36,521 - INFO - Saved o3_y_test_mortality.csv to ../CSV/exports/split_set\\o3_y_test_mortality.csv\n",
      "2024-12-15 16:34:37,394 - INFO - Saved o4_X_test.csv to ../CSV/exports/split_set\\o4_X_test.csv\n",
      "2024-12-15 16:34:37,403 - INFO - Saved o4_y_test_los.csv to ../CSV/exports/split_set\\o4_y_test_los.csv\n",
      "2024-12-15 16:34:37,409 - INFO - Saved o4_y_test_mortality.csv to ../CSV/exports/split_set\\o4_y_test_mortality.csv\n",
      "2024-12-15 16:34:37,410 - INFO - All datasets have been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"../CSV/exports/split_set\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logging.info(f\"Output directory set to: {output_dir}\")\n",
    "\n",
    "# Variables to save\n",
    "variables_to_save = {\n",
    "    \"o1_X_external.csv\": o1_X_external,\n",
    "    \"o1_y_external_los.csv\": o1_y_external_los,\n",
    "    \"o1_y_external_mortality.csv\": o1_y_external_mortality,\n",
    "    \"o2_X_external.csv\": o2_X_external,\n",
    "    \"o2_y_external_los.csv\": o2_y_external_los,\n",
    "    \"o2_y_external_mortality.csv\": o2_y_external_mortality,\n",
    "    \"o3_X_external.csv\": o3_X_external,\n",
    "    \"o3_y_external_los.csv\": o3_y_external_los,\n",
    "    \"o3_y_external_mortality.csv\": o3_y_external_mortality,\n",
    "    \"o4_X_external.csv\": o4_X_external,\n",
    "    \"o4_y_external_los.csv\": o4_y_external_los,\n",
    "    \"o4_y_external_mortality.csv\": o4_y_external_mortality,\n",
    "    \"o1_X_train.csv\": o1_X_train,\n",
    "    \"o1_y_train_los.csv\": o1_y_train_los,\n",
    "    \"o1_y_train_mortality.csv\": o1_y_train_mortality,\n",
    "    \"o2_X_train.csv\": o2_X_train,\n",
    "    \"o2_y_train_los.csv\": o2_y_train_los,\n",
    "    \"o2_y_train_mortality.csv\": o2_y_train_mortality,\n",
    "    \"o3_X_train.csv\": o3_X_train,\n",
    "    \"o3_y_train_los.csv\": o3_y_train_los,\n",
    "    \"o3_y_train_mortality.csv\": o3_y_train_mortality,\n",
    "    \"o4_X_train.csv\": o4_X_train,\n",
    "    \"o4_y_train_los.csv\": o4_y_train_los,\n",
    "    \"o4_y_train_mortality.csv\": o4_y_train_mortality,\n",
    "    \"o1_X_validate.csv\": o1_X_validate,\n",
    "    \"o1_y_validate_los.csv\": o1_y_validate_los,\n",
    "    \"o1_y_validate_mortality.csv\": o1_y_validate_mortality,\n",
    "    \"o2_X_validate.csv\": o2_X_validate,\n",
    "    \"o2_y_validate_los.csv\": o2_y_validate_los,\n",
    "    \"o2_y_validate_mortality.csv\": o2_y_validate_mortality,\n",
    "    \"o3_X_validate.csv\": o3_X_validate,\n",
    "    \"o3_y_validate_los.csv\": o3_y_validate_los,\n",
    "    \"o3_y_validate_mortality.csv\": o3_y_validate_mortality,\n",
    "    \"o4_X_validate.csv\": o4_X_validate,\n",
    "    \"o4_y_validate_los.csv\": o4_y_validate_los,\n",
    "    \"o4_y_validate_mortality.csv\": o4_y_validate_mortality,\n",
    "    \"o1_X_test.csv\": o1_X_test,\n",
    "    \"o1_y_test_los.csv\": o1_y_test_los,\n",
    "    \"o1_y_test_mortality.csv\": o1_y_test_mortality,\n",
    "    \"o2_X_test.csv\": o2_X_test,\n",
    "    \"o2_y_test_los.csv\": o2_y_test_los,\n",
    "    \"o2_y_test_mortality.csv\": o2_y_test_mortality,\n",
    "    \"o3_X_test.csv\": o3_X_test,\n",
    "    \"o3_y_test_los.csv\": o3_y_test_los,\n",
    "    \"o3_y_test_mortality.csv\": o3_y_test_mortality,\n",
    "    \"o4_X_test.csv\": o4_X_test,\n",
    "    \"o4_y_test_los.csv\": o4_y_test_los,\n",
    "    \"o4_y_test_mortality.csv\": o4_y_test_mortality,\n",
    "}\n",
    "\n",
    "# Save each variable to its respective CSV file\n",
    "for file_name, variable in variables_to_save.items():\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    variable.to_csv(file_path, index=False)\n",
    "    logging.info(f\"Saved {file_name} to {file_path}\")\n",
    "\n",
    "# Logging the end of the process\n",
    "logging.info(\"All datasets have been processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8e534-ef3a-4f45-bc0c-e3207d6ee1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
