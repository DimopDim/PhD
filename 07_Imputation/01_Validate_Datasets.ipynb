{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb59e1e6-920f-4f59-ae41-b1785569cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8b85cd-1802-4baa-bc49-fe0c6d5f58e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"data_loading.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed2c83c9-7fb0-4101-bd54-8a473f3a5bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 22:58:54,523 - INFO - Starting the data loading process from o3_hour_overlap_window...\n",
      "2025-03-22 22:58:54,527 - INFO - Loading MIMIC-IV datasets\n",
      "2025-03-22 22:59:01,336 - INFO - Loaded mimic_mean_df successfully.\n",
      "2025-03-22 22:59:08,644 - INFO - Loaded mimic_median_df successfully.\n",
      "2025-03-22 22:59:15,686 - INFO - Loaded mimic_min_df successfully.\n",
      "2025-03-22 22:59:22,705 - INFO - Loaded mimic_max_df successfully.\n",
      "2025-03-22 22:59:22,706 - INFO - Loading eICU datasets\n",
      "2025-03-22 22:59:25,073 - INFO - Loaded eicu_mean_df successfully.\n",
      "2025-03-22 22:59:27,170 - INFO - Loaded eicu_median_df successfully.\n",
      "2025-03-22 22:59:29,176 - INFO - Loaded eicu_min_df successfully.\n",
      "2025-03-22 22:59:31,292 - INFO - Loaded eicu_max_df successfully.\n",
      "2025-03-22 22:59:31,294 - INFO - Data loading process completed.\n"
     ]
    }
   ],
   "source": [
    "# Read MIMICs CSV files\n",
    "subfolder = 'o3_hour_overlap_window'\n",
    "\n",
    "# Log the start of the process\n",
    "logging.info(f\"Starting the data loading process from {subfolder}...\")\n",
    "\n",
    "logging.info(f\"Loading MIMIC-IV datasets\")\n",
    "try:\n",
    "    mimic_mean_df = pd.read_csv(f\"../01_MimicIV/CSV/Exports/datasets/whole_set/{subfolder}/o01_final_mean_with_los.csv\", low_memory=False)\n",
    "    logging.info(\"Loaded mimic_mean_df successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading mimic_mean_df: {e}\")\n",
    "\n",
    "try:\n",
    "    mimic_median_df = pd.read_csv(f\"../01_MimicIV/CSV/Exports/datasets/whole_set/{subfolder}/o02_final_median_with_los.csv\", low_memory=False)\n",
    "    logging.info(\"Loaded mimic_median_df successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading mimic_median_df: {e}\")\n",
    "\n",
    "try:\n",
    "    mimic_min_df = pd.read_csv(f\"../01_MimicIV/CSV/Exports/datasets/whole_set/{subfolder}/o03_final_min_with_los.csv\", low_memory=False)\n",
    "    logging.info(\"Loaded mimic_min_df successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading mimic_min_df: {e}\")\n",
    "\n",
    "try:\n",
    "    mimic_max_df = pd.read_csv(f\"../01_MimicIV/CSV/Exports/datasets/whole_set/{subfolder}/o04_final_max_with_los.csv\", low_memory=False)\n",
    "    logging.info(\"Loaded mimic_max_df successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading mimic_max_df: {e}\")\n",
    "\n",
    "# Read eICUs CSV files\n",
    "logging.info(\"Loading eICU datasets\")\n",
    "try:\n",
    "    eicu_mean_df = pd.read_csv(f\"../02_eICU/CSV/Exports/datasets/whole_set/{subfolder}/o01_final_mean_table.csv\", low_memory=False)\n",
    "    logging.info(\"Loaded eicu_mean_df successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading eicu_mean_df: {e}\")\n",
    "\n",
    "try:\n",
    "    eicu_median_df = pd.read_csv(f\"../02_eICU/CSV/Exports/datasets/whole_set/{subfolder}/o02_final_median_table.csv\", low_memory=False)\n",
    "    logging.info(\"Loaded eicu_median_df successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading eicu_median_df: {e}\")\n",
    "\n",
    "try:\n",
    "    eicu_min_df = pd.read_csv(f\"../02_eICU/CSV/Exports/datasets/whole_set/{subfolder}/o03_final_min_table.csv\", low_memory=False)\n",
    "    logging.info(\"Loaded eicu_min_df successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading eicu_min_df: {e}\")\n",
    "\n",
    "try:\n",
    "    eicu_max_df = pd.read_csv(f\"../02_eICU/CSV/Exports/datasets/whole_set/{subfolder}/o04_final_max_table.csv\", low_memory=False)\n",
    "    logging.info(\"Loaded eicu_max_df successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading eicu_max_df: {e}\")\n",
    "\n",
    "logging.info(\"Data loading process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d600510-2bb4-4f60-a222-045023f49119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_count</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>Time_Zone</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>language</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>race</th>\n",
       "      <th>Base Excess - Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Other - Mean.2</th>\n",
       "      <th>Triglycerides, Pleural - Mean</th>\n",
       "      <th>Thoracic Fluid Content (TFC) (NICOM)  - Mean</th>\n",
       "      <th>Head of Bed Measurement (Degree) - Mean</th>\n",
       "      <th>ARCH-1 - Mean</th>\n",
       "      <th>Factor VII - Mean</th>\n",
       "      <th>Creatinine, Body Fluid - Mean</th>\n",
       "      <th>Ethanol - Mean</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>English</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Survive</td>\n",
       "      <td>8.357373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>English</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Survive</td>\n",
       "      <td>8.357373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>English</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Survive</td>\n",
       "      <td>8.357373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>English</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Survive</td>\n",
       "      <td>8.357373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>English</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Survive</td>\n",
       "      <td>8.357373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58139</th>\n",
       "      <td>58140</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>12</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Survive</td>\n",
       "      <td>1.937847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58140</th>\n",
       "      <td>58141</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>13</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Survive</td>\n",
       "      <td>1.937847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58141</th>\n",
       "      <td>58142</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>14</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Survive</td>\n",
       "      <td>1.937847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58142</th>\n",
       "      <td>58143</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>15</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Survive</td>\n",
       "      <td>1.937847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58143</th>\n",
       "      <td>58144</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>16</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Survive</td>\n",
       "      <td>1.937847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58144 rows Ã— 1074 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_count  subject_id   hadm_id  Time_Zone gender  age language  \\\n",
       "0              1    10004733  27411876          1      M   51  English   \n",
       "1              2    10004733  27411876          2      M   51  English   \n",
       "2              3    10004733  27411876          3      M   51  English   \n",
       "3              4    10004733  27411876          4      M   51  English   \n",
       "4              5    10004733  27411876          5      M   51  English   \n",
       "...          ...         ...       ...        ...    ...  ...      ...   \n",
       "58139      58140    19999987  23865745         12      F   57  English   \n",
       "58140      58141    19999987  23865745         13      F   57  English   \n",
       "58141      58142    19999987  23865745         14      F   57  English   \n",
       "58142      58143    19999987  23865745         15      F   57  English   \n",
       "58143      58144    19999987  23865745         16      F   57  English   \n",
       "\n",
       "      marital_status     race  Base Excess - Mean  ...  Other - Mean.2  \\\n",
       "0             SINGLE  UNKNOWN                 0.0  ...             NaN   \n",
       "1             SINGLE  UNKNOWN                 0.0  ...             NaN   \n",
       "2             SINGLE  UNKNOWN                 0.0  ...             NaN   \n",
       "3             SINGLE  UNKNOWN                 0.0  ...             NaN   \n",
       "4             SINGLE  UNKNOWN                 0.0  ...             NaN   \n",
       "...              ...      ...                 ...  ...             ...   \n",
       "58139            NaN  UNKNOWN                 1.0  ...             NaN   \n",
       "58140            NaN  UNKNOWN                 1.0  ...             NaN   \n",
       "58141            NaN  UNKNOWN                 1.0  ...             NaN   \n",
       "58142            NaN  UNKNOWN                 1.0  ...             NaN   \n",
       "58143            NaN  UNKNOWN                 1.0  ...             NaN   \n",
       "\n",
       "       Triglycerides, Pleural - Mean  \\\n",
       "0                                NaN   \n",
       "1                                NaN   \n",
       "2                                NaN   \n",
       "3                                NaN   \n",
       "4                                NaN   \n",
       "...                              ...   \n",
       "58139                            NaN   \n",
       "58140                            NaN   \n",
       "58141                            NaN   \n",
       "58142                            NaN   \n",
       "58143                            NaN   \n",
       "\n",
       "       Thoracic Fluid Content (TFC) (NICOM)  - Mean  \\\n",
       "0                                               NaN   \n",
       "1                                               NaN   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               NaN   \n",
       "...                                             ...   \n",
       "58139                                           NaN   \n",
       "58140                                           NaN   \n",
       "58141                                           NaN   \n",
       "58142                                           NaN   \n",
       "58143                                           NaN   \n",
       "\n",
       "       Head of Bed Measurement (Degree) - Mean  ARCH-1 - Mean  \\\n",
       "0                                          NaN            NaN   \n",
       "1                                          NaN            NaN   \n",
       "2                                          NaN            NaN   \n",
       "3                                          NaN            NaN   \n",
       "4                                          NaN            NaN   \n",
       "...                                        ...            ...   \n",
       "58139                                      NaN            NaN   \n",
       "58140                                      NaN            NaN   \n",
       "58141                                      NaN            NaN   \n",
       "58142                                      NaN            NaN   \n",
       "58143                                      NaN            NaN   \n",
       "\n",
       "       Factor VII - Mean  Creatinine, Body Fluid - Mean  Ethanol - Mean  \\\n",
       "0                    NaN                            NaN             NaN   \n",
       "1                    NaN                            NaN             NaN   \n",
       "2                    NaN                            NaN             NaN   \n",
       "3                    NaN                            NaN             NaN   \n",
       "4                    NaN                            NaN             NaN   \n",
       "...                  ...                            ...             ...   \n",
       "58139                NaN                            NaN             NaN   \n",
       "58140                NaN                            NaN             NaN   \n",
       "58141                NaN                            NaN             NaN   \n",
       "58142                NaN                            NaN             NaN   \n",
       "58143                NaN                            NaN             NaN   \n",
       "\n",
       "       hospital_expire_flag       los  \n",
       "0                   Survive  8.357373  \n",
       "1                   Survive  8.357373  \n",
       "2                   Survive  8.357373  \n",
       "3                   Survive  8.357373  \n",
       "4                   Survive  8.357373  \n",
       "...                     ...       ...  \n",
       "58139               Survive  1.937847  \n",
       "58140               Survive  1.937847  \n",
       "58141               Survive  1.937847  \n",
       "58142               Survive  1.937847  \n",
       "58143               Survive  1.937847  \n",
       "\n",
       "[58144 rows x 1074 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display (mimic_mean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad05574d-790c-4f25-bdcc-3bdcf735e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to keep\n",
    "mimic_columns_to_keep = pd.read_csv('CSV/import/mimic_features.csv')\n",
    "eicu_columns_to_keep = pd.read_csv('CSV/import/eicu_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c04fc7f-ba43-4c69-8ef9-12b25082bc1a",
   "metadata": {},
   "source": [
    "# Mimic IV Ver 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a292569-5ccb-4055-9da7-0c5af3180d5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 23:01:09,329 - INFO - Starting the chunk-based merging process for MIMIC-IV dataframes.\n",
      "2025-03-22 23:01:09,330 - INFO - Processing chunk 1...\n",
      "2025-03-22 23:01:09,332 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:09,593 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:09,594 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:09,846 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:09,847 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:10,129 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:10,130 - INFO - Processing chunk 2...\n",
      "2025-03-22 23:01:10,131 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:10,382 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:10,383 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:10,684 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:10,685 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:10,985 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:10,986 - INFO - Processing chunk 3...\n",
      "2025-03-22 23:01:10,988 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:11,237 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:11,238 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:11,500 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:11,501 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:11,783 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:11,784 - INFO - Processing chunk 4...\n",
      "2025-03-22 23:01:11,785 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:12,010 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:12,011 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:12,283 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:12,284 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:12,565 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:12,566 - INFO - Processing chunk 5...\n",
      "2025-03-22 23:01:12,567 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:12,804 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:12,805 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:13,071 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:13,072 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:13,380 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:13,381 - INFO - Processing chunk 6...\n",
      "2025-03-22 23:01:13,382 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:13,614 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:13,615 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:13,880 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:13,881 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:14,159 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:14,160 - INFO - Processing chunk 7...\n",
      "2025-03-22 23:01:14,161 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:14,398 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:14,399 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:14,650 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:14,651 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:14,920 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:14,921 - INFO - Processing chunk 8...\n",
      "2025-03-22 23:01:14,922 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:15,178 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:15,179 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:15,464 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:15,465 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:15,746 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:15,747 - INFO - Processing chunk 9...\n",
      "2025-03-22 23:01:15,748 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:15,977 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:15,978 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:16,221 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:16,222 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:16,492 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:16,493 - INFO - Processing chunk 10...\n",
      "2025-03-22 23:01:16,494 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:16,716 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:16,717 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:16,964 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:16,965 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:17,239 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:17,239 - INFO - Processing chunk 11...\n",
      "2025-03-22 23:01:17,241 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:17,456 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:17,457 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:17,715 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:17,716 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:17,979 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:17,980 - INFO - Processing chunk 12...\n",
      "2025-03-22 23:01:17,981 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:18,213 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:18,214 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:18,466 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:18,467 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:18,732 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:18,733 - INFO - Processing chunk 13...\n",
      "2025-03-22 23:01:18,734 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:18,949 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:18,951 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:19,195 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:19,197 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:19,455 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:19,456 - INFO - Processing chunk 14...\n",
      "2025-03-22 23:01:19,457 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:19,672 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:19,673 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:19,919 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:19,920 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:20,179 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:20,180 - INFO - Processing chunk 15...\n",
      "2025-03-22 23:01:20,181 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:20,400 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:20,401 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:20,649 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:20,650 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:20,911 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:20,912 - INFO - Processing chunk 16...\n",
      "2025-03-22 23:01:20,913 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:21,132 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:21,133 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:21,383 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:21,384 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:21,673 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:21,674 - INFO - Processing chunk 17...\n",
      "2025-03-22 23:01:21,677 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:21,914 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:21,915 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:22,179 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:22,180 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:22,457 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:22,458 - INFO - Processing chunk 18...\n",
      "2025-03-22 23:01:22,460 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:22,688 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:22,689 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:22,948 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:22,949 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:23,219 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:23,220 - INFO - Processing chunk 19...\n",
      "2025-03-22 23:01:23,221 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:23,432 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:23,434 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:23,694 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:23,695 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:23,972 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:23,973 - INFO - Processing chunk 20...\n",
      "2025-03-22 23:01:23,975 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:24,198 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:24,200 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:24,451 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:24,452 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:24,711 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:24,712 - INFO - Processing chunk 21...\n",
      "2025-03-22 23:01:24,713 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:24,936 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:24,937 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:25,185 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:25,186 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:25,449 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:25,450 - INFO - Processing chunk 22...\n",
      "2025-03-22 23:01:25,451 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:25,669 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:25,671 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:25,929 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:25,930 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:26,194 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:26,195 - INFO - Processing chunk 23...\n",
      "2025-03-22 23:01:26,196 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:26,435 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:26,436 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:26,701 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:26,702 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:26,982 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:26,983 - INFO - Processing chunk 24...\n",
      "2025-03-22 23:01:26,984 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:27,215 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:27,216 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:27,461 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:27,462 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:27,732 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:27,733 - INFO - Processing chunk 25...\n",
      "2025-03-22 23:01:27,734 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:27,975 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:27,976 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:28,246 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:28,247 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:28,515 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:28,516 - INFO - Processing chunk 26...\n",
      "2025-03-22 23:01:28,517 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:28,748 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:28,749 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:29,005 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:29,006 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:29,274 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:29,275 - INFO - Processing chunk 27...\n",
      "2025-03-22 23:01:29,276 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:29,500 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:29,501 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:29,757 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:29,758 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:30,027 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:30,028 - INFO - Processing chunk 28...\n",
      "2025-03-22 23:01:30,029 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:30,244 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:30,246 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:30,496 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:30,497 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:30,757 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:30,758 - INFO - Processing chunk 29...\n",
      "2025-03-22 23:01:30,759 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:31,012 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:31,013 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:31,277 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:31,278 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:31,562 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:31,563 - INFO - Processing chunk 30...\n",
      "2025-03-22 23:01:31,564 - INFO - Merging with mimic_median_df...\n",
      "2025-03-22 23:01:31,762 - INFO - Merged chunk with mimic_median_df successfully.\n",
      "2025-03-22 23:01:31,764 - INFO - Merging with mimic_min_df...\n",
      "2025-03-22 23:01:31,968 - INFO - Merged chunk with mimic_min_df successfully.\n",
      "2025-03-22 23:01:31,969 - INFO - Merging with mimic_max_df...\n",
      "2025-03-22 23:01:32,173 - INFO - Merged chunk with mimic_max_df successfully.\n",
      "2025-03-22 23:01:32,173 - INFO - Concatenating all merged chunks...\n",
      "2025-03-22 23:01:32,803 - INFO - Concatenated all chunks successfully.\n",
      "2025-03-22 23:01:32,804 - INFO - Replacing suffixes in column names...\n",
      "2025-03-22 23:01:32,836 - INFO - Replaced suffixes successfully.\n",
      "2025-03-22 23:01:32,837 - INFO - Reordering columns: moving 'hospital_expire_flag' and 'los' to the end...\n",
      "2025-03-22 23:01:34,886 - INFO - Reordered columns successfully.\n",
      "2025-03-22 23:01:34,887 - INFO - Renaming the last two columns to preserve original names...\n",
      "2025-03-22 23:01:34,889 - INFO - Renamed the last two columns successfully.\n",
      "2025-03-22 23:01:34,890 - INFO - Chunk-based merging process for MIMIC-IV dataframes completed successfully.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Starting the chunk-based merging process for MIMIC-IV dataframes.\")\n",
    "\n",
    "try:\n",
    "    chunk_size = 2000\n",
    "    merged_chunks = []  # List to store merged chunks\n",
    "\n",
    "    # Iteratively process chunks from mimic_mean_df\n",
    "    for i in range(0, len(mimic_mean_df), chunk_size):\n",
    "        logging.info(f\"Processing chunk {i // chunk_size + 1}...\")\n",
    "\n",
    "        # Slice the chunk from mimic_mean_df\n",
    "        chunk = mimic_mean_df.iloc[i:i + chunk_size]\n",
    "\n",
    "        # Merge with mimic_median_df\n",
    "        logging.info(\"Merging with mimic_median_df...\")\n",
    "        chunk = chunk.merge(\n",
    "            mimic_median_df,\n",
    "            on=['row_count', 'subject_id', 'hadm_id', 'Time_Zone', 'gender', 'age', 'language', 'marital_status', 'race', 'hospital_expire_flag', 'los'],\n",
    "            suffixes=('_mean', '_median')\n",
    "        )\n",
    "        logging.info(\"Merged chunk with mimic_median_df successfully.\")\n",
    "\n",
    "        # Merge with mimic_min_df\n",
    "        logging.info(\"Merging with mimic_min_df...\")\n",
    "        chunk = chunk.merge(\n",
    "            mimic_min_df,\n",
    "            on=['row_count', 'subject_id', 'hadm_id', 'Time_Zone', 'gender', 'age', 'language', 'marital_status', 'race', 'hospital_expire_flag', 'los'],\n",
    "            suffixes=('', '_min')\n",
    "        )\n",
    "        logging.info(\"Merged chunk with mimic_min_df successfully.\")\n",
    "\n",
    "        # Merge with mimic_max_df\n",
    "        logging.info(\"Merging with mimic_max_df...\")\n",
    "        chunk = chunk.merge(\n",
    "            mimic_max_df,\n",
    "            on=['row_count', 'subject_id', 'hadm_id', 'Time_Zone', 'gender', 'age', 'language', 'marital_status', 'race', 'hospital_expire_flag', 'los'],\n",
    "            suffixes=('', '_max')\n",
    "        )\n",
    "        logging.info(\"Merged chunk with mimic_max_df successfully.\")\n",
    "\n",
    "        # Append merged chunk to the list\n",
    "        merged_chunks.append(chunk)\n",
    "\n",
    "    # Concatenate all chunks into a single dataframe\n",
    "    logging.info(\"Concatenating all merged chunks...\")\n",
    "    merged_mimic_df = pd.concat(merged_chunks, ignore_index=True)\n",
    "    logging.info(\"Concatenated all chunks successfully.\")\n",
    "\n",
    "    # Replace suffixes\n",
    "    logging.info(\"Replacing suffixes in column names...\")\n",
    "    merged_mimic_df.columns = merged_mimic_df.columns.str.replace(r'\\s*-\\s*Mean', ' (Mean)', regex=True)\n",
    "    merged_mimic_df.columns = merged_mimic_df.columns.str.replace(r'\\s*-\\s*Median', ' (Median)', regex=True)\n",
    "    merged_mimic_df.columns = merged_mimic_df.columns.str.replace(r'\\s*-\\s*Min', ' (Min)', regex=True)\n",
    "    merged_mimic_df.columns = merged_mimic_df.columns.str.replace(r'\\s*-\\s*Max', ' (Max)', regex=True)\n",
    "    logging.info(\"Replaced suffixes successfully.\")\n",
    "\n",
    "    # Move the 'hospital_expire_flag' and 'LOS' columns to the end of the dataframe\n",
    "    logging.info(\"Reordering columns: moving 'hospital_expire_flag' and 'los' to the end...\")\n",
    "    hospital_expire_flag_column = merged_mimic_df.pop('hospital_expire_flag')\n",
    "    los_column = merged_mimic_df.pop('los')\n",
    "    merged_mimic_df = pd.concat([merged_mimic_df, hospital_expire_flag_column, los_column], axis=1)\n",
    "    logging.info(\"Reordered columns successfully.\")\n",
    "\n",
    "    # Rename the last two columns to preserve their original names\n",
    "    logging.info(\"Renaming the last two columns to preserve original names...\")\n",
    "    merged_mimic_df.columns = list(merged_mimic_df.columns[:-2]) + ['hospital_expire_flag', 'los']\n",
    "    logging.info(\"Renamed the last two columns successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during the merging process: {e}\")\n",
    "\n",
    "logging.info(\"Chunk-based merging process for MIMIC-IV dataframes completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6391a870-1677-4e85-b08c-087d3fc6a9af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 23:02:06,544 - INFO - Starting chunk-wise processing of merged_mimic_df.\n",
      "C:\\Users\\Dimopoulos\\anaconda3\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "2025-03-22 23:02:08,987 - INFO - Processing chunk 1/10...\n",
      "2025-03-22 23:02:09,110 - INFO - Chunk 1/10 processed successfully.\n",
      "2025-03-22 23:02:09,111 - INFO - Processing chunk 2/10...\n",
      "2025-03-22 23:02:09,228 - INFO - Chunk 2/10 processed successfully.\n",
      "2025-03-22 23:02:09,229 - INFO - Processing chunk 3/10...\n",
      "2025-03-22 23:02:09,340 - INFO - Chunk 3/10 processed successfully.\n",
      "2025-03-22 23:02:09,341 - INFO - Processing chunk 4/10...\n",
      "2025-03-22 23:02:09,453 - INFO - Chunk 4/10 processed successfully.\n",
      "2025-03-22 23:02:09,454 - INFO - Processing chunk 5/10...\n",
      "2025-03-22 23:02:09,559 - INFO - Chunk 5/10 processed successfully.\n",
      "2025-03-22 23:02:09,560 - INFO - Processing chunk 6/10...\n",
      "2025-03-22 23:02:09,668 - INFO - Chunk 6/10 processed successfully.\n",
      "2025-03-22 23:02:09,669 - INFO - Processing chunk 7/10...\n",
      "2025-03-22 23:02:09,779 - INFO - Chunk 7/10 processed successfully.\n",
      "2025-03-22 23:02:09,780 - INFO - Processing chunk 8/10...\n",
      "2025-03-22 23:02:09,890 - INFO - Chunk 8/10 processed successfully.\n",
      "2025-03-22 23:02:09,891 - INFO - Processing chunk 9/10...\n",
      "2025-03-22 23:02:10,003 - INFO - Chunk 9/10 processed successfully.\n",
      "2025-03-22 23:02:10,004 - INFO - Processing chunk 10/10...\n",
      "2025-03-22 23:02:10,113 - INFO - Chunk 10/10 processed successfully.\n",
      "2025-03-22 23:02:10,888 - INFO - All chunks processed and concatenated successfully.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Starting chunk-wise processing of merged_mimic_df.\")\n",
    "# Define GCS columns for processing\n",
    "gcs_mean_cols = ['GCS - Eye Opening (Mean)', 'GCS - Verbal Response (Mean)', 'GCS - Motor Response (Mean)']\n",
    "gcs_median_cols = ['GCS - Eye Opening (Median)', 'GCS - Verbal Response (Median)', 'GCS - Motor Response (Median)']\n",
    "gcs_min_cols = ['GCS - Eye Opening (Min)', 'GCS - Verbal Response (Min)', 'GCS - Motor Response (Min)']\n",
    "gcs_max_cols = ['GCS - Eye Opening (Max)', 'GCS - Verbal Response (Max)', 'GCS - Motor Response (Max)']\n",
    "gcs_columns = gcs_mean_cols + gcs_median_cols + gcs_min_cols + gcs_max_cols\n",
    "\n",
    "# Split dataframe into chunks\n",
    "num_chunks = 10  \n",
    "chunk_list = np.array_split(merged_mimic_df, num_chunks)\n",
    "\n",
    "# Placeholder for processed chunks\n",
    "processed_chunks = []\n",
    "\n",
    "for i, chunk in enumerate(chunk_list):\n",
    "    try:\n",
    "        logging.info(f\"Processing chunk {i + 1}/{num_chunks}...\")\n",
    "        \n",
    "        # Convert GCS columns to numeric, coercing errors to NaN\n",
    "        for col in gcs_columns:\n",
    "            chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n",
    "        \n",
    "        # Aggregate GCS values\n",
    "        chunk['GCS (Mean)'] = chunk[gcs_mean_cols].sum(axis=1, skipna=True)\n",
    "        chunk['GCS (Median)'] = chunk[gcs_median_cols].sum(axis=1, skipna=True)\n",
    "        chunk['GCS (Min)'] = chunk[gcs_min_cols].sum(axis=1, skipna=True)\n",
    "        chunk['GCS (Max)'] = chunk[gcs_max_cols].sum(axis=1, skipna=True)\n",
    "\n",
    "        # Replace rows where all original GCS columns were NaN with NaN in the new GCS columns\n",
    "        mask_mean = chunk[gcs_mean_cols].isna().all(axis=1)\n",
    "        \n",
    "        mask_median = chunk[gcs_median_cols].isna().all(axis=1)\n",
    "        mask_min = chunk[gcs_min_cols].isna().all(axis=1)\n",
    "        mask_max = chunk[gcs_max_cols].isna().all(axis=1)\n",
    "\n",
    "        chunk.loc[mask_mean, 'GCS (Mean)'] = np.nan\n",
    "        chunk.loc[mask_median, 'GCS (Median)'] = np.nan\n",
    "        chunk.loc[mask_min, 'GCS (Min)'] = np.nan\n",
    "        chunk.loc[mask_max, 'GCS (Max)'] = np.nan\n",
    "\n",
    "        # Drop the original GCS component columns\n",
    "        chunk.drop(columns=gcs_columns, inplace=True)\n",
    "\n",
    "        # Append processed chunk to the list\n",
    "        processed_chunks.append(chunk)\n",
    "        logging.info(f\"Chunk {i + 1}/{num_chunks} processed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing chunk {i + 1}: {e}\")\n",
    "\n",
    "# Concatenate all processed chunks into a single DataFrame\n",
    "merged_mimic_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "logging.info(\"All chunks processed and concatenated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c78ddc64-98a0-416d-b132-a76e60a2509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 23:02:38,380 - INFO - Starting chunk-wise processing of Braden components.\n",
      "C:\\Users\\Dimopoulos\\anaconda3\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "2025-03-22 23:02:41,051 - INFO - Processing chunk 1/10...\n",
      "2025-03-22 23:02:41,187 - INFO - Chunk 1/10 processed successfully.\n",
      "2025-03-22 23:02:41,189 - INFO - Processing chunk 2/10...\n",
      "2025-03-22 23:02:41,341 - INFO - Chunk 2/10 processed successfully.\n",
      "2025-03-22 23:02:41,342 - INFO - Processing chunk 3/10...\n",
      "2025-03-22 23:02:41,484 - INFO - Chunk 3/10 processed successfully.\n",
      "2025-03-22 23:02:41,485 - INFO - Processing chunk 4/10...\n",
      "2025-03-22 23:02:41,619 - INFO - Chunk 4/10 processed successfully.\n",
      "2025-03-22 23:02:41,620 - INFO - Processing chunk 5/10...\n",
      "2025-03-22 23:02:41,758 - INFO - Chunk 5/10 processed successfully.\n",
      "2025-03-22 23:02:41,759 - INFO - Processing chunk 6/10...\n",
      "2025-03-22 23:02:41,887 - INFO - Chunk 6/10 processed successfully.\n",
      "2025-03-22 23:02:41,888 - INFO - Processing chunk 7/10...\n",
      "2025-03-22 23:02:42,020 - INFO - Chunk 7/10 processed successfully.\n",
      "2025-03-22 23:02:42,021 - INFO - Processing chunk 8/10...\n",
      "2025-03-22 23:02:42,147 - INFO - Chunk 8/10 processed successfully.\n",
      "2025-03-22 23:02:42,148 - INFO - Processing chunk 9/10...\n",
      "2025-03-22 23:02:42,274 - INFO - Chunk 9/10 processed successfully.\n",
      "2025-03-22 23:02:42,275 - INFO - Processing chunk 10/10...\n",
      "2025-03-22 23:02:42,412 - INFO - Chunk 10/10 processed successfully.\n",
      "2025-03-22 23:02:43,152 - INFO - All chunks processed and concatenated successfully.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Starting chunk-wise processing of Braden components.\")\n",
    "\n",
    "# Define Braden columns for processing\n",
    "braden_mean_cols = [\n",
    "    'Braden Sensory Perception (Mean)', 'Braden Moisture (Mean)', 'Braden Activity (Mean)', \n",
    "    'Braden Mobility (Mean)', 'Braden Nutrition (Mean)', 'Braden Friction/Shear (Mean)'\n",
    "]\n",
    "braden_median_cols = [\n",
    "    'Braden Sensory Perception (Median)', 'Braden Moisture (Median)', 'Braden Activity (Median)', \n",
    "    'Braden Mobility (Median)', 'Braden Nutrition (Median)', 'Braden Friction/Shear (Median)'\n",
    "]\n",
    "braden_min_cols = [\n",
    "    'Braden Sensory Perception (Min)', 'Braden Moisture (Min)', 'Braden Activity (Min)', \n",
    "    'Braden Mobility (Min)', 'Braden Nutrition (Min)', 'Braden Friction/Shear (Min)'\n",
    "]\n",
    "braden_max_cols = [\n",
    "    'Braden Sensory Perception (Max)', 'Braden Moisture (Max)', 'Braden Activity (Max)', \n",
    "    'Braden Mobility (Max)', 'Braden Nutrition (Max)', 'Braden Friction/Shear (Max)'\n",
    "]\n",
    "braden_columns = braden_mean_cols + braden_median_cols + braden_min_cols + braden_max_cols\n",
    "\n",
    "# Split dataframe into chunks\n",
    "num_chunks = 10\n",
    "chunk_list = np.array_split(merged_mimic_df, num_chunks)\n",
    "\n",
    "# Placeholder for processed chunks\n",
    "processed_chunks = []\n",
    "\n",
    "for i, chunk in enumerate(chunk_list):\n",
    "    try:\n",
    "        logging.info(f\"Processing chunk {i + 1}/{num_chunks}...\")\n",
    "        \n",
    "        # Convert Braden columns to numeric, coercing errors to NaN\n",
    "        for col in braden_columns:\n",
    "            chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n",
    "        \n",
    "        # Aggregate Braden values\n",
    "        chunk['Braden (Mean)'] = chunk[braden_mean_cols].sum(axis=1, skipna=True)\n",
    "        chunk['Braden (Median)'] = chunk[braden_median_cols].sum(axis=1, skipna=True)\n",
    "        chunk['Braden (Min)'] = chunk[braden_min_cols].sum(axis=1, skipna=True)\n",
    "        chunk['Braden (Max)'] = chunk[braden_max_cols].sum(axis=1, skipna=True)\n",
    "\n",
    "        # Replace rows where all original Braden columns were NaN with NaN in the new Braden columns\n",
    "        mask_mean = chunk[braden_mean_cols].isna().all(axis=1)\n",
    "        mask_median = chunk[braden_median_cols].isna().all(axis=1)\n",
    "        mask_min = chunk[braden_min_cols].isna().all(axis=1)\n",
    "        mask_max = chunk[braden_max_cols].isna().all(axis=1)\n",
    "\n",
    "        chunk.loc[mask_mean, 'Braden (Mean)'] = np.nan\n",
    "        chunk.loc[mask_median, 'Braden (Median)'] = np.nan\n",
    "        chunk.loc[mask_min, 'Braden (Min)'] = np.nan\n",
    "        chunk.loc[mask_max, 'Braden (Max)'] = np.nan\n",
    "\n",
    "        # Drop the original Braden component columns\n",
    "        chunk.drop(columns=braden_columns, inplace=True)\n",
    "\n",
    "        # Append processed chunk to the list\n",
    "        processed_chunks.append(chunk)\n",
    "        logging.info(f\"Chunk {i + 1}/{num_chunks} processed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing chunk {i + 1}: {e}\")\n",
    "\n",
    "# Concatenate all processed chunks into a single DataFrame\n",
    "merged_mimic_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "logging.info(\"All chunks processed and concatenated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aebcb5bd-7c26-42b0-8af5-c0a705cd4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces and commas\n",
    "merged_mimic_df.columns = merged_mimic_df.columns.str.replace(r'[ ,]+', '_', regex=True)\n",
    "\n",
    "# Drop second column from the column_names_df\n",
    "mimic_columns_to_keep.drop(columns=['Unnamed: 1'], inplace=True)\n",
    "\n",
    "# Extract column names from columns_to_keep DataFrame\n",
    "columns_to_keep_names = mimic_columns_to_keep['column'].tolist()\n",
    "\n",
    "# Select only the desired columns\n",
    "mimic_temp = merged_mimic_df[columns_to_keep_names]\n",
    "\n",
    "# Remove Duplicate Columns\n",
    "df_mimic_unique = mimic_temp.loc[:, ~mimic_temp.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7922f0a8-5e05-4cd9-a48e-4a18c3d89412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply values by 4 in 'Ionized Calcium' column, leaving NaN values unchanged for normalization with eicu\n",
    "df_mimic_unique.loc[:, 'Ionized_Calcium_(Max)'] = mimic_temp['Ionized_Calcium_(Max)'].apply(lambda x: x * 4 if pd.notna(x) else x)\n",
    "df_mimic_unique.loc[:, 'Ionized_Calcium_(Mean)'] = mimic_temp['Ionized_Calcium_(Mean)'].apply(lambda x: x * 4 if pd.notna(x) else x)\n",
    "df_mimic_unique.loc[:, 'Ionized_Calcium_(Median)'] = mimic_temp['Ionized_Calcium_(Median)'].apply(lambda x: x * 4 if pd.notna(x) else x)\n",
    "df_mimic_unique.loc[:, 'Ionized_Calcium_(Min)'] = mimic_temp['Ionized_Calcium_(Min)'].apply(lambda x: x * 4 if pd.notna(x) else x)\n",
    "\n",
    "# Make a copy df_mimic_unique in order to avoid SettingWithCopyWarning\n",
    "df_mimic_unique = df_mimic_unique.copy()\n",
    "\n",
    "# Glucose merge - calculate the mean for each aggregation type and handle NaN values\n",
    "df_mimic_unique['Glucose (Max)'] = df_mimic_unique[['Glucose_(Max)', 'Glucose_(Max).1', 'Glucose_(Max).2']].mean(axis=1)\n",
    "df_mimic_unique['Glucose (Mean)'] = df_mimic_unique[['Glucose_(Mean)', 'Glucose_(Mean).1', 'Glucose_(Mean).2']].mean(axis=1)\n",
    "df_mimic_unique['Glucose (Median)'] = df_mimic_unique[['Glucose_(Median)', 'Glucose_(Median).1', 'Glucose_(Median).2']].mean(axis=1)\n",
    "df_mimic_unique['Glucose (Min)'] = df_mimic_unique[['Glucose_(Min)', 'Glucose_(Min).1', 'Glucose_(Min).2']].mean(axis=1)\n",
    "\n",
    "# Drop original Glucose columns to keep only the summarized columns\n",
    "df_mimic_unique.drop(columns=[\n",
    "    'Glucose_(Max)', 'Glucose_(Max).1', 'Glucose_(Max).2',\n",
    "    'Glucose_(Mean)', 'Glucose_(Mean).1', 'Glucose_(Mean).2',\n",
    "    'Glucose_(Median)', 'Glucose_(Median).1', 'Glucose_(Median).2',\n",
    "    'Glucose_(Min)', 'Glucose_(Min).1', 'Glucose_(Min).2'\n",
    "], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "283e032b-e607-4ae0-8e22-c91c8686b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 23:04:16,385 - INFO - Starting the pH column processing.\n",
      "2025-03-22 23:04:16,532 - INFO - Calculating 'pH (Max)' column.\n",
      "2025-03-22 23:04:59,228 - INFO - Calculating 'pH (Mean)' column.\n",
      "2025-03-22 23:05:40,445 - INFO - Calculating 'pH (Median)' column.\n",
      "2025-03-22 23:06:21,719 - INFO - Calculating 'pH (Min)' column.\n",
      "2025-03-22 23:07:01,789 - INFO - Dropping original pH columns.\n",
      "2025-03-22 23:07:01,835 - INFO - pH column processing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Starting the pH column processing.\")\n",
    "\n",
    "# Make a copy of df_mimic_unique to avoid SettingWithCopyWarning\n",
    "df_mimic_unique = df_mimic_unique.copy()\n",
    "\n",
    "# pH merge\n",
    "logging.info(\"Calculating 'pH (Max)' column.\")\n",
    "df_mimic_unique.loc[:, 'pH (Max)'] = df_mimic_unique.apply(\n",
    "    lambda row: row[['pH_(Max)', 'pH_(Max).1', 'pH_(Max).3']].mean()\n",
    "    if not all(row[['pH_(Max)', 'pH_(Max).1', 'pH_(Max).3']].isna())\n",
    "    else np.nan, axis=1\n",
    ")\n",
    "\n",
    "logging.info(\"Calculating 'pH (Mean)' column.\")\n",
    "df_mimic_unique.loc[:, 'pH (Mean)'] = df_mimic_unique.apply(\n",
    "    lambda row: row[['pH_(Mean)', 'pH_(Mean).1', 'pH_(Mean).2', 'pH_(Mean).3']].mean()\n",
    "    if not all(row[['pH_(Mean)', 'pH_(Mean).1', 'pH_(Mean).2', 'pH_(Mean).3']].isna())\n",
    "    else np.nan, axis=1\n",
    ")\n",
    "\n",
    "logging.info(\"Calculating 'pH (Median)' column.\")\n",
    "df_mimic_unique.loc[:, 'pH (Median)'] = df_mimic_unique.apply(\n",
    "    lambda row: row[['pH_(Median)', 'pH_(Median).1', 'pH_(Median).3']].mean()\n",
    "    if not all(row[['pH_(Median)', 'pH_(Median).1', 'pH_(Median).3']].isna())\n",
    "    else np.nan, axis=1\n",
    ")\n",
    "\n",
    "logging.info(\"Calculating 'pH (Min)' column.\")\n",
    "df_mimic_unique.loc[:, 'pH (Min)'] = df_mimic_unique.apply(\n",
    "    lambda row: row[['pH_(Min)', 'pH_(Min).1', 'pH_(Min).3']].mean()\n",
    "    if not all(row[['pH_(Min)', 'pH_(Min).1', 'pH_(Min).3']].isna())\n",
    "    else np.nan, axis=1\n",
    ")\n",
    "\n",
    "# Drop original pH columns to keep only the summarized columns\n",
    "logging.info(\"Dropping original pH columns.\")\n",
    "df_mimic_unique.drop(columns=[\n",
    "    'pH_(Max)', 'pH_(Max).1', 'pH_(Max).3',\n",
    "    'pH_(Mean)', 'pH_(Mean).1', 'pH_(Mean).2', 'pH_(Mean).3',\n",
    "    'pH_(Median)', 'pH_(Median).1', 'pH_(Median).3',\n",
    "    'pH_(Min)', 'pH_(Min).1', 'pH_(Min).3'\n",
    "], inplace=True)\n",
    "\n",
    "logging.info(\"pH column processing completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b02c8-4585-4384-ad73-38fa9ae9464a",
   "metadata": {},
   "source": [
    "# eICU Ver. 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bee654d-c994-4182-8461-a0d38ddd565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 23:08:03,695 - INFO - Starting the merging process for eICU dataframes.\n",
      "2025-03-22 23:08:03,697 - INFO - Merging eicu_meam_df and eicu_median_df...\n",
      "2025-03-22 23:08:03,946 - INFO - Merged eicu_meam_df and eicu_median_df successfully.\n",
      "2025-03-22 23:08:03,947 - INFO - Merging with eicu_min_df...\n",
      "2025-03-22 23:08:04,393 - INFO - Merged with eicu_min_df successfully.\n",
      "2025-03-22 23:08:04,394 - INFO - Merging with eicu_max_df...\n",
      "2025-03-22 23:08:05,004 - INFO - Merged with eicu_max_df successfully.\n",
      "2025-03-22 23:08:05,006 - INFO - Reordering columns: moving 'unitdischargestatus' and 'LOS' to the end...\n",
      "2025-03-22 23:08:05,703 - INFO - Reordered columns successfully.\n",
      "2025-03-22 23:08:05,704 - INFO - Renaming the last two columns to preserve original names...\n",
      "2025-03-22 23:08:05,705 - INFO - Renamed the last two columns successfully.\n",
      "2025-03-22 23:08:05,707 - INFO - Merging process for eICU dataframes completed successfully.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Starting the merging process for eICU dataframes.\")\n",
    "\n",
    "try:\n",
    "    # Merge eICU dataframes\n",
    "    logging.info(\"Merging eicu_meam_df and eicu_median_df...\")\n",
    "    merged_eicu_df = eicu_mean_df.merge(\n",
    "        eicu_median_df, \n",
    "        on=['row_count', 'uniquepid', 'patientunitstayid', 'Time_Zone', 'gender', 'age', 'ethnicity', 'unitdischargestatus', 'LOS'], \n",
    "        suffixes=('_mean', '_median')\n",
    "    )\n",
    "    logging.info(\"Merged eicu_meam_df and eicu_median_df successfully.\")\n",
    "\n",
    "    logging.info(\"Merging with eicu_min_df...\")\n",
    "    merged_eicu_df = merged_eicu_df.merge(\n",
    "        eicu_min_df, \n",
    "        on=['row_count', 'uniquepid', 'patientunitstayid', 'Time_Zone', 'gender', 'age', 'ethnicity', 'unitdischargestatus', 'LOS'], \n",
    "        suffixes=('', '_min')\n",
    "    )\n",
    "    logging.info(\"Merged with eicu_min_df successfully.\")\n",
    "\n",
    "    logging.info(\"Merging with eicu_max_df...\")\n",
    "    merged_eicu_df = merged_eicu_df.merge(\n",
    "        eicu_max_df, \n",
    "        on=['row_count', 'uniquepid', 'patientunitstayid', 'Time_Zone', 'gender', 'age', 'ethnicity', 'unitdischargestatus', 'LOS'], \n",
    "        suffixes=('', '_max')\n",
    "    )\n",
    "    logging.info(\"Merged with eicu_max_df successfully.\")\n",
    "\n",
    "    # Move the 'unitdischargestatus' and 'LOS' columns to the end of the dataframe\n",
    "    logging.info(\"Reordering columns: moving 'unitdischargestatus' and 'LOS' to the end...\")\n",
    "    unitdischargestatus_column = merged_eicu_df.pop('unitdischargestatus')\n",
    "    los_column = merged_eicu_df.pop('LOS')\n",
    "    merged_eicu_df = pd.concat([merged_eicu_df, unitdischargestatus_column, los_column], axis=1)\n",
    "    logging.info(\"Reordered columns successfully.\")\n",
    "\n",
    "    # Rename the last two columns to preserve their original names\n",
    "    logging.info(\"Renaming the last two columns to preserve original names...\")\n",
    "    merged_eicu_df.columns = list(merged_eicu_df.columns[:-2]) + ['unitdischargestatus', 'LOS']\n",
    "    logging.info(\"Renamed the last two columns successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during the merging process: {e}\")\n",
    "\n",
    "logging.info(\"Merging process for eICU dataframes completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e30d5097-b7e1-4308-9a83-8f6928fdd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop second column from the column_names_df\n",
    "eicu_columns_to_keep.drop(columns=['Unnamed: 1'], inplace=True)\n",
    "\n",
    "# Extract column names from columns_to_keep DataFrame\n",
    "columns_to_keep_names = eicu_columns_to_keep['column'].tolist()\n",
    "\n",
    "# Select only the desired columns\n",
    "eicu_temp = merged_eicu_df[columns_to_keep_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aee1426f-941b-48d3-abf0-bab26e805919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimopoulos\\AppData\\Local\\Temp\\ipykernel_3544\\1866381996.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  eicu_temp.loc[:, 'unitdischargestatus'] = eicu_temp['unitdischargestatus'].replace({'Alive': 0, 'Expired': 1})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"--------Replace Block----------\"\"\"\n",
    "# Make a copy df_mimic_unique in order to avoid SettingWithCopyWarning\n",
    "eicu_temp = eicu_temp.copy()\n",
    "\n",
    "# Replace 'Alive' with 0 and 'Expired' with 1 in the 'unitdischargestatus' column\n",
    "eicu_temp.loc[:, 'unitdischargestatus'] = eicu_temp['unitdischargestatus'].replace({'Alive': 0, 'Expired': 1})\n",
    "\n",
    "# Replace 'Female' with 'F' and 'Male' with 'M' in the 'gender' column\n",
    "eicu_temp.loc[:, 'gender'] = eicu_temp['gender'].replace({'Female': 'F', 'Male': 'M'})\n",
    "\n",
    "\n",
    "# Replace values in the 'ethnicity' column for standardization\n",
    "eicu_temp.loc[:, 'ethnicity'] = eicu_temp['ethnicity'].replace({\n",
    "    'African American': 'BLACK/AFRICAN AMERICAN',\n",
    "    'Caucasian': 'WHITE',\n",
    "    'Hispanic': 'HISPANIC OR LATINO',\n",
    "    'Asian': 'ASIAN',\n",
    "    'Native American': 'AMERICAN INDIAN/ALASKA NATIVE',\n",
    "    'Other/Unknown': 'UNKNOWN'\n",
    "})\n",
    "\n",
    "# Replace age values higher than 89 with 90, and convert age to integer\n",
    "eicu_temp.loc[:, 'age'] = eicu_temp['age'].replace('> 89', 90)\n",
    "eicu_temp.loc[:, 'age'] = eicu_temp['age'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0dae583-affb-4854-b300-e388789290e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces and commas\n",
    "df_mimic_unique.columns = df_mimic_unique.columns.str.replace(r'[ ,]+', '_', regex=True)\n",
    "\n",
    "eicu_temp.columns = eicu_temp.columns.str.replace(r'[ ,]+', '_', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58138c89-8eb6-496b-a454-fc9f54c112e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 23:08:57,589 - INFO - Starting the processing of bedside glucose columns in eicu_temp.\n",
      "2025-03-22 23:08:57,667 - INFO - Calculating 'bedside_glucose (Max)' column.\n",
      "2025-03-22 23:10:02,906 - INFO - Calculating 'bedside_glucose (Mean)' column.\n",
      "2025-03-22 23:11:09,312 - INFO - Calculating 'bedside_glucose (Median)' column.\n",
      "2025-03-22 23:12:15,566 - INFO - Calculating 'bedside_glucose (Min)' column.\n",
      "2025-03-22 23:13:22,081 - INFO - Dropping original bedside glucose columns.\n",
      "2025-03-22 23:13:22,148 - INFO - Replacing spaces and commas in column names with underscores.\n",
      "2025-03-22 23:13:22,150 - INFO - Processing of bedside glucose columns completed successfully.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Starting the processing of bedside glucose columns in eicu_temp.\")\n",
    "\n",
    "try:\n",
    "    # Make a copy of eicu_temp to avoid SettingWithCopyWarning\n",
    "    eicu_temp = eicu_temp.copy()\n",
    "\n",
    "    # Calculate 'bedside_glucose (Max)'\n",
    "    logging.info(\"Calculating 'bedside_glucose (Max)' column.\")\n",
    "    eicu_temp.loc[:, 'bedside_glucose (Max)'] = eicu_temp.apply(\n",
    "        lambda row: row[['bedside_glucose_(Max)', 'Bedside_Glucose_(Max)']].mean()\n",
    "        if not all(row[['bedside_glucose_(Max)', 'Bedside_Glucose_(Max)']].isna())\n",
    "        else np.nan, axis=1\n",
    "    )\n",
    "\n",
    "    # Calculate 'bedside_glucose (Mean)'\n",
    "    logging.info(\"Calculating 'bedside_glucose (Mean)' column.\")\n",
    "    eicu_temp.loc[:, 'bedside_glucose (Mean)'] = eicu_temp.apply(\n",
    "        lambda row: row[['bedside_glucose_(Mean)', 'Bedside_Glucose_(Mean)']].mean()\n",
    "        if not all(row[['bedside_glucose_(Mean)', 'Bedside_Glucose_(Mean)']].isna())\n",
    "        else np.nan, axis=1\n",
    "    )\n",
    "\n",
    "    # Calculate 'bedside_glucose (Median)'\n",
    "    logging.info(\"Calculating 'bedside_glucose (Median)' column.\")\n",
    "    eicu_temp.loc[:, 'bedside_glucose (Median)'] = eicu_temp.apply(\n",
    "        lambda row: row[['bedside_glucose_(Median)', 'Bedside_Glucose_(Median)']].mean()\n",
    "        if not all(row[['bedside_glucose_(Median)', 'Bedside_Glucose_(Median)']].isna())\n",
    "        else np.nan, axis=1\n",
    "    )\n",
    "\n",
    "    # Calculate 'bedside_glucose (Min)'\n",
    "    logging.info(\"Calculating 'bedside_glucose (Min)' column.\")\n",
    "    eicu_temp.loc[:, 'bedside_glucose (Min)'] = eicu_temp.apply(\n",
    "        lambda row: row[['bedside_glucose_(Min)', 'Bedside_Glucose_(Min)']].mean()\n",
    "        if not all(row[['bedside_glucose_(Min)', 'Bedside_Glucose_(Min)']].isna())\n",
    "        else np.nan, axis=1\n",
    "    )\n",
    "\n",
    "    # Drop original bedside glucose columns\n",
    "    logging.info(\"Dropping original bedside glucose columns.\")\n",
    "    eicu_temp.drop(columns=[\n",
    "        'bedside_glucose_(Max)', 'Bedside_Glucose_(Max)',\n",
    "        'bedside_glucose_(Mean)', 'Bedside_Glucose_(Mean)',\n",
    "        'bedside_glucose_(Median)', 'Bedside_Glucose_(Median)',\n",
    "        'bedside_glucose_(Min)', 'Bedside_Glucose_(Min)'\n",
    "    ], inplace=True)\n",
    "\n",
    "    # Replace spaces or commas in column names with underscores\n",
    "    logging.info(\"Replacing spaces and commas in column names with underscores.\")\n",
    "    eicu_temp.columns = eicu_temp.columns.str.replace(r'[ ,]+', '_', regex=True)\n",
    "\n",
    "    logging.info(\"Processing of bedside glucose columns completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during the processing of bedside glucose columns: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ffb609c-7de6-4ba0-b5b3-a1ffd1a0c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename eICU header to align with mimics\n",
    "column_eicu_mapping = {\n",
    "    'column': 'column',\n",
    "    'row_count': 'row_count',\n",
    "    'uniquepid': 'subject_id',\n",
    "    'patientunitstayid': 'hadm_id',\n",
    "    'Time_Zone': 'Time_Zone',\n",
    "    'gender': 'gender',\n",
    "    'age': 'age',\n",
    "    'ethnicity': 'race',\n",
    "    'Base_Excess_(Max)': 'Base_Excess_(Max)',\n",
    "    'Base_Excess_(Mean)': 'Base_Excess_(Mean)',\n",
    "    'Base_Excess_(Median)': 'Base_Excess_(Median)',\n",
    "    'Base_Excess_(Min)': 'Base_Excess_(Min)',\n",
    "    'lactate_(Max)': 'Lactate_(Max)',\n",
    "    'lactate_(Mean)': 'Lactate_(Mean)',\n",
    "    'lactate_(Median)': 'Lactate_(Median)',\n",
    "    'lactate_(Min)': 'Lactate_(Min)',\n",
    "    'paCO2_(Max)': 'pCO2_(Max)',\n",
    "    'paCO2_(Mean)': 'pCO2_(Mean)',\n",
    "    'paCO2_(Median)': 'pCO2_(Median)',\n",
    "    'paCO2_(Min)': 'pCO2_(Min)',\n",
    "    'Total_CO2_(Max)': 'Calculated_Total_CO2_(Max)',\n",
    "    'Total_CO2_(Mean)': 'Calculated_Total_CO2_(Mean)',\n",
    "    'Total_CO2_(Median)': 'Calculated_Total_CO2_(Median)',\n",
    "    'Total_CO2_(Min)': 'Calculated_Total_CO2_(Min)',\n",
    "    'BUN_(Max)': 'BUN_(Max)',\n",
    "    'BUN_(Mean)': 'BUN_(Mean)',\n",
    "    'BUN_(Median)': 'BUN_(Median)',\n",
    "    'BUN_(Min)': 'BUN_(Min)',\n",
    "    'pH_(Max)': 'pH_(Max)',\n",
    "    'pH_(Mean)': 'pH_(Mean)',\n",
    "    'pH_(Median)': 'pH_(Median)',\n",
    "    'pH_(Min)': 'pH_(Min)',\n",
    "    'paO2_(Max)': 'pO2_(Max)',\n",
    "    'paO2_(Mean)': 'pO2_(Mean)',\n",
    "    'paO2_(Median)': 'pO2_(Median)',\n",
    "    'paO2_(Min)': 'pO2_(Min)',\n",
    "    'ALT_(SGPT)_(Max)': 'Alanine_Aminotransferase_(ALT)_(Max)',\n",
    "    'ALT_(SGPT)_(Mean)': 'Alanine_Aminotransferase_(ALT)_(Mean)',\n",
    "    'ALT_(SGPT)_(Median)': 'Alanine_Aminotransferase_(ALT)_(Median)',\n",
    "    'ALT_(SGPT)_(Min)': 'Alanine_Aminotransferase_(ALT)_(Min)',\n",
    "    'alkaline_phos._(Max)': 'Alkaline_Phosphatase_(Max)',\n",
    "    'alkaline_phos._(Mean)': 'Alkaline_Phosphatase_(Mean)',\n",
    "    'alkaline_phos._(Median)': 'Alkaline_Phosphatase_(Median)',\n",
    "    'alkaline_phos._(Min)': 'Alkaline_Phosphatase_(Min)',\n",
    "    'anion_gap_(Max)': 'Anion_Gap_(Max)',\n",
    "    'anion_gap_(Mean)': 'Anion_Gap_(Mean)',\n",
    "    'anion_gap_(Median)': 'Anion_Gap_(Median)',\n",
    "    'anion_gap_(Min)': 'Anion_Gap_(Min)',\n",
    "    'AST_(SGOT)_(Max)': 'Asparate_Aminotransferase_(AST)_(Max)',\n",
    "    'AST_(SGOT)_(Mean)': 'Asparate_Aminotransferase_(AST)_(Mean)',\n",
    "    'AST_(SGOT)_(Median)': 'Asparate_Aminotransferase_(AST)_(Median)',\n",
    "    'AST_(SGOT)_(Min)': 'Asparate_Aminotransferase_(AST)_(Min)',\n",
    "    'bicarbonate_(Max)': 'Bicarbonate_(Max)',\n",
    "    'bicarbonate_(Mean)': 'Bicarbonate_(Mean)',\n",
    "    'bicarbonate_(Median)': 'Bicarbonate_(Median)',\n",
    "    'bicarbonate_(Min)': 'Bicarbonate_(Min)',\n",
    "    'chloride_(Max)': 'Chloride_(Max)',\n",
    "    'chloride_(Mean)': 'Chloride_(Mean)',\n",
    "    'chloride_(Median)': 'Chloride_(Median)',\n",
    "    'chloride_(Min)': 'Chloride_(Min)',\n",
    "    'creatinine_(Max)': 'Creatinine_(Max)',\n",
    "    'creatinine_(Mean)': 'Creatinine_(Mean)',\n",
    "    'creatinine_(Median)': 'Creatinine_(Median)',\n",
    "    'creatinine_(Min)': 'Creatinine_(Min)',\n",
    "    'glucose_(Max)': 'Glucose_(Max)',\n",
    "    'glucose_(Mean)': 'Glucose_(Mean)',\n",
    "    'glucose_(Median)': 'Glucose_(Median)',\n",
    "    'glucose_(Min)': 'Glucose_(Min)',\n",
    "    'magnesium_(Max)': 'Magnesium_(Max)',\n",
    "    'magnesium_(Mean)': 'Magnesium_(Mean)',\n",
    "    'magnesium_(Median)': 'Magnesium_(Median)',\n",
    "    'magnesium_(Min)': 'Magnesium_(Min)',\n",
    "    'phosphate_(Max)': 'Phosphate_(Max)',\n",
    "    'phosphate_(Mean)': 'Phosphate_(Mean)',\n",
    "    'phosphate_(Median)': 'Phosphate_(Median)',\n",
    "    'phosphate_(Min)': 'Phosphate_(Min)',\n",
    "    'potassium_(Max)': 'Potassium_(Max)',\n",
    "    'potassium_(Mean)': 'Potassium_(Mean)',\n",
    "    'potassium_(Median)': 'Potassium_(Median)',\n",
    "    'potassium_(Min)': 'Potassium_(Min)',\n",
    "    'sodium_(Max)': 'Sodium_(Max)',\n",
    "    'sodium_(Mean)': 'Sodium_(Mean)',\n",
    "    'sodium_(Median)': 'Sodium_(Median)',\n",
    "    'sodium_(Min)': 'Sodium_(Min)',\n",
    "    'Hct_(Max)': 'Hematocrit_(Max)',\n",
    "    'Hct_(Mean)': 'Hematocrit_(Mean)',\n",
    "    'Hct_(Median)': 'Hematocrit_(Median)',\n",
    "    'Hct_(Min)': 'Hematocrit_(Min)',\n",
    "    'Hgb_(Max)': 'Hemoglobin_(Max)',\n",
    "    'Hgb_(Mean)': 'Hemoglobin_(Mean)',\n",
    "    'Hgb_(Median)': 'Hemoglobin_(Median)',\n",
    "    'Hgb_(Min)': 'Hemoglobin_(Min)',\n",
    "    'PT_-_INR_(Max)': 'INR(PT)_(Max)',\n",
    "    'PT_-_INR_(Mean)': 'INR(PT)_(Mean)',\n",
    "    'PT_-_INR_(Median)': 'INR(PT)_(Median)',\n",
    "    'PT_-_INR_(Min)': 'INR(PT)_(Min)',\n",
    "    'MCH_(Max)': 'MCH_(Max)',\n",
    "    'MCH_(Mean)': 'MCH_(Mean)',\n",
    "    'MCH_(Median)': 'MCH_(Median)',\n",
    "    'MCH_(Min)': 'MCH_(Min)',\n",
    "    'MCHC_(Max)': 'MCHC_(Max)',\n",
    "    'MCHC_(Mean)': 'MCHC_(Mean)',\n",
    "    'MCHC_(Median)': 'MCHC_(Median)',\n",
    "    'MCHC_(Min)': 'MCHC_(Min)',\n",
    "    'MCV_(Max)': 'MCV_(Max)',\n",
    "    'MCV_(Mean)': 'MCV_(Mean)',\n",
    "    'MCV_(Median)': 'MCV_(Median)',\n",
    "    'MCV_(Min)': 'MCV_(Min)',\n",
    "    'platelets_x_1000_(Max)': 'Platelet_Count_(Max)',\n",
    "    'platelets_x_1000_(Mean)': 'Platelet_Count_(Mean)',\n",
    "    'platelets_x_1000_(Median)': 'Platelet_Count_(Median)',\n",
    "    'platelets_x_1000_(Min)': 'Platelet_Count_(Min)',\n",
    "    'PT_(Max)': 'PT_(Max)',\n",
    "    'PT_(Mean)': 'PT_(Mean)',\n",
    "    'PT_(Median)': 'PT_(Median)',\n",
    "    'PT_(Min)': 'PT_(Min)',\n",
    "    'PTT_(Max)': 'PTT_(Max)',\n",
    "    'PTT_(Mean)': 'PTT_(Mean)',\n",
    "    'PTT_(Median)': 'PTT_(Median)',\n",
    "    'PTT_(Min)': 'PTT_(Min)',\n",
    "    'RDW_(Max)': 'RDW_(Max)',\n",
    "    'RDW_(Mean)': 'RDW_(Mean)',\n",
    "    'RDW_(Median)': 'RDW_(Median)',\n",
    "    'RDW_(Min)': 'RDW_(Min)',\n",
    "    'RBC_(Max)': 'Red_Blood_Cells_(Max)',\n",
    "    'RBC_(Mean)': 'Red_Blood_Cells_(Mean)',\n",
    "    'RBC_(Median)': 'Red_Blood_Cells_(Median)',\n",
    "    'RBC_(Min)': 'Red_Blood_Cells_(Min)',\n",
    "    'WBC_x_1000_(Max)': 'White_Blood_Cells_(Max)',\n",
    "    'WBC_x_1000_(Mean)': 'White_Blood_Cells_(Mean)',\n",
    "    'WBC_x_1000_(Median)': 'White_Blood_Cells_(Median)',\n",
    "    'WBC_x_1000_(Min)': 'White_Blood_Cells_(Min)',\n",
    "    'Heart_Rate_(Max)': 'Heart_Rate_(bpm)_(Max)',\n",
    "    'Heart_Rate_(Mean)': 'Heart_Rate_(bpm)_(Mean)',\n",
    "    'Heart_Rate_(Median)': 'Heart_Rate_(bpm)_(Median)',\n",
    "    'Heart_Rate_(Min)': 'Heart_Rate_(bpm)_(Min)',\n",
    "    'Non-Invasive_BP_Diastolic_(Max)': 'Non_Invasive_Blood_Pressure_systolic_(mmHg)_(Max)',\n",
    "    'Non-Invasive_BP_Diastolic_(Mean)': 'Non_Invasive_Blood_Pressure_systolic_(mmHg)_(Mean)',\n",
    "    'Non-Invasive_BP_Diastolic_(Median)': 'Non_Invasive_Blood_Pressure_systolic_(mmHg)_(Median)',\n",
    "    'Non-Invasive_BP_Diastolic_(Min)': 'Non_Invasive_Blood_Pressure_systolic_(mmHg)_(Min)',\n",
    "    'Non-Invasive_BP_Systolic_(Max)': 'Non_Invasive_Blood_Pressure_diastolic_(mmHg)_(Max)',\n",
    "    'Non-Invasive_BP_Systolic_(Mean)': 'Non_Invasive_Blood_Pressure_diastolic_(mmHg)_(Mean)',\n",
    "    'Non-Invasive_BP_Systolic_(Median)': 'Non_Invasive_Blood_Pressure_diastolic_(mmHg)_(Median)',\n",
    "    'Non-Invasive_BP_Systolic_(Min)': 'Non_Invasive_Blood_Pressure_diastolic_(mmHg)_(Min)',\n",
    "    'Non-Invasive_BP_Mean_(Max)': 'Non_Invasive_Blood_Pressure_mean_(mmHg)_(Max)',\n",
    "    'Non-Invasive_BP_Mean_(Mean)': 'Non_Invasive_Blood_Pressure_mean_(mmHg)_(Mean)',\n",
    "    'Non-Invasive_BP_Mean_(Median)': 'Non_Invasive_Blood_Pressure_mean_(mmHg)_(Median)',\n",
    "    'Non-Invasive_BP_Mean_(Min)': 'Non_Invasive_Blood_Pressure_mean_(mmHg)_(Min)',\n",
    "    'Respiratory_Rate_(Max)': 'Respiratory_Rate_(insp/min)_(Max)',\n",
    "    'Respiratory_Rate_(Mean)': 'Respiratory_Rate_(insp/min)_(Mean)',\n",
    "    'Respiratory_Rate_(Median)': 'Respiratory_Rate_(insp/min)_(Median)',\n",
    "    'Respiratory_Rate_(Min)': 'Respiratory_Rate_(insp/min)_(Min)',\n",
    "    'O2_Saturation_(Max)': 'O2_saturation_pulseoxymetry_(%)_(Max)',\n",
    "    'O2_Saturation_(Mean)': 'O2_saturation_pulseoxymetry_(%)_(Mean)',\n",
    "    'O2_Saturation_(Median)': 'O2_saturation_pulseoxymetry_(%)_(Median)',\n",
    "    'O2_Saturation_(Min)': 'O2_saturation_pulseoxymetry_(%)_(Min)',\n",
    "    'CI_(Max)': 'Chloride_(serum)_(Max)',\n",
    "    'CI_(Mean)': 'Chloride_(serum)_(Mean)',\n",
    "    'CI_(Median)': 'Chloride_(serum)_(Median)',\n",
    "    'CI_(Min)': 'Chloride_(serum)_(Min)',\n",
    "    'calcium_(Max)': 'Calcium_non-ionized_(Max)',\n",
    "    'calcium_(Mean)': 'Calcium_non-ionized_(Mean)',\n",
    "    'calcium_(Median)': 'Calcium_non-ionized_(Median)',\n",
    "    'calcium_(Min)': 'Calcium_non-ionized_(Min)',\n",
    "    'CPK_(Max)': 'CK_(CPK)_(Max)',\n",
    "    'CPK_(Mean)': 'CK_(CPK)_(Mean)',\n",
    "    'CPK_(Median)': 'CK_(CPK)_(Median)',\n",
    "    'CPK_(Min)': 'CK_(CPK)_(Min)',\n",
    "    'Temperature_(F)_(Max)': 'Temperature_Fahrenheit_(F)_(Max)',\n",
    "    'Temperature_(F)_(Mean)': 'Temperature_Fahrenheit_(F)_(Mean)',\n",
    "    'Temperature_(F)_(Median)': 'Temperature_Fahrenheit_(F)_(Median)',\n",
    "    'Temperature_(F)_(Min)': 'Temperature_Fahrenheit_(F)_(Min)',\n",
    "    'Pain_Score_(Max)': 'Pain_Level_(Max)',\n",
    "    'Pain_Score_(Mean)': 'Pain_Level_(Mean)',\n",
    "    'Pain_Score_(Median)': 'Pain_Level_(Median)',\n",
    "    'Pain_Score_(Min)': 'Pain_Level_(Min)',\n",
    "    'LPM_O2_(Max)': 'O2_Flow_(L/min)_(Max)',\n",
    "    'LPM_O2_(Mean)': 'O2_Flow_(L/min)_(Mean)',\n",
    "    'LPM_O2_(Median)': 'O2_Flow_(L/min)_(Median)',\n",
    "    'LPM_O2_(Min)': 'O2_Flow_(L/min)_(Min)',\n",
    "    'O2_L/%_(Max)': 'Inspired_O2_Fraction_(Max)',\n",
    "    'O2_L/%_(Mean)': 'Inspired_O2_Fraction_(Mean)',\n",
    "    'O2_L/%_(Median)': 'Inspired_O2_Fraction_(Median)',\n",
    "    'O2_L/%_(Min)': 'Inspired_O2_Fraction_(Min)',\n",
    "    'ionized_calcium_(Max)': 'Ionized_Calcium_(Max)',\n",
    "    'ionized_calcium_(Mean)': 'Ionized_Calcium_(Mean)',\n",
    "    'ionized_calcium_(Median)': 'Ionized_Calcium_(Median)',\n",
    "    'ionized_calcium_(Min)': 'Ionized_Calcium_(Min)',\n",
    "    'albumin_(Max)': 'Albumin_(Max)',\n",
    "    'albumin_(Mean)': 'Albumin_(Mean)',\n",
    "    'albumin_(Median)': 'Albumin_(Median)',\n",
    "    'albumin_(Min)': 'Albumin_(Min)',\n",
    "    'GCS_Total_(Max)': 'GCS_(Max)',\n",
    "    'GCS_Total_(Mean)': 'GCS_(Mean)',\n",
    "    'GCS_Total_(Median)': 'GCS_(Median)',\n",
    "    'GCS_Total_(Min)': 'GCS_(Min)',\n",
    "    'LDH_(Max)': 'LDH_(Max)',\n",
    "    'LDH_(Mean)': 'LDH_(Mean)',\n",
    "    'LDH_(Median)': 'LDH_(Median)',\n",
    "    'LDH_(Min)': 'LDH_(Min)',\n",
    "    'ethanol_(Max)': 'ETOH_(Max)',\n",
    "    'ethanol_(Mean)': 'ETOH_(Mean)',\n",
    "    'ethanol_(Median)': 'ETOH_(Median)',\n",
    "    'ethanol_(Min)': 'ETOH_(Min)',\n",
    "    'Invasive_BP_Systolic_(Max)': 'Arterial_Blood_Pressure_systolic_(mmHg)_(Max)',\n",
    "    'Invasive_BP_Systolic_(Mean)': 'Arterial_Blood_Pressure_systolic_(mmHg)_(Mean)',\n",
    "    'Invasive_BP_Systolic_(Median)': 'Arterial_Blood_Pressure_systolic_(mmHg)_(Median)',\n",
    "    'Invasive_BP_Systolic_(Min)': 'Arterial_Blood_Pressure_systolic_(mmHg)_(Min)',\n",
    "    'Invasive_BP_Mean_(Max)': 'Arterial_Blood_Pressure_mean_(mmHg)_(Max)',\n",
    "    'Invasive_BP_Mean_(Mean)': 'Arterial_Blood_Pressure_mean_(mmHg)_(Mean)',\n",
    "    'Invasive_BP_Mean_(Median)': 'Arterial_Blood_Pressure_mean_(mmHg)_(Median)',\n",
    "    'Invasive_BP_Mean_(Min)': 'Arterial_Blood_Pressure_mean_(mmHg)_(Min)',\n",
    "    'serum_osmolality_(Max)': 'Serum_Osmolality_(Max)',\n",
    "    'serum_osmolality_(Mean)': 'Serum_Osmolality_(Mean)',\n",
    "    'serum_osmolality_(Median)': 'Serum_Osmolality_(Median)',\n",
    "    'serum_osmolality_(Min)': 'Serum_Osmolality_(Min)',\n",
    "    'troponin_-_I_(Max)': 'Troponin-T_(Max)',\n",
    "    'troponin_-_I_(Mean)': 'Troponin-T_(Mean)',\n",
    "    'troponin_-_I_(Median)': 'Troponin-T_(Median)',\n",
    "    'troponin_-_I_(Min)': 'Troponin-T_(Min)',\n",
    "    'uric_acid_(Max)': 'Uric_Acid_(Max)',\n",
    "    'uric_acid_(Mean)': 'Uric_Acid_(Mean)',\n",
    "    'uric_acid_(Median)': 'Uric_Acid_(Median)',\n",
    "    'uric_acid_(Min)': 'Uric_Acid_(Min)',\n",
    "    'ammonia_(Max)': 'Ammonia_(Max)',\n",
    "    'ammonia_(Mean)': 'Ammonia_(Mean)',\n",
    "    'ammonia_(Median)': 'Ammonia_(Median)',\n",
    "    'ammonia_(Min)': 'Ammonia_(Min)',\n",
    "    'CRP_(Max)': 'C_Reactive_Protein_(CRP)_(Max)',\n",
    "    'CRP_(Mean)': 'C_Reactive_Protein_(CRP)_(Mean)',\n",
    "    'CRP_(Median)': 'C_Reactive_Protein_(CRP)_(Min)',\n",
    "    'CRP_(Min)': 'C_Reactive_Protein_(CRP)_(Median)',\n",
    "    'fibrinogen_(Max)': 'Fibrinogen_(Max)',\n",
    "    'fibrinogen_(Mean)': 'Fibrinogen_(Mean)',\n",
    "    'fibrinogen_(Median)': 'Fibrinogen_(Median)',\n",
    "    'fibrinogen_(Min)': 'Fibrinogen_(Min)',\n",
    "    'PA_Systolic_(Max)': 'Pulmonary_Artery_Pressure_systolic_(mmHg)_(Max)',\n",
    "    'PA_Systolic_(Mean)': 'Pulmonary_Artery_Pressure_systolic_(mmHg)_(Mean)',\n",
    "    'PA_Systolic_(Median)': 'Pulmonary_Artery_Pressure_systolic_(mmHg)_(Median)',\n",
    "    'PA_Systolic_(Min)': 'Pulmonary_Artery_Pressure_systolic_(mmHg)_(Min)',\t\n",
    "    'PA_Diastolic_(Max)': 'Pulmonary_Artery_Pressure_diastolic_(mmHg)_(Max)',\n",
    "    'PA_Diastolic_(Mean)': 'Pulmonary_Artery_Pressure_diastolic_(mmHg)_(Mean)',\n",
    "    'PA_Diastolic_(Median)': 'Pulmonary_Artery_Pressure_diastolic_(mmHg)_(Median)',\n",
    "    'PA_Diastolic_(Min)': 'Pulmonary_Artery_Pressure_diastolic_(mmHg)_(Min)',\n",
    "    'PA_Mean_(Max)': 'Pulmonary_Artery_Pressure_mean_(mmHg)_(Max)',\n",
    "    'PA_Mean_(Mean)': 'Pulmonary_Artery_Pressure_mean_(mmHg)_(Mean)',\n",
    "    'PA_Mean_(Median)': 'Pulmonary_Artery_Pressure_mean_(mmHg)_(Median)',\n",
    "    'PA_Mean_(Min)': 'Pulmonary_Artery_Pressure_mean_(mmHg)_(Min)',\n",
    "    'bedside_glucose_(Max)': 'Glucose_finger_stick_(range_70-100)_(Max)',\n",
    "    'bedside_glucose_(Mean)': 'Glucose_finger_stick_(range_70-100)_(Mean)',\n",
    "    'bedside_glucose_(Median)': 'Glucose_finger_stick_(range_70-100)_(Median)',\n",
    "    'bedside_glucose_(Min)': 'Glucose_finger_stick_(range_70-100)_(Min)',\n",
    "    'reticulocyte_count_(Max)': 'Reticulocyte_Count_Automated_(Mean)',\n",
    "    'reticulocyte_count_(Mean)': 'Reticulocyte_Count_Automated_(Median)',\n",
    "    'reticulocyte_count_(Median)': 'Reticulocyte_Count_Automated_(Min)',\n",
    "    'reticulocyte_count_(Min)': 'Reticulocyte_Count_Automated_(Max)',\n",
    "    '-basos_(Max)': 'Differential-Basos_(Max)',\n",
    "    '-basos_(Mean)': 'Differential-Basos_(Mean)',\n",
    "    '-basos_(Median)': 'Differential-Basos_(Median)',\n",
    "    '-basos_(Min)': 'Differential-Basos_(Min)',\n",
    "    '-eos_(Max)': 'Differential-Eos_(Max)',\n",
    "    '-eos_(Mean)': 'Differential-Eos_(Mean)',\n",
    "    '-eos_(Median)': 'Differential-Eos_(Median)',\n",
    "    '-eos_(Min)': 'Differential-Eos_(Min)',\n",
    "    '-lymphs_(Max)': 'Differential-Lymphs_(Max)',\n",
    "    '-lymphs_(Mean)': 'Differential-Lymphs_(Mean)',\n",
    "    '-lymphs_(Median)': 'Differential-Lymphs_(Median)',\n",
    "    '-lymphs_(Min)': 'Differential-Lymphs_(Min)',\n",
    "    '-monos_(Max)': 'Differential-Monos_(Max)',\n",
    "    '-monos_(Mean)': 'Differential-Monos_(Mean)',\n",
    "    '-monos_(Median)': 'Differential-Monos_(Median)',\n",
    "    '-monos_(Min)': 'Differential-Monos_(Min)',\n",
    "    '-polys_(Max)': 'Differential-Neuts_(Max)',\n",
    "    '-polys_(Mean)': 'Differential-Neuts_(Mean)',\n",
    "    '-polys_(Median)': 'Differential-Neuts_(Median)',\n",
    "    '-polys_(Min)': 'Differential-Neuts_(Min)',\n",
    "    'haptoglobin_(Max)': 'Haptoglobin_(Max)',\n",
    "    'haptoglobin_(Mean)': 'Haptoglobin_(Mean)',\n",
    "    'haptoglobin_(Median)': 'Haptoglobin_(Median)',\n",
    "    'haptoglobin_(Min)': 'Haptoglobin_(Min)',\n",
    "    'direct_bilirubin_(Max)': 'Bilirubin_Direct_(Max)',\n",
    "    'direct_bilirubin_(Mean)': 'Bilirubin_Direct_(Mean)',\n",
    "    'direct_bilirubin_(Median)': 'Bilirubin_Direct_(Median)',\n",
    "    'direct_bilirubin_(Min)': 'Bilirubin_Direct_(Min)',\n",
    "    'free_T4_(Max)': 'Thyroxine_(T4)_Free_(Max)',\n",
    "    'free_T4_(Mean)': 'Thyroxine_(T4)_Free_(Mean)',\n",
    "    'free_T4_(Median)': 'Thyroxine_(T4)_Free_(Median)',\n",
    "    'free_T4_(Min)': 'Thyroxine_(T4)_Free_(Min)',\n",
    "    'ESR_(Max)': 'Sedimentation_Rate_(Max)',\n",
    "    'ESR_(Mean)': 'Sedimentation_Rate_(Mean)',\n",
    "    'ESR_(Median)': 'Sedimentation_Rate_(Median)',\n",
    "    'ESR_(Min)': 'Sedimentation_Rate_(Min)',\n",
    "    'CPK-MB_INDEX_(Max)': 'CK-MB_(Max)',\n",
    "    'CPK-MB_INDEX_(Mean)': 'CK-MB_(Mean)',\n",
    "    'CPK-MB_INDEX_(Median)': 'CK-MB_(Median)',\n",
    "    'CPK-MB_INDEX_(Min)': 'CK-MB_(Min)',\n",
    "    'amylase_(Max)': 'Amylase_(Max)',\n",
    "    'amylase_(Mean)': 'Amylase_(Mean)',\n",
    "    'amylase_(Median)': 'Amylase_(Median)',\n",
    "    'amylase_(Min)': 'Amylase_(Min)',\n",
    "    'PEEP_(Max)': 'PEEP_set_(cmH2O)_(Max)',\n",
    "    'PEEP_(Mean)': 'PEEP_set_(cmH2O)_(Mean)',\n",
    "    'PEEP_(Median)': 'PEEP_set_(cmH2O)_(Median)',\n",
    "    'PEEP_(Min)': 'PEEP_set_(cmH2O)_(Min)',\n",
    "    'CVP_(Max)': 'Central_Venous_Pressure_(mmHg)_(Max)',\n",
    "    'CVP_(Mean)': 'Central_Venous_Pressure_(mmHg)_(Mean)',\n",
    "    'CVP_(Median)': 'Central_Venous_Pressure_(mmHg)_(Median)',\n",
    "    'CVP_(Min)': 'Central_Venous_Pressure_(mmHg)_(Min)',\n",
    "    'total_bilirubin_(Max)': 'Total_Bilirubin_(Max)',\n",
    "    'total_bilirubin_(Mean)': 'Total_Bilirubin_(Mean)',\n",
    "    'total_bilirubin_(Median)': 'Total_Bilirubin_(Median)',\n",
    "    'total_bilirubin_(Min)': 'Total_Bilirubin_(Min)',\n",
    "    'Invasive_BP_Diastolic_(Max)': 'Arterial_Blood_Pressure_diastolic_(mmHg)_(Max)',\n",
    "    'Invasive_BP_Diastolic_(Mean)': 'Arterial_Blood_Pressure_diastolic_(mmHg)_(Mean)',\n",
    "    'Invasive_BP_Diastolic_(Median)': 'Arterial_Blood_Pressure_diastolic_(mmHg)_(Median)',\n",
    "    'Invasive_BP_Diastolic_(Min)': 'Arterial_Blood_Pressure_diastolic_(mmHg)_(Min)',\n",
    "    'unitdischargestatus': 'hospital_expire_flag',\n",
    "    'LOS': 'los'\n",
    "}\n",
    "\n",
    "# Replace the DataFrame and column names mapping\n",
    "eicu_temp.rename(columns=column_eicu_mapping, inplace=True)\n",
    "\n",
    "temperature_rename_mapping = {\n",
    "    'Temperature_Fahrenheit_(Â°F)_(Max)': 'Temperature_Fahrenheit_(F)_(Max)',\n",
    "    'Temperature_Fahrenheit_(Â°F)_(Mean)': 'Temperature_Fahrenheit_(F)_(Mean)',\n",
    "    'Temperature_Fahrenheit_(Â°F)_(Median)': 'Temperature_Fahrenheit_(F)_(Median)',\n",
    "    'Temperature_Fahrenheit_(Â°F)_(Min)': 'Temperature_Fahrenheit_(F)_(Min)'\n",
    "}\n",
    "\n",
    "# Rename the columns using the dictionary and reassign the DataFrame\n",
    "df_mimic_unique = df_mimic_unique.rename(columns=temperature_rename_mapping)\n",
    "\n",
    "# Remove \"-\" from the 'subject_id' column in eicu\n",
    "eicu_temp['subject_id'] = eicu_temp['subject_id'].str.replace('-', '')\n",
    "\n",
    "# Convert 'subject_id' in eicu to int64\n",
    "eicu_temp['subject_id'] = eicu_temp['subject_id'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2765649c-c2f9-4a72-a898-39dbb52bfaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimopoulos\\AppData\\Local\\Temp\\ipykernel_3544\\2443844813.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_mimic_unique['hospital_expire_flag'] = df_mimic_unique['hospital_expire_flag'].replace({'Survive': 0, 'Death': 1})\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Survive' with 0 and 'Death' with 1 in the 'hospital_expire_flag' column\n",
    "df_mimic_unique['hospital_expire_flag'] = df_mimic_unique['hospital_expire_flag'].replace({'Survive': 0, 'Death': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c2cf980-33e4-4e7e-bf65-2577d7a4dc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column names are identical between mimic_df and eicu_df.\n"
     ]
    }
   ],
   "source": [
    "# Check if mimic and eicu datasets have the same dtype and header names\n",
    "# Get the column names from each DataFrame\n",
    "mimic_columns = set(df_mimic_unique.columns)\n",
    "eicu_columns = set(eicu_temp.columns)\n",
    "\n",
    "# Get the column names and dtypes of mimic_df\n",
    "mimic_info = df_mimic_unique.dtypes\n",
    "\n",
    "# Get the column names and dtypes of eicu_df\n",
    "eicu_info = eicu_temp.dtypes\n",
    "\n",
    "# Find the columns that are in mimic_df but not in eicu_df\n",
    "mimic_not_in_eicu = mimic_columns - eicu_columns\n",
    "\n",
    "# Find the columns that are in eicu_df but not in mimic_df\n",
    "eicu_not_in_mimic = eicu_columns - mimic_columns\n",
    "\n",
    "# Display columns that are different\n",
    "if mimic_not_in_eicu:\n",
    "    print(\"Columns in mimic_df but not in eicu_df:\")\n",
    "    print(mimic_not_in_eicu)\n",
    "\n",
    "if eicu_not_in_mimic:\n",
    "    print(\"\\nColumns in eicu_df but not in mimic_df:\")\n",
    "    print(eicu_not_in_mimic)\n",
    "\n",
    "if not mimic_not_in_eicu and not eicu_not_in_mimic:\n",
    "    print(\"The column names are identical between mimic_df and eicu_df.\")\n",
    "\n",
    "# Check if the number of columns is the same\n",
    "if len(mimic_info) != len(eicu_info):\n",
    "    print(\"Number of columns is different between mimic_df and eicu_df.\")\n",
    "else:\n",
    "    # Iterate over the columns and compare the data type.\n",
    "    for column_name in mimic_info.index:\n",
    "        mimic_dtype = mimic_info[column_name]\n",
    "        eicu_dtype = eicu_info[column_name]\n",
    "        if mimic_dtype != eicu_dtype:\n",
    "            print(f\"Column '{column_name}' has different data types: mimic_df has '{mimic_dtype}' and eicu_df has '{eicu_dtype}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03df8590-216a-41a0-80af-84f48f53ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'age' in eicu_temp to numeric, handling any non-numeric values by coercing to NaN, then convert to Int64 (nullable integer type)\n",
    "eicu_temp['age'] = pd.to_numeric(eicu_temp['age'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Convert 'hospital_expire_flag' in eicu_temp to numeric, handling non-numeric values, and convert to Int64\n",
    "eicu_temp['hospital_expire_flag'] = pd.to_numeric(eicu_temp['hospital_expire_flag'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Ensure 'age' and 'hospital_expire_flag' in df_mimic_unique are also Int64 to handle any potential missing values consistently\n",
    "df_mimic_unique['age'] = df_mimic_unique['age'].astype('Int64')\n",
    "df_mimic_unique['hospital_expire_flag'] = df_mimic_unique['hospital_expire_flag'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98cf22d0-3ca5-411a-ba8b-1cc750768729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put 'hospital_expire_flag' and 'los' to the end of df_mimic_unique\n",
    "hospital_expire_flag_mimic = df_mimic_unique.pop('hospital_expire_flag')\n",
    "los_mimic = df_mimic_unique.pop('los')\n",
    "df_mimic_unique = pd.concat([df_mimic_unique, hospital_expire_flag_mimic, los_mimic], axis=1)\n",
    "\n",
    "# Move 'hospital_expire_flag' and 'los' to the end of eicu_temp\n",
    "hospital_expire_flag_eicu = eicu_temp.pop('hospital_expire_flag')\n",
    "los_eicu = eicu_temp.pop('los')\n",
    "eicu_temp = pd.concat([eicu_temp, hospital_expire_flag_eicu, los_eicu], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40788ea7-b854-4df8-a849-9bb3da5f1478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 23:15:01,021 - INFO - Starting the export of Mimic DataFrame to a CSV file.\n",
      "2025-03-22 23:15:01,023 - INFO - Directory CSV/exports/whole_set does not exist. Creating it.\n",
      "2025-03-22 23:15:13,629 - INFO - DataFrame successfully exported to CSV/exports/whole_set/o3_hour_overlap_window_mimic.csv.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logging.info(\"Starting the export of Mimic DataFrame to a CSV file.\")\n",
    "    \n",
    "    # Define the output path\n",
    "    output_path = (f\"CSV/exports/whole_set/{subfolder}_mimic.csv\")\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        logging.info(f\"Directory {output_dir} does not exist. Creating it.\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Export the merged DataFrame to a CSV file\n",
    "    df_mimic_unique.to_csv(output_path, index=False)\n",
    "    logging.info(f\"DataFrame successfully exported to {output_path}.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during the export of the DataFrame: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd2a784d-787d-4508-ae14-da2046b40e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 23:15:32,763 - INFO - Starting the export of eICU DataFrame to a CSV file.\n",
      "2025-03-22 23:15:50,251 - INFO - DataFrame successfully exported to CSV/exports/whole_set/o3_hour_overlap_window_eicu.csv.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logging.info(\"Starting the export of eICU DataFrame to a CSV file.\")\n",
    "    \n",
    "    # Define the output path\n",
    "    output_path = (f\"CSV/exports/whole_set/{subfolder}_eicu.csv\")\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        logging.info(f\"Directory {output_dir} does not exist. Creating it.\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Export the merged DataFrame to a CSV file\n",
    "    eicu_temp.to_csv(output_path, index=False)\n",
    "    logging.info(f\"DataFrame successfully exported to {output_path}.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during the export of the DataFrame: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0022e162-e02e-481e-8e42-674d7e55f03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86672, 317)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eicu_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14625aab-d3fc-401a-910f-3576ca5f4526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58144, 317)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mimic_unique.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
