{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ed3e02-f081-456b-856f-a09ce46d1f96",
   "metadata": {},
   "source": [
    "# CIR-03: Hierarchical Imputation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e6e9d4-78c6-4011-b711-c666178fa491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, Dense, Concatenate, GRU, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09007333-6aa8-45a4-8236-9a4b891fd7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial logger setup\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Global variable to hold the active file handler\n",
    "current_file_handler = None\n",
    "\n",
    "# Create the stream handler\n",
    "stream_handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "def switch_log_file(filename):\n",
    "    global current_file_handler\n",
    "\n",
    "    # If a file handler already exists, remove and close it\n",
    "    if current_file_handler:\n",
    "        logger.removeHandler(current_file_handler)\n",
    "        current_file_handler.close()\n",
    "\n",
    "    # Create a new file handler\n",
    "    current_file_handler = logging.FileHandler(filename)\n",
    "    current_file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(current_file_handler)\n",
    "\n",
    "    logger.info(f\"Switched logging to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5cfd680-ccfb-4b55-b717-3ab5b6cf4688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2025-05-07 23:47:03,630 - INFO - Switched logging to logs/CIR-2.log\n",
      "2025-05-07 23:47:03,645 - INFO - This is being logged to CIR-2.log\n"
=======
      "2025-05-07 18:42:30,382 - INFO - Switched logging to logs/CIR-2.log\n",
      "2025-05-07 18:42:30,382 - INFO - This is being logged to CIR-2.log\n"
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-2.log')\n",
    "logger.info(\"This is being logged to CIR-2.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b582432-2054-46b9-8452-70037a2f0b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2025-05-07 23:47:03,693 - INFO - +++++++++++++++++CIR-2+++++++++++++++++++++++++\n",
      "2025-05-07 23:47:03,693 - INFO - Start Loading Dataframes.\n",
      "2025-05-07 23:47:03,693 - INFO - Loading... -> o1_X_external.csv\n",
      "2025-05-07 23:47:13,272 - INFO - Loading... -> o1_X_test.csv\n",
      "2025-05-07 23:47:14,029 - INFO - Loading... -> o1_X_train.csv\n",
      "2025-05-07 23:47:19,196 - INFO - Loading... -> o1_X_validate.csv\n",
      "2025-05-07 23:47:20,253 - INFO - Loading... -> o1_y_external_los.csv\n",
      "2025-05-07 23:47:20,334 - INFO - Loading... -> o1_y_external_mortality.csv\n",
      "2025-05-07 23:47:20,396 - INFO - Loading... -> o1_y_test_los.csv\n",
      "2025-05-07 23:47:20,426 - INFO - Loading... -> o1_y_test_mortality.csv\n",
      "2025-05-07 23:47:20,440 - INFO - Loading... -> o1_y_train_los.csv\n",
      "2025-05-07 23:47:20,522 - INFO - Loading... -> o1_y_train_mortality.csv\n",
      "2025-05-07 23:47:20,554 - INFO - Loading... -> o1_y_validate_los.csv\n",
      "2025-05-07 23:47:20,559 - INFO - Loading... -> o1_y_validate_mortality.csv\n",
      "2025-05-07 23:47:20,569 - INFO - Loading... -> o2_X_external.csv\n",
      "2025-05-07 23:47:26,378 - INFO - Loading... -> o2_X_test.csv\n",
      "2025-05-07 23:47:26,967 - INFO - Loading... -> o2_X_train.csv\n",
      "2025-05-07 23:47:30,623 - INFO - Loading... -> o2_X_validate.csv\n",
      "2025-05-07 23:47:30,970 - INFO - Loading... -> o2_y_external_los.csv\n",
      "2025-05-07 23:47:31,006 - INFO - Loading... -> o2_y_external_mortality.csv\n",
      "2025-05-07 23:47:31,031 - INFO - Loading... -> o2_y_test_los.csv\n",
      "2025-05-07 23:47:31,034 - INFO - Loading... -> o2_y_test_mortality.csv\n",
      "2025-05-07 23:47:31,050 - INFO - Loading... -> o2_y_train_los.csv\n",
      "2025-05-07 23:47:31,081 - INFO - Loading... -> o2_y_train_mortality.csv\n",
      "2025-05-07 23:47:31,099 - INFO - Loading... -> o2_y_validate_los.csv\n",
      "2025-05-07 23:47:31,112 - INFO - Loading... -> o2_y_validate_mortality.csv\n",
      "2025-05-07 23:47:31,112 - INFO - Loading... -> o3_X_external.csv\n",
      "2025-05-07 23:47:35,896 - INFO - Loading... -> o3_X_test.csv\n",
      "2025-05-07 23:47:36,467 - INFO - Loading... -> o3_X_train.csv\n",
      "2025-05-07 23:47:39,679 - INFO - Loading... -> o3_X_validate.csv\n",
      "2025-05-07 23:47:40,186 - INFO - Loading... -> o3_y_external_los.csv\n",
      "2025-05-07 23:47:40,234 - INFO - Loading... -> o3_y_external_mortality.csv\n",
      "2025-05-07 23:47:40,266 - INFO - Loading... -> o3_y_test_los.csv\n",
      "2025-05-07 23:47:40,296 - INFO - Loading... -> o3_y_test_mortality.csv\n",
      "2025-05-07 23:47:40,310 - INFO - Loading... -> o3_y_train_los.csv\n",
      "2025-05-07 23:47:40,362 - INFO - Loading... -> o3_y_train_mortality.csv\n",
      "2025-05-07 23:47:40,378 - INFO - Loading... -> o3_y_validate_los.csv\n",
      "2025-05-07 23:47:40,401 - INFO - Loading... -> o3_y_validate_mortality.csv\n",
      "2025-05-07 23:47:40,427 - INFO - Loading... -> o4_X_external.csv\n",
      "2025-05-07 23:47:42,763 - INFO - Loading... -> o4_X_test.csv\n",
      "2025-05-07 23:47:42,953 - INFO - Loading... -> o4_X_train.csv\n",
      "2025-05-07 23:47:44,230 - INFO - Loading... -> o4_X_validate.csv\n",
      "2025-05-07 23:47:44,427 - INFO - Loading... -> o4_y_external_los.csv\n",
      "2025-05-07 23:47:44,446 - INFO - Loading... -> o4_y_external_mortality.csv\n",
      "2025-05-07 23:47:44,462 - INFO - Loading... -> o4_y_test_los.csv\n",
      "2025-05-07 23:47:44,469 - INFO - Loading... -> o4_y_test_mortality.csv\n",
      "2025-05-07 23:47:44,478 - INFO - Loading... -> o4_y_train_los.csv\n",
      "2025-05-07 23:47:44,503 - INFO - Loading... -> o4_y_train_mortality.csv\n",
      "2025-05-07 23:47:44,509 - INFO - Loading... -> o4_y_validate_los.csv\n",
      "2025-05-07 23:47:44,525 - INFO - Loading... -> o4_y_validate_mortality.csv\n",
      "2025-05-07 23:47:44,541 - INFO - o1_X_external loaded successfully with shape (234720, 345)\n",
      "2025-05-07 23:47:44,541 - INFO - o1_X_test loaded successfully with shape (15312, 345)\n",
      "2025-05-07 23:47:44,541 - INFO - o1_X_train loaded successfully with shape (122496, 345)\n",
      "2025-05-07 23:47:44,541 - INFO - o1_X_validate loaded successfully with shape (15312, 345)\n",
      "2025-05-07 23:47:44,541 - INFO - o1_y_external_los loaded successfully with shape (234720, 1)\n",
      "2025-05-07 23:47:44,556 - INFO - o1_y_external_mortality loaded successfully with shape (234720, 1)\n",
      "2025-05-07 23:47:44,565 - INFO - o1_y_test_los loaded successfully with shape (15312, 1)\n",
      "2025-05-07 23:47:44,567 - INFO - o1_y_test_mortality loaded successfully with shape (15312, 1)\n",
      "2025-05-07 23:47:44,572 - INFO - o1_y_train_los loaded successfully with shape (122496, 1)\n",
      "2025-05-07 23:47:44,576 - INFO - o1_y_train_mortality loaded successfully with shape (122496, 1)\n",
      "2025-05-07 23:47:44,580 - INFO - o1_y_validate_los loaded successfully with shape (15312, 1)\n",
      "2025-05-07 23:47:44,583 - INFO - o1_y_validate_mortality loaded successfully with shape (15312, 1)\n",
      "2025-05-07 23:47:44,589 - INFO - o2_X_external loaded successfully with shape (117360, 345)\n",
      "2025-05-07 23:47:44,591 - INFO - o2_X_test loaded successfully with shape (7656, 345)\n",
      "2025-05-07 23:47:44,593 - INFO - o2_X_train loaded successfully with shape (61248, 345)\n",
      "2025-05-07 23:47:44,598 - INFO - o2_X_validate loaded successfully with shape (7656, 345)\n",
      "2025-05-07 23:47:44,603 - INFO - o2_y_external_los loaded successfully with shape (117360, 1)\n",
      "2025-05-07 23:47:44,603 - INFO - o2_y_external_mortality loaded successfully with shape (117360, 1)\n",
      "2025-05-07 23:47:44,610 - INFO - o2_y_test_los loaded successfully with shape (7656, 1)\n",
      "2025-05-07 23:47:44,615 - INFO - o2_y_test_mortality loaded successfully with shape (7656, 1)\n",
      "2025-05-07 23:47:44,615 - INFO - o2_y_train_los loaded successfully with shape (61248, 1)\n",
      "2025-05-07 23:47:44,619 - INFO - o2_y_train_mortality loaded successfully with shape (61248, 1)\n",
      "2025-05-07 23:47:44,619 - INFO - o2_y_validate_los loaded successfully with shape (7656, 1)\n",
      "2025-05-07 23:47:44,625 - INFO - o2_y_validate_mortality loaded successfully with shape (7656, 1)\n",
      "2025-05-07 23:47:44,625 - INFO - o3_X_external loaded successfully with shape (78240, 345)\n",
      "2025-05-07 23:47:44,625 - INFO - o3_X_test loaded successfully with shape (5104, 345)\n",
      "2025-05-07 23:47:44,631 - INFO - o3_X_train loaded successfully with shape (40832, 345)\n",
      "2025-05-07 23:47:44,635 - INFO - o3_X_validate loaded successfully with shape (5104, 345)\n",
      "2025-05-07 23:47:44,635 - INFO - o3_y_external_los loaded successfully with shape (78240, 1)\n",
      "2025-05-07 23:47:44,635 - INFO - o3_y_external_mortality loaded successfully with shape (78240, 1)\n",
      "2025-05-07 23:47:44,635 - INFO - o3_y_test_los loaded successfully with shape (5104, 1)\n",
      "2025-05-07 23:47:44,635 - INFO - o3_y_test_mortality loaded successfully with shape (5104, 1)\n",
      "2025-05-07 23:47:44,635 - INFO - o3_y_train_los loaded successfully with shape (40832, 1)\n",
      "2025-05-07 23:47:44,635 - INFO - o3_y_train_mortality loaded successfully with shape (40832, 1)\n",
      "2025-05-07 23:47:44,646 - INFO - o3_y_validate_los loaded successfully with shape (5104, 1)\n",
      "2025-05-07 23:47:44,650 - INFO - o3_y_validate_mortality loaded successfully with shape (5104, 1)\n",
      "2025-05-07 23:47:44,650 - INFO - o4_X_external loaded successfully with shape (58680, 345)\n",
      "2025-05-07 23:47:44,650 - INFO - o4_X_test loaded successfully with shape (3828, 345)\n",
      "2025-05-07 23:47:44,650 - INFO - o4_X_train loaded successfully with shape (30624, 345)\n",
      "2025-05-07 23:47:44,656 - INFO - o4_X_validate loaded successfully with shape (3828, 345)\n",
      "2025-05-07 23:47:44,656 - INFO - o4_y_external_los loaded successfully with shape (58680, 1)\n",
      "2025-05-07 23:47:44,662 - INFO - o4_y_external_mortality loaded successfully with shape (58680, 1)\n",
      "2025-05-07 23:47:44,666 - INFO - o4_y_test_los loaded successfully with shape (3828, 1)\n",
      "2025-05-07 23:47:44,668 - INFO - o4_y_test_mortality loaded successfully with shape (3828, 1)\n",
      "2025-05-07 23:47:44,668 - INFO - o4_y_train_los loaded successfully with shape (30624, 1)\n",
      "2025-05-07 23:47:44,668 - INFO - o4_y_train_mortality loaded successfully with shape (30624, 1)\n",
      "2025-05-07 23:47:44,668 - INFO - o4_y_validate_los loaded successfully with shape (3828, 1)\n",
      "2025-05-07 23:47:44,677 - INFO - o4_y_validate_mortality loaded successfully with shape (3828, 1)\n",
      "2025-05-07 23:47:44,679 - INFO - Load Complete.\n",
      "2025-05-07 23:47:44,683 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n"
=======
      "2025-05-07 18:42:30,398 - INFO - +++++++++++++++++CIR-2+++++++++++++++++++++++++\n",
      "2025-05-07 18:42:30,398 - INFO - Start Loading Dataframes.\n",
      "2025-05-07 18:42:30,398 - INFO - Loading... -> o1_X_external.csv\n",
      "2025-05-07 18:42:37,218 - INFO - Loading... -> o1_X_test.csv\n",
      "2025-05-07 18:42:37,725 - INFO - Loading... -> o1_X_train.csv\n",
      "2025-05-07 18:42:41,784 - INFO - Loading... -> o1_X_validate.csv\n",
      "2025-05-07 18:42:42,301 - INFO - Loading... -> o1_y_external_los.csv\n",
      "2025-05-07 18:42:42,339 - INFO - Loading... -> o1_y_external_mortality.csv\n",
      "2025-05-07 18:42:42,370 - INFO - Loading... -> o1_y_test_los.csv\n",
      "2025-05-07 18:42:42,386 - INFO - Loading... -> o1_y_test_mortality.csv\n",
      "2025-05-07 18:42:42,386 - INFO - Loading... -> o1_y_train_los.csv\n",
      "2025-05-07 18:42:42,423 - INFO - Loading... -> o1_y_train_mortality.csv\n",
      "2025-05-07 18:42:42,439 - INFO - Loading... -> o1_y_validate_los.csv\n",
      "2025-05-07 18:42:42,455 - INFO - Loading... -> o1_y_validate_mortality.csv\n",
      "2025-05-07 18:42:42,455 - INFO - Loading... -> o2_X_external.csv\n",
      "2025-05-07 18:42:45,895 - INFO - Loading... -> o2_X_test.csv\n",
      "2025-05-07 18:42:46,164 - INFO - Loading... -> o2_X_train.csv\n",
      "2025-05-07 18:42:48,132 - INFO - Loading... -> o2_X_validate.csv\n",
      "2025-05-07 18:42:48,386 - INFO - Loading... -> o2_y_external_los.csv\n",
      "2025-05-07 18:42:48,417 - INFO - Loading... -> o2_y_external_mortality.csv\n",
      "2025-05-07 18:42:48,433 - INFO - Loading... -> o2_y_test_los.csv\n",
      "2025-05-07 18:42:48,439 - INFO - Loading... -> o2_y_test_mortality.csv\n",
      "2025-05-07 18:42:48,439 - INFO - Loading... -> o2_y_train_los.csv\n",
      "2025-05-07 18:42:48,455 - INFO - Loading... -> o2_y_train_mortality.csv\n",
      "2025-05-07 18:42:48,470 - INFO - Loading... -> o2_y_validate_los.csv\n",
      "2025-05-07 18:42:48,470 - INFO - Loading... -> o2_y_validate_mortality.csv\n",
      "2025-05-07 18:42:48,486 - INFO - Loading... -> o3_X_external.csv\n",
      "2025-05-07 18:42:50,833 - INFO - Loading... -> o3_X_test.csv\n",
      "2025-05-07 18:42:51,024 - INFO - Loading... -> o3_X_train.csv\n",
      "2025-05-07 18:42:52,388 - INFO - Loading... -> o3_X_validate.csv\n",
      "2025-05-07 18:42:52,589 - INFO - Loading... -> o3_y_external_los.csv\n",
      "2025-05-07 18:42:52,609 - INFO - Loading... -> o3_y_external_mortality.csv\n",
      "2025-05-07 18:42:52,617 - INFO - Loading... -> o3_y_test_los.csv\n",
      "2025-05-07 18:42:52,627 - INFO - Loading... -> o3_y_test_mortality.csv\n",
      "2025-05-07 18:42:52,627 - INFO - Loading... -> o3_y_train_los.csv\n",
      "2025-05-07 18:42:52,643 - INFO - Loading... -> o3_y_train_mortality.csv\n",
      "2025-05-07 18:42:52,643 - INFO - Loading... -> o3_y_validate_los.csv\n",
      "2025-05-07 18:42:52,658 - INFO - Loading... -> o3_y_validate_mortality.csv\n",
      "2025-05-07 18:42:52,658 - INFO - Loading... -> o4_X_external.csv\n",
      "2025-05-07 18:42:54,477 - INFO - Loading... -> o4_X_test.csv\n",
      "2025-05-07 18:42:54,628 - INFO - Loading... -> o4_X_train.csv\n",
      "2025-05-07 18:42:55,674 - INFO - Loading... -> o4_X_validate.csv\n",
      "2025-05-07 18:42:55,821 - INFO - Loading... -> o4_y_external_los.csv\n",
      "2025-05-07 18:42:55,843 - INFO - Loading... -> o4_y_external_mortality.csv\n",
      "2025-05-07 18:42:55,843 - INFO - Loading... -> o4_y_test_los.csv\n",
      "2025-05-07 18:42:55,859 - INFO - Loading... -> o4_y_test_mortality.csv\n",
      "2025-05-07 18:42:55,859 - INFO - Loading... -> o4_y_train_los.csv\n",
      "2025-05-07 18:42:55,874 - INFO - Loading... -> o4_y_train_mortality.csv\n",
      "2025-05-07 18:42:55,874 - INFO - Loading... -> o4_y_validate_los.csv\n",
      "2025-05-07 18:42:55,874 - INFO - Loading... -> o4_y_validate_mortality.csv\n",
      "2025-05-07 18:42:55,890 - INFO - o1_X_external loaded successfully with shape (234720, 345)\n",
      "2025-05-07 18:42:55,890 - INFO - o1_X_test loaded successfully with shape (15312, 345)\n",
      "2025-05-07 18:42:55,890 - INFO - o1_X_train loaded successfully with shape (122496, 345)\n",
      "2025-05-07 18:42:55,890 - INFO - o1_X_validate loaded successfully with shape (15312, 345)\n",
      "2025-05-07 18:42:55,890 - INFO - o1_y_external_los loaded successfully with shape (234720, 1)\n",
      "2025-05-07 18:42:55,890 - INFO - o1_y_external_mortality loaded successfully with shape (234720, 1)\n",
      "2025-05-07 18:42:55,890 - INFO - o1_y_test_los loaded successfully with shape (15312, 1)\n",
      "2025-05-07 18:42:55,890 - INFO - o1_y_test_mortality loaded successfully with shape (15312, 1)\n",
      "2025-05-07 18:42:55,890 - INFO - o1_y_train_los loaded successfully with shape (122496, 1)\n",
      "2025-05-07 18:42:55,890 - INFO - o1_y_train_mortality loaded successfully with shape (122496, 1)\n",
      "2025-05-07 18:42:55,890 - INFO - o1_y_validate_los loaded successfully with shape (15312, 1)\n",
      "2025-05-07 18:42:55,890 - INFO - o1_y_validate_mortality loaded successfully with shape (15312, 1)\n",
      "2025-05-07 18:42:55,890 - INFO - o2_X_external loaded successfully with shape (117360, 345)\n",
      "2025-05-07 18:42:55,890 - INFO - o2_X_test loaded successfully with shape (7656, 345)\n",
      "2025-05-07 18:42:55,890 - INFO - o2_X_train loaded successfully with shape (61248, 345)\n",
      "2025-05-07 18:42:55,906 - INFO - o2_X_validate loaded successfully with shape (7656, 345)\n",
      "2025-05-07 18:42:55,906 - INFO - o2_y_external_los loaded successfully with shape (117360, 1)\n",
      "2025-05-07 18:42:55,906 - INFO - o2_y_external_mortality loaded successfully with shape (117360, 1)\n",
      "2025-05-07 18:42:55,906 - INFO - o2_y_test_los loaded successfully with shape (7656, 1)\n",
      "2025-05-07 18:42:55,906 - INFO - o2_y_test_mortality loaded successfully with shape (7656, 1)\n",
      "2025-05-07 18:42:55,906 - INFO - o2_y_train_los loaded successfully with shape (61248, 1)\n",
      "2025-05-07 18:42:55,906 - INFO - o2_y_train_mortality loaded successfully with shape (61248, 1)\n",
      "2025-05-07 18:42:55,906 - INFO - o2_y_validate_los loaded successfully with shape (7656, 1)\n",
      "2025-05-07 18:42:55,906 - INFO - o2_y_validate_mortality loaded successfully with shape (7656, 1)\n",
      "2025-05-07 18:42:55,906 - INFO - o3_X_external loaded successfully with shape (78240, 345)\n",
      "2025-05-07 18:42:55,906 - INFO - o3_X_test loaded successfully with shape (5104, 345)\n",
      "2025-05-07 18:42:55,906 - INFO - o3_X_train loaded successfully with shape (40832, 345)\n",
      "2025-05-07 18:42:55,906 - INFO - o3_X_validate loaded successfully with shape (5104, 345)\n",
      "2025-05-07 18:42:55,906 - INFO - o3_y_external_los loaded successfully with shape (78240, 1)\n",
      "2025-05-07 18:42:55,906 - INFO - o3_y_external_mortality loaded successfully with shape (78240, 1)\n",
      "2025-05-07 18:42:55,906 - INFO - o3_y_test_los loaded successfully with shape (5104, 1)\n",
      "2025-05-07 18:42:55,906 - INFO - o3_y_test_mortality loaded successfully with shape (5104, 1)\n",
      "2025-05-07 18:42:55,921 - INFO - o3_y_train_los loaded successfully with shape (40832, 1)\n",
      "2025-05-07 18:42:55,921 - INFO - o3_y_train_mortality loaded successfully with shape (40832, 1)\n",
      "2025-05-07 18:42:55,921 - INFO - o3_y_validate_los loaded successfully with shape (5104, 1)\n",
      "2025-05-07 18:42:55,921 - INFO - o3_y_validate_mortality loaded successfully with shape (5104, 1)\n",
      "2025-05-07 18:42:55,921 - INFO - o4_X_external loaded successfully with shape (58680, 345)\n",
      "2025-05-07 18:42:55,921 - INFO - o4_X_test loaded successfully with shape (3828, 345)\n",
      "2025-05-07 18:42:55,921 - INFO - o4_X_train loaded successfully with shape (30624, 345)\n",
      "2025-05-07 18:42:55,921 - INFO - o4_X_validate loaded successfully with shape (3828, 345)\n",
      "2025-05-07 18:42:55,921 - INFO - o4_y_external_los loaded successfully with shape (58680, 1)\n",
      "2025-05-07 18:42:55,921 - INFO - o4_y_external_mortality loaded successfully with shape (58680, 1)\n",
      "2025-05-07 18:42:55,921 - INFO - o4_y_test_los loaded successfully with shape (3828, 1)\n",
      "2025-05-07 18:42:55,921 - INFO - o4_y_test_mortality loaded successfully with shape (3828, 1)\n",
      "2025-05-07 18:42:55,921 - INFO - o4_y_train_los loaded successfully with shape (30624, 1)\n",
      "2025-05-07 18:42:55,921 - INFO - o4_y_train_mortality loaded successfully with shape (30624, 1)\n",
      "2025-05-07 18:42:55,921 - INFO - o4_y_validate_los loaded successfully with shape (3828, 1)\n",
      "2025-05-07 18:42:55,921 - INFO - o4_y_validate_mortality loaded successfully with shape (3828, 1)\n",
      "2025-05-07 18:42:55,921 - INFO - Load Complete.\n",
      "2025-05-07 18:42:55,921 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n"
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "data_path = \"../04_ANN/CSV/exports/split_set/without_multiple_rows\"\n",
    "all_files = os.listdir(data_path)\n",
    "\n",
    "logging.info(\"+++++++++++++++++CIR-2+++++++++++++++++++++++++\")\n",
    "logging.info(\"Start Loading Dataframes.\")\n",
    "\n",
    "# Load CSVs into a dictionary of dataframes\n",
    "dataframes = {}\n",
    "for file in all_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        var_name = file.replace(\".csv\", \"\").replace(\"-\", \"_\")\n",
    "        logging.info(f\"Loading... -> {file}\")\n",
    "        dataframes[var_name] = pd.read_csv(os.path.join(data_path, file)).astype('float32')\n",
    "\n",
    "# Log loaded datasets\n",
    "for var_name, df in dataframes.items():\n",
    "    globals()[var_name] = df\n",
    "    logging.info(f\"{var_name} loaded successfully with shape {df.shape}\")\n",
    "logging.info(\"Load Complete.\")\n",
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d3072-f6a8-4100-adc2-5cc68a031614",
   "metadata": {},
   "source": [
    "# CIR-14: Implement Row Segmentation by Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d216f073-770a-482b-98f7-2f56fbca1e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2025-05-07 23:47:44,699 - INFO - Switched logging to logs/CIR-14.log\n",
      "2025-05-07 23:47:44,699 - INFO - This is being logged to CIR-14.log\n"
=======
      "2025-05-07 18:42:55,943 - INFO - Switched logging to logs/CIR-14.log\n",
      "2025-05-07 18:42:55,943 - INFO - This is being logged to CIR-14.log\n"
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-14.log')\n",
    "logger.info(\"This is being logged to CIR-14.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a3dfc4a-aaa2-4c81-837c-5daad8897160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Segments the dataframe rows into categories based on the percentage of missing values.\n",
    "\"\"\"\n",
    "def segment_rows_by_missingness(df: pd.DataFrame):\n",
    "    row_missing_perc = df.isnull().mean(axis=1)\n",
    "\n",
    "    segments = {\n",
    "        'very_low_missing 0% < 20%': df[(row_missing_perc <= 0.20)],\n",
    "        'low_missing 21% <= 40%': df[(row_missing_perc > 0.20) & (row_missing_perc <= 0.40)],\n",
    "        'moderate_missing 41% <= 60%': df[(row_missing_perc > 0.40) & (row_missing_perc <= 0.60)],\n",
    "        'high_missing > 60%': df[(row_missing_perc > 0.60)]\n",
    "    }\n",
    "\n",
    "    row_indices = {\n",
    "        name: segment.index.tolist() for name, segment in segments.items()\n",
    "    }\n",
    "\n",
    "    return segments, row_indices, row_missing_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2138fc7-4ae4-450e-90ef-7142d331d211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2025-05-07 23:47:44,753 - INFO - ---------------\n",
      "2025-05-07 23:47:44,757 - INFO - Segmenting rows by missingness for: o1_X_external\n",
      "2025-05-07 23:47:44,762 - INFO - o1_X_external - Total rows: 234720\n",
      "2025-05-07 23:47:45,858 - INFO - o1_X_external - very_low_missing 0% < 20%: 576 rows\n",
      "2025-05-07 23:47:45,858 - INFO - o1_X_external - low_missing 21% <= 40%: 49729 rows\n",
      "2025-05-07 23:47:45,871 - INFO - o1_X_external - moderate_missing 41% <= 60%: 153407 rows\n",
      "2025-05-07 23:47:45,871 - INFO - o1_X_external - high_missing > 60%: 31008 rows\n",
      "2025-05-07 23:47:45,871 - INFO - ---------------\n",
      "2025-05-07 23:47:45,871 - INFO - Segmenting rows by missingness for: o1_X_test\n",
      "2025-05-07 23:47:45,888 - INFO - o1_X_test - Total rows: 15312\n",
      "2025-05-07 23:47:46,058 - INFO - o1_X_test - very_low_missing 0% < 20%: 1248 rows\n",
      "2025-05-07 23:47:46,061 - INFO - o1_X_test - low_missing 21% <= 40%: 7872 rows\n",
      "2025-05-07 23:47:46,061 - INFO - o1_X_test - moderate_missing 41% <= 60%: 5424 rows\n",
      "2025-05-07 23:47:46,061 - INFO - o1_X_test - high_missing > 60%: 768 rows\n",
      "2025-05-07 23:47:46,066 - INFO - ---------------\n",
      "2025-05-07 23:47:46,066 - INFO - Segmenting rows by missingness for: o1_X_train\n",
      "2025-05-07 23:47:46,066 - INFO - o1_X_train - Total rows: 122496\n",
      "2025-05-07 23:47:46,522 - INFO - o1_X_train - very_low_missing 0% < 20%: 8544 rows\n",
      "2025-05-07 23:47:46,538 - INFO - o1_X_train - low_missing 21% <= 40%: 63696 rows\n",
      "2025-05-07 23:47:46,543 - INFO - o1_X_train - moderate_missing 41% <= 60%: 44832 rows\n",
      "2025-05-07 23:47:46,547 - INFO - o1_X_train - high_missing > 60%: 5424 rows\n",
      "2025-05-07 23:47:46,551 - INFO - ---------------\n",
      "2025-05-07 23:47:46,555 - INFO - Segmenting rows by missingness for: o1_X_validate\n",
      "2025-05-07 23:47:46,559 - INFO - o1_X_validate - Total rows: 15312\n",
      "2025-05-07 23:47:46,671 - INFO - o1_X_validate - very_low_missing 0% < 20%: 912 rows\n",
      "2025-05-07 23:47:46,671 - INFO - o1_X_validate - low_missing 21% <= 40%: 7920 rows\n",
      "2025-05-07 23:47:46,671 - INFO - o1_X_validate - moderate_missing 41% <= 60%: 5664 rows\n",
      "2025-05-07 23:47:46,678 - INFO - o1_X_validate - high_missing > 60%: 816 rows\n",
      "2025-05-07 23:47:46,681 - INFO - ---------------\n",
      "2025-05-07 23:47:46,686 - INFO - Segmenting rows by missingness for: o2_X_external\n",
      "2025-05-07 23:47:46,688 - INFO - o2_X_external - Total rows: 117360\n",
      "2025-05-07 23:47:47,141 - INFO - o2_X_external - very_low_missing 0% < 20%: 288 rows\n",
      "2025-05-07 23:47:47,141 - INFO - o2_X_external - low_missing 21% <= 40%: 24865 rows\n",
      "2025-05-07 23:47:47,141 - INFO - o2_X_external - moderate_missing 41% <= 60%: 76703 rows\n",
      "2025-05-07 23:47:47,141 - INFO - o2_X_external - high_missing > 60%: 15504 rows\n",
      "2025-05-07 23:47:47,157 - INFO - ---------------\n",
      "2025-05-07 23:47:47,161 - INFO - Segmenting rows by missingness for: o2_X_test\n",
      "2025-05-07 23:47:47,161 - INFO - o2_X_test - Total rows: 7656\n",
      "2025-05-07 23:47:47,263 - INFO - o2_X_test - very_low_missing 0% < 20%: 624 rows\n",
      "2025-05-07 23:47:47,263 - INFO - o2_X_test - low_missing 21% <= 40%: 3936 rows\n",
      "2025-05-07 23:47:47,268 - INFO - o2_X_test - moderate_missing 41% <= 60%: 2712 rows\n",
      "2025-05-07 23:47:47,268 - INFO - o2_X_test - high_missing > 60%: 384 rows\n",
      "2025-05-07 23:47:47,268 - INFO - ---------------\n",
      "2025-05-07 23:47:47,268 - INFO - Segmenting rows by missingness for: o2_X_train\n",
      "2025-05-07 23:47:47,268 - INFO - o2_X_train - Total rows: 61248\n",
      "2025-05-07 23:47:47,556 - INFO - o2_X_train - very_low_missing 0% < 20%: 4272 rows\n",
      "2025-05-07 23:47:47,556 - INFO - o2_X_train - low_missing 21% <= 40%: 31848 rows\n",
      "2025-05-07 23:47:47,563 - INFO - o2_X_train - moderate_missing 41% <= 60%: 22416 rows\n",
      "2025-05-07 23:47:47,569 - INFO - o2_X_train - high_missing > 60%: 2712 rows\n",
      "2025-05-07 23:47:47,570 - INFO - ---------------\n",
      "2025-05-07 23:47:47,578 - INFO - Segmenting rows by missingness for: o2_X_validate\n",
      "2025-05-07 23:47:47,585 - INFO - o2_X_validate - Total rows: 7656\n",
      "2025-05-07 23:47:47,633 - INFO - o2_X_validate - very_low_missing 0% < 20%: 456 rows\n",
      "2025-05-07 23:47:47,633 - INFO - o2_X_validate - low_missing 21% <= 40%: 3960 rows\n",
      "2025-05-07 23:47:47,649 - INFO - o2_X_validate - moderate_missing 41% <= 60%: 2832 rows\n",
      "2025-05-07 23:47:47,654 - INFO - o2_X_validate - high_missing > 60%: 408 rows\n",
      "2025-05-07 23:47:47,654 - INFO - ---------------\n",
      "2025-05-07 23:47:47,654 - INFO - Segmenting rows by missingness for: o3_X_external\n",
      "2025-05-07 23:47:47,665 - INFO - o3_X_external - Total rows: 78240\n",
      "2025-05-07 23:47:47,973 - INFO - o3_X_external - very_low_missing 0% < 20%: 192 rows\n",
      "2025-05-07 23:47:47,989 - INFO - o3_X_external - low_missing 21% <= 40%: 16579 rows\n",
      "2025-05-07 23:47:47,989 - INFO - o3_X_external - moderate_missing 41% <= 60%: 51133 rows\n",
      "2025-05-07 23:47:47,989 - INFO - o3_X_external - high_missing > 60%: 10336 rows\n",
      "2025-05-07 23:47:47,989 - INFO - ---------------\n",
      "2025-05-07 23:47:47,989 - INFO - Segmenting rows by missingness for: o3_X_test\n",
      "2025-05-07 23:47:48,005 - INFO - o3_X_test - Total rows: 5104\n",
      "2025-05-07 23:47:48,068 - INFO - o3_X_test - very_low_missing 0% < 20%: 416 rows\n",
      "2025-05-07 23:47:48,068 - INFO - o3_X_test - low_missing 21% <= 40%: 2624 rows\n",
      "2025-05-07 23:47:48,068 - INFO - o3_X_test - moderate_missing 41% <= 60%: 1808 rows\n",
      "2025-05-07 23:47:48,068 - INFO - o3_X_test - high_missing > 60%: 256 rows\n",
      "2025-05-07 23:47:48,086 - INFO - ---------------\n",
      "2025-05-07 23:47:48,090 - INFO - Segmenting rows by missingness for: o3_X_train\n",
      "2025-05-07 23:47:48,094 - INFO - o3_X_train - Total rows: 40832\n",
      "2025-05-07 23:47:48,306 - INFO - o3_X_train - very_low_missing 0% < 20%: 2848 rows\n",
      "2025-05-07 23:47:48,313 - INFO - o3_X_train - low_missing 21% <= 40%: 21232 rows\n",
      "2025-05-07 23:47:48,313 - INFO - o3_X_train - moderate_missing 41% <= 60%: 14944 rows\n",
      "2025-05-07 23:47:48,313 - INFO - o3_X_train - high_missing > 60%: 1808 rows\n",
      "2025-05-07 23:47:48,324 - INFO - ---------------\n",
      "2025-05-07 23:47:48,328 - INFO - Segmenting rows by missingness for: o3_X_validate\n",
      "2025-05-07 23:47:48,332 - INFO - o3_X_validate - Total rows: 5104\n",
      "2025-05-07 23:47:48,396 - INFO - o3_X_validate - very_low_missing 0% < 20%: 304 rows\n",
      "2025-05-07 23:47:48,401 - INFO - o3_X_validate - low_missing 21% <= 40%: 2640 rows\n",
      "2025-05-07 23:47:48,406 - INFO - o3_X_validate - moderate_missing 41% <= 60%: 1888 rows\n",
      "2025-05-07 23:47:48,412 - INFO - o3_X_validate - high_missing > 60%: 272 rows\n",
      "2025-05-07 23:47:48,417 - INFO - ---------------\n",
      "2025-05-07 23:47:48,420 - INFO - Segmenting rows by missingness for: o4_X_external\n",
      "2025-05-07 23:47:48,420 - INFO - o4_X_external - Total rows: 58680\n",
      "2025-05-07 23:47:48,639 - INFO - o4_X_external - very_low_missing 0% < 20%: 144 rows\n",
      "2025-05-07 23:47:48,656 - INFO - o4_X_external - low_missing 21% <= 40%: 12430 rows\n",
      "2025-05-07 23:47:48,656 - INFO - o4_X_external - moderate_missing 41% <= 60%: 38354 rows\n",
      "2025-05-07 23:47:48,656 - INFO - o4_X_external - high_missing > 60%: 7752 rows\n",
      "2025-05-07 23:47:48,656 - INFO - ---------------\n",
      "2025-05-07 23:47:48,665 - INFO - Segmenting rows by missingness for: o4_X_test\n",
      "2025-05-07 23:47:48,665 - INFO - o4_X_test - Total rows: 3828\n",
      "2025-05-07 23:47:48,703 - INFO - o4_X_test - very_low_missing 0% < 20%: 312 rows\n",
      "2025-05-07 23:47:48,703 - INFO - o4_X_test - low_missing 21% <= 40%: 1968 rows\n",
      "2025-05-07 23:47:48,703 - INFO - o4_X_test - moderate_missing 41% <= 60%: 1356 rows\n",
      "2025-05-07 23:47:48,711 - INFO - o4_X_test - high_missing > 60%: 192 rows\n",
      "2025-05-07 23:47:48,711 - INFO - ---------------\n",
      "2025-05-07 23:47:48,711 - INFO - Segmenting rows by missingness for: o4_X_train\n",
      "2025-05-07 23:47:48,719 - INFO - o4_X_train - Total rows: 30624\n",
      "2025-05-07 23:47:48,818 - INFO - o4_X_train - very_low_missing 0% < 20%: 2136 rows\n",
      "2025-05-07 23:47:48,820 - INFO - o4_X_train - low_missing 21% <= 40%: 15924 rows\n",
      "2025-05-07 23:47:48,822 - INFO - o4_X_train - moderate_missing 41% <= 60%: 11208 rows\n",
      "2025-05-07 23:47:48,824 - INFO - o4_X_train - high_missing > 60%: 1356 rows\n",
      "2025-05-07 23:47:48,828 - INFO - ---------------\n",
      "2025-05-07 23:47:48,828 - INFO - Segmenting rows by missingness for: o4_X_validate\n",
      "2025-05-07 23:47:48,830 - INFO - o4_X_validate - Total rows: 3828\n",
      "2025-05-07 23:47:48,846 - INFO - o4_X_validate - very_low_missing 0% < 20%: 228 rows\n",
      "2025-05-07 23:47:48,855 - INFO - o4_X_validate - low_missing 21% <= 40%: 1980 rows\n",
      "2025-05-07 23:47:48,857 - INFO - o4_X_validate - moderate_missing 41% <= 60%: 1416 rows\n",
      "2025-05-07 23:47:48,861 - INFO - o4_X_validate - high_missing > 60%: 204 rows\n",
      "2025-05-07 23:47:48,862 - INFO - ---------------\n"
=======
      "2025-05-07 18:42:55,976 - INFO - ---------------\n",
      "2025-05-07 18:42:55,976 - INFO - Segmenting rows by missingness for: o1_X_external\n",
      "2025-05-07 18:42:55,976 - INFO - o1_X_external - Total rows: 234720\n",
      "2025-05-07 18:42:56,422 - INFO - o1_X_external - very_low_missing 0% < 20%: 576 rows\n",
      "2025-05-07 18:42:56,422 - INFO - o1_X_external - low_missing 21% <= 40%: 49729 rows\n",
      "2025-05-07 18:42:56,422 - INFO - o1_X_external - moderate_missing 41% <= 60%: 153407 rows\n",
      "2025-05-07 18:42:56,422 - INFO - o1_X_external - high_missing > 60%: 31008 rows\n",
      "2025-05-07 18:42:56,422 - INFO - ---------------\n",
      "2025-05-07 18:42:56,422 - INFO - Segmenting rows by missingness for: o1_X_test\n",
      "2025-05-07 18:42:56,438 - INFO - o1_X_test - Total rows: 15312\n",
      "2025-05-07 18:42:56,497 - INFO - o1_X_test - very_low_missing 0% < 20%: 1248 rows\n",
      "2025-05-07 18:42:56,497 - INFO - o1_X_test - low_missing 21% <= 40%: 7872 rows\n",
      "2025-05-07 18:42:56,497 - INFO - o1_X_test - moderate_missing 41% <= 60%: 5424 rows\n",
      "2025-05-07 18:42:56,497 - INFO - o1_X_test - high_missing > 60%: 768 rows\n",
      "2025-05-07 18:42:56,497 - INFO - ---------------\n",
      "2025-05-07 18:42:56,497 - INFO - Segmenting rows by missingness for: o1_X_train\n",
      "2025-05-07 18:42:56,497 - INFO - o1_X_train - Total rows: 122496\n",
      "2025-05-07 18:42:56,728 - INFO - o1_X_train - very_low_missing 0% < 20%: 8544 rows\n",
      "2025-05-07 18:42:56,728 - INFO - o1_X_train - low_missing 21% <= 40%: 63696 rows\n",
      "2025-05-07 18:42:56,728 - INFO - o1_X_train - moderate_missing 41% <= 60%: 44832 rows\n",
      "2025-05-07 18:42:56,728 - INFO - o1_X_train - high_missing > 60%: 5424 rows\n",
      "2025-05-07 18:42:56,728 - INFO - ---------------\n",
      "2025-05-07 18:42:56,728 - INFO - Segmenting rows by missingness for: o1_X_validate\n",
      "2025-05-07 18:42:56,728 - INFO - o1_X_validate - Total rows: 15312\n",
      "2025-05-07 18:42:56,782 - INFO - o1_X_validate - very_low_missing 0% < 20%: 912 rows\n",
      "2025-05-07 18:42:56,782 - INFO - o1_X_validate - low_missing 21% <= 40%: 7920 rows\n",
      "2025-05-07 18:42:56,782 - INFO - o1_X_validate - moderate_missing 41% <= 60%: 5664 rows\n",
      "2025-05-07 18:42:56,782 - INFO - o1_X_validate - high_missing > 60%: 816 rows\n",
      "2025-05-07 18:42:56,782 - INFO - ---------------\n",
      "2025-05-07 18:42:56,782 - INFO - Segmenting rows by missingness for: o2_X_external\n",
      "2025-05-07 18:42:56,782 - INFO - o2_X_external - Total rows: 117360\n",
      "2025-05-07 18:42:56,998 - INFO - o2_X_external - very_low_missing 0% < 20%: 288 rows\n",
      "2025-05-07 18:42:56,998 - INFO - o2_X_external - low_missing 21% <= 40%: 24865 rows\n",
      "2025-05-07 18:42:56,998 - INFO - o2_X_external - moderate_missing 41% <= 60%: 76703 rows\n",
      "2025-05-07 18:42:57,014 - INFO - o2_X_external - high_missing > 60%: 15504 rows\n",
      "2025-05-07 18:42:57,014 - INFO - ---------------\n",
      "2025-05-07 18:42:57,014 - INFO - Segmenting rows by missingness for: o2_X_test\n",
      "2025-05-07 18:42:57,014 - INFO - o2_X_test - Total rows: 7656\n",
      "2025-05-07 18:42:57,045 - INFO - o2_X_test - very_low_missing 0% < 20%: 624 rows\n",
      "2025-05-07 18:42:57,045 - INFO - o2_X_test - low_missing 21% <= 40%: 3936 rows\n",
      "2025-05-07 18:42:57,045 - INFO - o2_X_test - moderate_missing 41% <= 60%: 2712 rows\n",
      "2025-05-07 18:42:57,045 - INFO - o2_X_test - high_missing > 60%: 384 rows\n",
      "2025-05-07 18:42:57,045 - INFO - ---------------\n",
      "2025-05-07 18:42:57,045 - INFO - Segmenting rows by missingness for: o2_X_train\n",
      "2025-05-07 18:42:57,045 - INFO - o2_X_train - Total rows: 61248\n",
      "2025-05-07 18:42:57,167 - INFO - o2_X_train - very_low_missing 0% < 20%: 4272 rows\n",
      "2025-05-07 18:42:57,167 - INFO - o2_X_train - low_missing 21% <= 40%: 31848 rows\n",
      "2025-05-07 18:42:57,167 - INFO - o2_X_train - moderate_missing 41% <= 60%: 22416 rows\n",
      "2025-05-07 18:42:57,167 - INFO - o2_X_train - high_missing > 60%: 2712 rows\n",
      "2025-05-07 18:42:57,167 - INFO - ---------------\n",
      "2025-05-07 18:42:57,167 - INFO - Segmenting rows by missingness for: o2_X_validate\n",
      "2025-05-07 18:42:57,167 - INFO - o2_X_validate - Total rows: 7656\n",
      "2025-05-07 18:42:57,199 - INFO - o2_X_validate - very_low_missing 0% < 20%: 456 rows\n",
      "2025-05-07 18:42:57,199 - INFO - o2_X_validate - low_missing 21% <= 40%: 3960 rows\n",
      "2025-05-07 18:42:57,199 - INFO - o2_X_validate - moderate_missing 41% <= 60%: 2832 rows\n",
      "2025-05-07 18:42:57,199 - INFO - o2_X_validate - high_missing > 60%: 408 rows\n",
      "2025-05-07 18:42:57,199 - INFO - ---------------\n",
      "2025-05-07 18:42:57,199 - INFO - Segmenting rows by missingness for: o3_X_external\n",
      "2025-05-07 18:42:57,199 - INFO - o3_X_external - Total rows: 78240\n",
      "2025-05-07 18:42:57,346 - INFO - o3_X_external - very_low_missing 0% < 20%: 192 rows\n",
      "2025-05-07 18:42:57,346 - INFO - o3_X_external - low_missing 21% <= 40%: 16579 rows\n",
      "2025-05-07 18:42:57,346 - INFO - o3_X_external - moderate_missing 41% <= 60%: 51133 rows\n",
      "2025-05-07 18:42:57,346 - INFO - o3_X_external - high_missing > 60%: 10336 rows\n",
      "2025-05-07 18:42:57,346 - INFO - ---------------\n",
      "2025-05-07 18:42:57,361 - INFO - Segmenting rows by missingness for: o3_X_test\n",
      "2025-05-07 18:42:57,361 - INFO - o3_X_test - Total rows: 5104\n",
      "2025-05-07 18:42:57,380 - INFO - o3_X_test - very_low_missing 0% < 20%: 416 rows\n",
      "2025-05-07 18:42:57,380 - INFO - o3_X_test - low_missing 21% <= 40%: 2624 rows\n",
      "2025-05-07 18:42:57,380 - INFO - o3_X_test - moderate_missing 41% <= 60%: 1808 rows\n",
      "2025-05-07 18:42:57,380 - INFO - o3_X_test - high_missing > 60%: 256 rows\n",
      "2025-05-07 18:42:57,380 - INFO - ---------------\n",
      "2025-05-07 18:42:57,388 - INFO - Segmenting rows by missingness for: o3_X_train\n",
      "2025-05-07 18:42:57,388 - INFO - o3_X_train - Total rows: 40832\n",
      "2025-05-07 18:42:57,470 - INFO - o3_X_train - very_low_missing 0% < 20%: 2848 rows\n",
      "2025-05-07 18:42:57,470 - INFO - o3_X_train - low_missing 21% <= 40%: 21232 rows\n",
      "2025-05-07 18:42:57,470 - INFO - o3_X_train - moderate_missing 41% <= 60%: 14944 rows\n",
      "2025-05-07 18:42:57,479 - INFO - o3_X_train - high_missing > 60%: 1808 rows\n",
      "2025-05-07 18:42:57,479 - INFO - ---------------\n",
      "2025-05-07 18:42:57,479 - INFO - Segmenting rows by missingness for: o3_X_validate\n",
      "2025-05-07 18:42:57,481 - INFO - o3_X_validate - Total rows: 5104\n",
      "2025-05-07 18:42:57,491 - INFO - o3_X_validate - very_low_missing 0% < 20%: 304 rows\n",
      "2025-05-07 18:42:57,491 - INFO - o3_X_validate - low_missing 21% <= 40%: 2640 rows\n",
      "2025-05-07 18:42:57,499 - INFO - o3_X_validate - moderate_missing 41% <= 60%: 1888 rows\n",
      "2025-05-07 18:42:57,499 - INFO - o3_X_validate - high_missing > 60%: 272 rows\n",
      "2025-05-07 18:42:57,501 - INFO - ---------------\n",
      "2025-05-07 18:42:57,501 - INFO - Segmenting rows by missingness for: o4_X_external\n",
      "2025-05-07 18:42:57,501 - INFO - o4_X_external - Total rows: 58680\n",
      "2025-05-07 18:42:57,620 - INFO - o4_X_external - very_low_missing 0% < 20%: 144 rows\n",
      "2025-05-07 18:42:57,620 - INFO - o4_X_external - low_missing 21% <= 40%: 12430 rows\n",
      "2025-05-07 18:42:57,622 - INFO - o4_X_external - moderate_missing 41% <= 60%: 38354 rows\n",
      "2025-05-07 18:42:57,622 - INFO - o4_X_external - high_missing > 60%: 7752 rows\n",
      "2025-05-07 18:42:57,622 - INFO - ---------------\n",
      "2025-05-07 18:42:57,622 - INFO - Segmenting rows by missingness for: o4_X_test\n",
      "2025-05-07 18:42:57,622 - INFO - o4_X_test - Total rows: 3828\n",
      "2025-05-07 18:42:57,642 - INFO - o4_X_test - very_low_missing 0% < 20%: 312 rows\n",
      "2025-05-07 18:42:57,642 - INFO - o4_X_test - low_missing 21% <= 40%: 1968 rows\n",
      "2025-05-07 18:42:57,642 - INFO - o4_X_test - moderate_missing 41% <= 60%: 1356 rows\n",
      "2025-05-07 18:42:57,642 - INFO - o4_X_test - high_missing > 60%: 192 rows\n",
      "2025-05-07 18:42:57,642 - INFO - ---------------\n",
      "2025-05-07 18:42:57,650 - INFO - Segmenting rows by missingness for: o4_X_train\n",
      "2025-05-07 18:42:57,650 - INFO - o4_X_train - Total rows: 30624\n",
      "2025-05-07 18:42:57,712 - INFO - o4_X_train - very_low_missing 0% < 20%: 2136 rows\n",
      "2025-05-07 18:42:57,712 - INFO - o4_X_train - low_missing 21% <= 40%: 15924 rows\n",
      "2025-05-07 18:42:57,712 - INFO - o4_X_train - moderate_missing 41% <= 60%: 11208 rows\n",
      "2025-05-07 18:42:57,712 - INFO - o4_X_train - high_missing > 60%: 1356 rows\n",
      "2025-05-07 18:42:57,712 - INFO - ---------------\n",
      "2025-05-07 18:42:57,720 - INFO - Segmenting rows by missingness for: o4_X_validate\n",
      "2025-05-07 18:42:57,720 - INFO - o4_X_validate - Total rows: 3828\n",
      "2025-05-07 18:42:57,732 - INFO - o4_X_validate - very_low_missing 0% < 20%: 228 rows\n",
      "2025-05-07 18:42:57,732 - INFO - o4_X_validate - low_missing 21% <= 40%: 1980 rows\n",
      "2025-05-07 18:42:57,732 - INFO - o4_X_validate - moderate_missing 41% <= 60%: 1416 rows\n",
      "2025-05-07 18:42:57,732 - INFO - o4_X_validate - high_missing > 60%: 204 rows\n",
      "2025-05-07 18:42:57,732 - INFO - ---------------\n"
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
     ]
    }
   ],
   "source": [
    "# Segment and log all X_ dataframes\n",
    "logging.info(\"---------------\")\n",
    "for var_name, df in dataframes.items():\n",
    "    if not var_name.startswith(\"o\") or \"_X_\" not in var_name:\n",
    "        continue  # Skip non-feature or target datasets\n",
    "\n",
    "    logging.info(f\"Segmenting rows by missingness for: {var_name}\")\n",
    "    logging.info(f\"{var_name} - Total rows: {df.shape[0]}\")\n",
    "\n",
    "    segments, row_indices, row_missing_perc = segment_rows_by_missingness(df)\n",
    "\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        logging.info(f\"{var_name} - {segment_name}: {len(segment_df)} rows\")\n",
    "    logging.info(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "528c545a-d22c-4b99-ad2e-0eb4c8be5beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2025-05-07 23:47:48,890 - INFO - Processing missing distribution plot for o1_X_external\n",
      "2025-05-07 23:47:53,835 - INFO - Saved professional missingness plot to figures/CIR-14\\o1_X_external_missing_distribution.png\n",
      "2025-05-07 23:47:53,835 - INFO - Processing missing distribution plot for o1_X_test\n",
      "2025-05-07 23:47:55,706 - INFO - Saved professional missingness plot to figures/CIR-14\\o1_X_test_missing_distribution.png\n",
      "2025-05-07 23:47:55,706 - INFO - Processing missing distribution plot for o1_X_train\n",
      "2025-05-07 23:47:58,362 - INFO - Saved professional missingness plot to figures/CIR-14\\o1_X_train_missing_distribution.png\n",
      "2025-05-07 23:47:58,362 - INFO - Processing missing distribution plot for o1_X_validate\n",
      "2025-05-07 23:47:59,954 - INFO - Saved professional missingness plot to figures/CIR-14\\o1_X_validate_missing_distribution.png\n",
      "2025-05-07 23:47:59,954 - INFO - Processing missing distribution plot for o2_X_external\n",
      "2025-05-07 23:48:02,143 - INFO - Saved professional missingness plot to figures/CIR-14\\o2_X_external_missing_distribution.png\n",
      "2025-05-07 23:48:02,143 - INFO - Processing missing distribution plot for o2_X_test\n",
      "2025-05-07 23:48:03,299 - INFO - Saved professional missingness plot to figures/CIR-14\\o2_X_test_missing_distribution.png\n",
      "2025-05-07 23:48:03,299 - INFO - Processing missing distribution plot for o2_X_train\n",
      "2025-05-07 23:48:04,977 - INFO - Saved professional missingness plot to figures/CIR-14\\o2_X_train_missing_distribution.png\n",
      "2025-05-07 23:48:04,977 - INFO - Processing missing distribution plot for o2_X_validate\n",
      "2025-05-07 23:48:06,390 - INFO - Saved professional missingness plot to figures/CIR-14\\o2_X_validate_missing_distribution.png\n",
      "2025-05-07 23:48:06,390 - INFO - Processing missing distribution plot for o3_X_external\n",
      "2025-05-07 23:48:08,888 - INFO - Saved professional missingness plot to figures/CIR-14\\o3_X_external_missing_distribution.png\n",
      "2025-05-07 23:48:08,888 - INFO - Processing missing distribution plot for o3_X_test\n",
      "2025-05-07 23:48:10,112 - INFO - Saved professional missingness plot to figures/CIR-14\\o3_X_test_missing_distribution.png\n",
      "2025-05-07 23:48:10,112 - INFO - Processing missing distribution plot for o3_X_train\n",
      "2025-05-07 23:48:11,651 - INFO - Saved professional missingness plot to figures/CIR-14\\o3_X_train_missing_distribution.png\n",
      "2025-05-07 23:48:11,651 - INFO - Processing missing distribution plot for o3_X_validate\n",
      "2025-05-07 23:48:12,820 - INFO - Saved professional missingness plot to figures/CIR-14\\o3_X_validate_missing_distribution.png\n",
      "2025-05-07 23:48:12,820 - INFO - Processing missing distribution plot for o4_X_external\n",
      "2025-05-07 23:48:15,286 - INFO - Saved professional missingness plot to figures/CIR-14\\o4_X_external_missing_distribution.png\n",
      "2025-05-07 23:48:15,286 - INFO - Processing missing distribution plot for o4_X_test\n",
      "2025-05-07 23:48:16,817 - INFO - Saved professional missingness plot to figures/CIR-14\\o4_X_test_missing_distribution.png\n",
      "2025-05-07 23:48:16,819 - INFO - Processing missing distribution plot for o4_X_train\n",
      "2025-05-07 23:48:18,223 - INFO - Saved professional missingness plot to figures/CIR-14\\o4_X_train_missing_distribution.png\n",
      "2025-05-07 23:48:18,223 - INFO - Processing missing distribution plot for o4_X_validate\n",
      "2025-05-07 23:48:19,425 - INFO - Saved professional missingness plot to figures/CIR-14\\o4_X_validate_missing_distribution.png\n"
=======
      "2025-05-07 18:42:57,752 - INFO - Processing missing distribution plot for o1_X_external\n",
      "2025-05-07 18:43:00,645 - INFO - Saved professional missingness plot to figures/CIR-14\\o1_X_external_missing_distribution.png\n",
      "2025-05-07 18:43:00,645 - INFO - Processing missing distribution plot for o1_X_test\n",
      "2025-05-07 18:43:01,549 - INFO - Saved professional missingness plot to figures/CIR-14\\o1_X_test_missing_distribution.png\n",
      "2025-05-07 18:43:01,549 - INFO - Processing missing distribution plot for o1_X_train\n",
      "2025-05-07 18:43:03,263 - INFO - Saved professional missingness plot to figures/CIR-14\\o1_X_train_missing_distribution.png\n",
      "2025-05-07 18:43:03,263 - INFO - Processing missing distribution plot for o1_X_validate\n",
      "2025-05-07 18:43:04,111 - INFO - Saved professional missingness plot to figures/CIR-14\\o1_X_validate_missing_distribution.png\n",
      "2025-05-07 18:43:04,111 - INFO - Processing missing distribution plot for o2_X_external\n",
      "2025-05-07 18:43:05,782 - INFO - Saved professional missingness plot to figures/CIR-14\\o2_X_external_missing_distribution.png\n",
      "2025-05-07 18:43:05,782 - INFO - Processing missing distribution plot for o2_X_test\n",
      "2025-05-07 18:43:06,577 - INFO - Saved professional missingness plot to figures/CIR-14\\o2_X_test_missing_distribution.png\n",
      "2025-05-07 18:43:06,577 - INFO - Processing missing distribution plot for o2_X_train\n",
      "2025-05-07 18:43:07,765 - INFO - Saved professional missingness plot to figures/CIR-14\\o2_X_train_missing_distribution.png\n",
      "2025-05-07 18:43:07,765 - INFO - Processing missing distribution plot for o2_X_validate\n",
      "2025-05-07 18:43:08,558 - INFO - Saved professional missingness plot to figures/CIR-14\\o2_X_validate_missing_distribution.png\n",
      "2025-05-07 18:43:08,558 - INFO - Processing missing distribution plot for o3_X_external\n",
      "2025-05-07 18:43:09,902 - INFO - Saved professional missingness plot to figures/CIR-14\\o3_X_external_missing_distribution.png\n",
      "2025-05-07 18:43:09,902 - INFO - Processing missing distribution plot for o3_X_test\n",
      "2025-05-07 18:43:10,717 - INFO - Saved professional missingness plot to figures/CIR-14\\o3_X_test_missing_distribution.png\n",
      "2025-05-07 18:43:10,717 - INFO - Processing missing distribution plot for o3_X_train\n",
      "2025-05-07 18:43:11,791 - INFO - Saved professional missingness plot to figures/CIR-14\\o3_X_train_missing_distribution.png\n",
      "2025-05-07 18:43:11,791 - INFO - Processing missing distribution plot for o3_X_validate\n",
      "2025-05-07 18:43:12,593 - INFO - Saved professional missingness plot to figures/CIR-14\\o3_X_validate_missing_distribution.png\n",
      "2025-05-07 18:43:12,593 - INFO - Processing missing distribution plot for o4_X_external\n",
      "2025-05-07 18:43:13,798 - INFO - Saved professional missingness plot to figures/CIR-14\\o4_X_external_missing_distribution.png\n",
      "2025-05-07 18:43:13,801 - INFO - Processing missing distribution plot for o4_X_test\n",
      "2025-05-07 18:43:14,566 - INFO - Saved professional missingness plot to figures/CIR-14\\o4_X_test_missing_distribution.png\n",
      "2025-05-07 18:43:14,566 - INFO - Processing missing distribution plot for o4_X_train\n",
      "2025-05-07 18:43:15,523 - INFO - Saved professional missingness plot to figures/CIR-14\\o4_X_train_missing_distribution.png\n",
      "2025-05-07 18:43:15,523 - INFO - Processing missing distribution plot for o4_X_validate\n",
      "2025-05-07 18:43:16,289 - INFO - Saved professional missingness plot to figures/CIR-14\\o4_X_validate_missing_distribution.png\n"
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
     ]
    }
   ],
   "source": [
    "# Ensure output directory exists\n",
    "base_plot_path = \"figures/CIR-14\"\n",
    "os.makedirs(base_plot_path, exist_ok=True)\n",
    "\n",
    "# Seaborn aesthetic settings\n",
    "sns.set(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "\n",
    "for var_name, df in dataframes.items():\n",
    "    if not var_name.startswith(\"o\") or \"_X_\" not in var_name:\n",
    "        continue  # Skip targets\n",
    "\n",
    "    logging.info(f\"Processing missing distribution plot for {var_name}\")\n",
    "\n",
    "    # Calculate row-wise missingness\n",
    "    row_missing_perc = df.isnull().mean(axis=1)\n",
    "    segments, _, _ = segment_rows_by_missingness(df)\n",
    "\n",
    "    # Prepare summary box content\n",
    "    summary_text = (\n",
    "        f\"Total rows: {len(df):,}\\n\"\n",
    "        f\"Very low (≤20%): {len(segments['very_low_missing 0% < 20%']):,}\\n\"\n",
    "        f\"Low (21–40%): {len(segments['low_missing 21% <= 40%']):,}\\n\"\n",
    "        f\"Moderate (41–60%): {len(segments['moderate_missing 41% <= 60%']):,}\\n\"\n",
    "        f\"High (>60%): {len(segments['high_missing > 60%']):,}\"\n",
    "    )\n",
    "\n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    sns.histplot(row_missing_perc, bins=20, kde=True, color='#2c7fb8', edgecolor='black', ax=ax)\n",
    "\n",
    "    # Customize titles and labels\n",
    "    ax.set_title(f\"Row-wise Missing Value Distribution\\n{var_name}\", fontsize=18, fontweight='bold')\n",
    "    ax.set_xlabel(\"Proportion of Missing Values\", fontsize=15)\n",
    "    ax.set_ylabel(\"Number of Rows\", fontsize=15)\n",
    "\n",
    "    # Add summary box to top-right\n",
    "    ax.text(\n",
    "        0.99, 0.95, summary_text,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=12,\n",
    "        verticalalignment='top',\n",
    "        horizontalalignment='right',\n",
    "        bbox=dict(boxstyle=\"round,pad=0.4\", facecolor='whitesmoke', alpha=0.85, edgecolor='gray')\n",
    "    )\n",
    "\n",
    "    # Add grid with transparency\n",
    "    ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.6)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "    # Optional: Add watermark tag\n",
    "    ax.text(0.01, 0.01, \"CIR-14\", transform=ax.transAxes,\n",
    "            fontsize=10, color='gray', alpha=0.7, ha='left', va='bottom')\n",
    "\n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plot_filename = os.path.join(base_plot_path, f\"{var_name}_missing_distribution.png\")\n",
    "    fig.savefig(plot_filename, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    logging.info(f\"Saved professional missingness plot to {plot_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1f44c-cae0-48dd-b990-cfdbfc2f545d",
   "metadata": {},
   "source": [
    "# CIR-15: Register Multiple Imputation Methods\n",
    "## mean, median, knn, iterative, xgboost, gan, LSTM, RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ab32877-0301-43d6-a194-648de0569bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2025-05-07 23:48:19,443 - INFO - Switched logging to logs/CIR-15.log\n",
      "2025-05-07 23:48:19,443 - INFO - This is being logged to CIR-15.log\n"
=======
      "2025-05-07 18:43:16,295 - INFO - Switched logging to logs/CIR-15.log\n",
      "2025-05-07 18:43:16,295 - INFO - This is being logged to CIR-15.log\n"
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-15.log')\n",
    "logger.info(\"This is being logged to CIR-15.log\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 10,
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
   "id": "1fc04268-628f-4419-a0b7-ccf4fd1f4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Impute missing values using XGBoost regression for each column independently.\n",
    "\"\"\"\n",
    "def xgboost_imputer(df, random_state=0):\n",
    "    df_imputed = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() == 0:\n",
    "            continue  # Skip fully observed columns\n",
    "\n",
    "        # Split rows with and without missing values in this column\n",
    "        not_null_idx = df[col].notnull()\n",
    "        null_idx = df[col].isnull()\n",
    "\n",
    "        X_train = df.loc[not_null_idx].drop(columns=[col])\n",
    "        y_train = df.loc[not_null_idx, col]\n",
    "        X_pred = df.loc[null_idx].drop(columns=[col])\n",
    "\n",
    "        # Skip if nothing to predict\n",
    "        if X_pred.empty:\n",
    "            continue\n",
    "\n",
    "        # Drop columns that are completely NaN\n",
    "        X_train = X_train.dropna(axis=1, how='all')\n",
    "        X_pred = X_pred[X_train.columns]  # keep same columns\n",
    "\n",
    "        # Fill remaining NaNs with column means (simple fallback)\n",
    "        X_train = X_train.fillna(X_train.mean())\n",
    "        X_pred = X_pred.fillna(X_train.mean())\n",
    "\n",
    "        # Train XGBoost model\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=random_state,\n",
    "            verbosity=0\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_pred)\n",
    "\n",
    "        # Impute predicted values\n",
    "        df_imputed.loc[null_idx, col] = y_pred\n",
    "\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 11,
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
   "id": "eba2c679-1f08-49ed-a8c6-6789ddcc6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Impute missing values using an LSTM autoencoder.\n",
    "Works best for dense rows (e.g., <40% missing).\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Impute missing values using an LSTM autoencoder.\n",
    "Works best for dense rows (e.g., <40% missing).\n",
    "\"\"\"\n",
    "\n",
    "# Cache the model outside the function (top-level variable)\n",
    "_lstm_model = None\n",
    "\n",
    "def lstm_imputer(df, random_state=0, epochs=1000, batch_size=64):\n",
    "    global _lstm_model\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    idx = df_copy.index\n",
    "    cols = df_copy.columns\n",
    "\n",
    "    # Fill missing values and normalize\n",
    "    df_filled = df_copy.fillna(df_copy.mean())\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = scaler.fit_transform(df_filled)\n",
    "    X = df_scaled.reshape((df_scaled.shape[0], 1, df_scaled.shape[1]))\n",
    "    input_dim = X.shape[2]\n",
    "\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Only build the model once\n",
    "    if _lstm_model is None:\n",
    "        input_layer = Input(shape=(1, input_dim))\n",
    "        encoded = LSTM(64, activation=\"relu\", return_sequences=False)(input_layer)\n",
    "        repeated = RepeatVector(1)(encoded)\n",
    "        decoded = LSTM(input_dim, activation=\"sigmoid\", return_sequences=True)(repeated)\n",
    "        _lstm_model = Model(inputs=input_layer, outputs=decoded)\n",
    "        _lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\")\n",
    "\n",
    "    # Train and log loss\n",
    "    for epoch in range(epochs):\n",
    "        history = _lstm_model.fit(X, X, epochs=1, batch_size=batch_size, verbose=0)\n",
    "        if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "            logging.info(f\"[LSTM Epoch {epoch}] Loss: {history.history['loss'][0]:.4f}\")\n",
    "\n",
    "    # Predict and inverse transform\n",
    "    X_imputed = _lstm_model.predict(X, verbose=0)\n",
    "    df_imputed_array = scaler.inverse_transform(X_imputed[:, 0, :])\n",
    "    df_imputed = pd.DataFrame(df_imputed_array, columns=cols, index=idx)\n",
    "\n",
    "    # Only fill missing values\n",
    "    for col in cols:\n",
    "        missing_mask = df[col].isnull()\n",
    "        df_copy.loc[missing_mask, col] = df_imputed.loc[missing_mask, col]\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 12,
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
   "id": "7bfa26cf-96ef-45ee-ae57-ac70868f3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GAN-style imputer for missing data based on GAIN.\n",
    "Arguments:\n",
    "    df (pd.DataFrame): Input dataframe with missing values.\n",
    "    random_state (int): Seed for reproducibility.\n",
    "    epochs (int): Number of training iterations.\n",
    "    batch_size (int): Batch size for training.\n",
    "Returns:\n",
    "    pd.DataFrame: Imputed dataframe.\n",
    "\"\"\"\n",
    "def gan_imputer(df, random_state=0, epochs=1000, batch_size=128):\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    cols = df_copy.columns\n",
    "    idx = df_copy.index\n",
    "\n",
    "    # ===== Step 1: Normalize & Create mask =====\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = scaler.fit_transform(df_copy.fillna(0))  # Fill NA with 0 for scaling\n",
    "    mask = ~df_copy.isnull().values  # 1 where observed, 0 where missing\n",
    "\n",
    "    data_dim = df_scaled.shape[1]\n",
    "    \n",
    "    # ===== Step 2: Generator =====\n",
    "    def build_generator():\n",
    "        inputs = Input(shape=(data_dim * 2,))\n",
    "        x = Dense(128, activation='relu')(inputs)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dense(data_dim, activation='sigmoid')(x)\n",
    "        return Model(inputs, x)\n",
    "\n",
    "    # ===== Step 3: Discriminator =====\n",
    "    def build_discriminator():\n",
    "        inputs = Input(shape=(data_dim * 2,))\n",
    "        x = Dense(128, activation='relu')(inputs)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dense(data_dim, activation='sigmoid')(x)\n",
    "        return Model(inputs, x)\n",
    "\n",
    "    G = build_generator()\n",
    "    D = build_discriminator()\n",
    "    G.compile(loss='binary_crossentropy', optimizer=Adam(0.001))\n",
    "    D.compile(loss='binary_crossentropy', optimizer=Adam(0.001))\n",
    "\n",
    "    # ===== Step 4: Training =====\n",
    "    for epoch in range(epochs):\n",
    "        # === Consistent batch size to avoid retracing ===\n",
    "        if df_scaled.shape[0] < batch_size:\n",
    "            repeat_factor = int(np.ceil(batch_size / df_scaled.shape[0]))\n",
    "            X_batch = np.tile(df_scaled, (repeat_factor, 1))[:batch_size]\n",
    "            M_batch = np.tile(mask, (repeat_factor, 1))[:batch_size]\n",
    "        else:\n",
    "            batch_idx = np.random.choice(df_scaled.shape[0], batch_size, replace=False)\n",
    "            X_batch = df_scaled[batch_idx]\n",
    "            M_batch = mask[batch_idx]\n",
    "\n",
    "        Z_batch = np.random.uniform(0, 0.01, size=X_batch.shape)\n",
    "        X_hat = M_batch * X_batch + (1 - M_batch) * Z_batch\n",
    "        G_input = np.concatenate([X_hat, M_batch], axis=1)\n",
    "\n",
    "        G_sample = G.predict(G_input, verbose=0)\n",
    "        X_fake = M_batch * X_batch + (1 - M_batch) * G_sample\n",
    "\n",
    "        D_input_real = np.concatenate([X_batch, M_batch], axis=1)\n",
    "        D_input_fake = np.concatenate([X_fake, M_batch], axis=1)\n",
    "\n",
    "        D_loss_real = D.train_on_batch(D_input_real, M_batch)\n",
    "        D_loss_fake = D.train_on_batch(D_input_fake, M_batch)\n",
    "\n",
    "        # === Train Generator ===\n",
    "        G_loss = G.train_on_batch(G_input, M_batch)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            logging.info(f\"[{epoch}] D_loss: {(D_loss_real + D_loss_fake) / 2:.4f} | G_loss: {G_loss:.4f}\")\n",
    "\n",
    "    # ===== Step 5: Imputation =====\n",
    "    Z_full = np.random.uniform(0, 0.01, size=df_scaled.shape)\n",
    "    X_hat_full = mask * df_scaled + (1 - mask) * Z_full\n",
    "    G_input_full = np.concatenate([X_hat_full, mask], axis=1)\n",
    "\n",
    "    G_imputed = G.predict(G_input_full, verbose=0)\n",
    "    X_final = mask * df_scaled + (1 - mask) * G_imputed\n",
    "\n",
    "    df_imputed_array = scaler.inverse_transform(X_final)\n",
    "    df_imputed = pd.DataFrame(df_imputed_array, columns=cols, index=idx)\n",
    "\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
=======
   "execution_count": 13,
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
   "id": "2e8aae06-9847-4169-9fd2-df67e8727d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Impute missing values using a GRU-based autoencoder.\n",
    "\"\"\"\n",
    "\n",
    "# Cache model to avoid retracing\n",
    "_rnn_model = None\n",
    "\n",
    "\"\"\"\n",
    "Parameter patience enables the early stopping if the training doesn't produce\n",
    "better MAE than the previews 10 or what number we put predictions.\n",
    "\n",
    "The mask_ratio controls the percentage of the known values are hidden in order\n",
    "to predict them.\n",
    "    - Low (0.05–0.1) Only a few known values are hidden per row.\n",
    "      Training is conservative but may not learn well.\n",
    "    - Medium (0.2–0.3) Balanced — enough challenge for learning while\n",
    "      preserving input context.\n",
    "    - High (0.4–0.5+) Very challenging — model must infer much of the input,\n",
    "      good for robustness, risky for small data.\n",
    "\"\"\"\n",
    "def rnn_imputer(df, random_state=0, epochs=1000, batch_size=64, mask_ratio=0.2, patience=10):\n",
    "    global _rnn_model\n",
    "\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    idx = df_copy.index\n",
    "    cols = df_copy.columns\n",
    "\n",
    "    # Step 1: Fill initial NaNs with column mean\n",
    "    df_filled = df_copy.fillna(df_copy.mean())\n",
    "\n",
    "    # Step 2: Scale\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = scaler.fit_transform(df_filled)\n",
    "\n",
    "    # Save original\n",
    "    X_original = df_scaled.copy()\n",
    "    input_dim = X_original.shape[1]\n",
    "\n",
    "    # Step 3: Build model once\n",
    "    if _rnn_model is None:\n",
    "        input_layer = Input(shape=(1, input_dim))\n",
    "        encoded = GRU(64, activation='relu', return_sequences=False)(input_layer)\n",
    "        encoded = Dropout(0.2)(encoded)  # 🧬 Dropout added\n",
    "        repeated = RepeatVector(1)(encoded)\n",
    "        decoded = GRU(input_dim, activation='sigmoid', return_sequences=True)(repeated)\n",
    "        _rnn_model = Model(inputs=input_layer, outputs=decoded)\n",
    "        _rnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "    # Step 4: Training loop with dynamic masking + early stopping\n",
    "    best_mae = float(\"inf\")\n",
    "    best_weights = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # === 🌀 Dynamic masking each epoch ===\n",
    "        mask = (np.random.rand(*X_original.shape) < mask_ratio)\n",
    "        X_masked = X_original.copy()\n",
    "        X_masked[mask] = 0\n",
    "\n",
    "        X_input = X_masked.reshape((X_masked.shape[0], 1, X_masked.shape[1]))\n",
    "        X_target = X_original.reshape((X_original.shape[0], 1, X_original.shape[1]))\n",
    "        loss_mask = tf.convert_to_tensor(mask.reshape((mask.shape[0], 1, mask.shape[1])), dtype=tf.float32)\n",
    "\n",
    "        # === Custom training step ===\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = _rnn_model(tf.convert_to_tensor(X_input, dtype=tf.float32))\n",
    "            loss = tf.reduce_sum(tf.square((preds - X_target) * loss_mask)) / tf.reduce_sum(loss_mask)\n",
    "\n",
    "        grads = tape.gradient(loss, _rnn_model.trainable_variables)\n",
    "        _rnn_model.optimizer.apply_gradients(zip(grads, _rnn_model.trainable_variables))\n",
    "\n",
    "        # === Benchmark MAE ===\n",
    "        mae = tf.reduce_sum(tf.abs((preds - X_target) * loss_mask)) / tf.reduce_sum(loss_mask)\n",
    "\n",
    "        # === Early stopping check ===\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_weights = _rnn_model.get_weights()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                logging.info(f\"[RNN Epoch {epoch}] Early stopping triggered. Best MAE: {best_mae.numpy():.4f}\")\n",
    "                break\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            logging.info(f\"[RNN Epoch {epoch}] Masked Loss MSE: {loss.numpy():.4f} | MAE: {mae.numpy():.4f}\")\n",
    "\n",
    "    # Restore best weights\n",
    "    if best_weights is not None:\n",
    "        _rnn_model.set_weights(best_weights)\n",
    "\n",
    "    # Step 5: Predict (impute)\n",
    "    X_pred = _rnn_model.predict(X_original.reshape((X_original.shape[0], 1, input_dim)), verbose=0)\n",
    "    X_imputed_array = scaler.inverse_transform(X_pred[:, 0, :])\n",
    "    df_imputed = pd.DataFrame(X_imputed_array, columns=cols, index=idx)\n",
    "\n",
    "    # Step 6: Replace only real missing values\n",
    "    for col in cols:\n",
    "        missing_mask = df[col].isnull()\n",
    "        df_copy.loc[missing_mask, col] = df_imputed.loc[missing_mask, col]\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
=======
   "execution_count": 14,
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
   "id": "9e58511d-c1c9-42e2-91b1-f802a7a5f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tee class to redirect output to both stdout and logging ---\n",
    "class Tee:\n",
    "    def __init__(self, *files, use_logging=False):\n",
    "        self.files = files\n",
    "        self.use_logging = use_logging\n",
    "\n",
    "    def write(self, obj):\n",
    "        for f in self.files:\n",
    "            f.write(obj)\n",
    "            f.flush()\n",
    "        if self.use_logging:\n",
    "            for line in obj.rstrip().splitlines():\n",
    "                logging.info(line)\n",
    "\n",
    "    def flush(self):\n",
    "        for f in self.files:\n",
    "            f.flush()\n",
    "\n",
    "# --- Iterative Imputer Function ---\n",
    "def impute_with_iterative(input_df, method, output_path, n_iter, log_verbose_file_path=None):\n",
    "    logging.info(f\"Starting Iterative Imputer with method={method} on input DataFrame of shape {input_df.shape}.\")\n",
    "\n",
    "    data_copy = input_df.copy()\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Estimator selection\n",
    "    if method == \"ExtraTrees\":\n",
    "        estimator = ExtraTreesRegressor(n_estimators=10, random_state=0, n_jobs=-1)\n",
    "    elif method == \"HistGradientBoosting\":\n",
    "        estimator = HistGradientBoostingRegressor(random_state=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}. Use 'ExtraTrees' or 'HistGradientBoosting'.\")\n",
    "\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=estimator,\n",
    "        max_iter=n_iter,\n",
    "        random_state=0,\n",
    "        verbose=2,\n",
    "        sample_posterior=False\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if log_verbose_file_path is not None:\n",
    "        os.makedirs(os.path.dirname(log_verbose_file_path), exist_ok=True)\n",
    "        original_stdout = sys.stdout\n",
    "        with open(log_verbose_file_path, \"w\") as log_file:\n",
    "            sys.stdout = Tee(sys.__stdout__, log_file, use_logging=True)\n",
    "            try:\n",
    "                imputed_array = imputer.fit_transform(data_copy)\n",
    "            finally:\n",
    "                sys.stdout = original_stdout\n",
    "    else:\n",
    "        sys.stdout = Tee(sys.__stdout__, use_logging=True)\n",
    "        try:\n",
    "            imputed_array = imputer.fit_transform(data_copy)\n",
    "        finally:\n",
    "            sys.stdout = sys.__stdout__\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    # Retain original index to avoid downstream assignment errors\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=data_copy.columns, index=data_copy.index)\n",
    "    imputed_df.to_csv(output_path, index=False)\n",
    "\n",
    "    logging.info(f\"Imputation completed in {runtime:.2f} seconds.\")\n",
    "    logging.info(f\"Number of NaNs after imputation: {np.isnan(imputed_df.values).sum()}\")\n",
    "    logging.info(f\"Imputed dataset saved at {output_path}\")\n",
    "\n",
    "    #describe_output_path = output_path.replace(\".csv\", \"_describe.csv\")\n",
    "    #imputed_df.describe().to_csv(describe_output_path)\n",
    "    #logging.info(f\"Basic statistics saved at {describe_output_path}\")\n",
    "\n",
    "    return imputed_df\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
=======
   "execution_count": 15,
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
   "id": "ef89dd2f-e455-433a-923a-5d1fe18f51b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Registry of Imputation Methods ---\n",
    "imputer_registry = {\n",
    "    \"mean\": SimpleImputer(strategy=\"mean\"),\n",
    "    \"median\": SimpleImputer(strategy=\"median\"),\n",
    "    \"knn\": KNNImputer(n_neighbors=5, weights=\"uniform\"),\n",
    "    \"iterative_function\": lambda df: impute_with_iterative(\n",
    "        input_df=df,\n",
    "        method=\"ExtraTrees\",\n",
    "        output_path=\"imputed_outputs/tmp.csv\",  # dummy or default path\n",
    "        n_iter=20\n",
    "    ),\n",
    "    \"iterative_simple\": IterativeImputer(estimator=ExtraTreesRegressor(n_estimators=10, random_state=42),\n",
    "                                   max_iter=10, random_state=42),\n",
    "    \"xgboost\": xgboost_imputer,\n",
    "    \"gan\": gan_imputer,\n",
    "    \"lstm\": lstm_imputer,\n",
    "    \"rnn\": rnn_imputer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a2c90-d2f9-4d3b-8732-6d27bdcbcbbc",
   "metadata": {},
   "source": [
    "# CIR-16: Build Core Hierarchical Controller Function"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 16,
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
   "id": "d5ed8030-eb6b-42a3-b014-54289367ad1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2025-05-07 23:58:51,189 - INFO - Switched logging to logs/CIR-16.log\n",
      "2025-05-07 23:58:51,189 - INFO - This is being logged to CIR-16.log\n"
=======
      "2025-05-07 18:43:16,406 - INFO - Switched logging to logs/CIR-16.log\n",
      "2025-05-07 18:43:16,406 - INFO - This is being logged to CIR-16.log\n"
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-16.log')\n",
    "logger.info(\"This is being logged to CIR-16.log\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 17,
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
   "id": "4d226725-059e-4e89-bafe-e269e768bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dynamic hierarchical imputer using cumulative row-wise missingness and assigned methods.\n",
    "\n",
    "Parameters:\n",
    "    df (pd.DataFrame): Dataset with missing values.\n",
    "    thresholds (list): List of group widths (must sum to ~1.0).\n",
    "    method_names (list): List of method names (must match thresholds).\n",
    "    method_registry (dict): Registered methods with keys as names and values as callables or sklearn objects.\n",
    "    random_state (int): Random seed for reproducibility.\n",
    "    return_method_log (bool): Return pd.Series logging method used per row.\n",
    "\n",
    "Returns:\n",
    "    imputed_df (pd.DataFrame)\n",
    "    method_log (pd.Series) — only if return_method_log=True\n",
    "\"\"\"\n",
    "\n",
    "def hierarchical_impute_dynamic(\n",
    "    df,\n",
    "    thresholds,\n",
    "    method_names,\n",
    "    method_registry,\n",
    "    random_state=0,\n",
    "    return_method_log=False,\n",
    "    dataset_name=None,\n",
    "    plot_id=None\n",
    "):\n",
    "    if len(thresholds) != len(method_names):\n",
    "        raise ValueError(\"The number of thresholds must match the number of methods.\")\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"missing_pct\"] = df_copy.isnull().mean(axis=1)\n",
    "    cols = df_copy.columns.drop(\"missing_pct\")\n",
    "\n",
    "    global_means = df_copy[cols].mean().fillna(0)\n",
    "    global_min = df_copy[cols].min()\n",
    "    global_max = df_copy[cols].max()\n",
    "\n",
    "    imputed_df = pd.DataFrame(index=df_copy.index, columns=cols)\n",
    "    method_log = pd.Series(index=df_copy.index, dtype=\"object\")\n",
    "\n",
    "    cum_thresholds = np.cumsum(thresholds)\n",
    "    if not np.isclose(cum_thresholds[-1], 1.0):\n",
    "        raise ValueError(\"Thresholds must sum to 1.0\")\n",
    "\n",
    "    previous_imputed = None\n",
    "\n",
    "    # For visualization\n",
    "    group_names = []\n",
    "    cumulative_rows = []\n",
    "    method_names_actual = []\n",
    "    cumulative_total = 0\n",
    "\n",
    "    for i, upper_bound in enumerate(cum_thresholds):\n",
    "        lower_bound = cum_thresholds[i - 1] if i > 0 else 0.0\n",
    "        idx = df_copy.index[\n",
    "            (df_copy[\"missing_pct\"] > lower_bound) & (df_copy[\"missing_pct\"] <= upper_bound)\n",
    "        ]\n",
    "        group_data = df_copy.loc[idx, cols].copy()\n",
    "\n",
    "        for col in group_data.columns:\n",
    "            if group_data[col].isnull().all():\n",
    "                group_data[col] = global_means[col]\n",
    "\n",
    "        if group_data.empty:\n",
    "            continue\n",
    "\n",
    "        method_name = method_names[i]\n",
    "        logging.info(f\"Group {i+1} ({lower_bound:.2f}, {upper_bound:.2f}] -> {method_name} | {len(group_data)} rows\")\n",
    "\n",
    "        imputer = get_imputer(method_name, method_registry)\n",
    "\n",
    "        if previous_imputed is None:\n",
    "            combined = group_data\n",
    "        else:\n",
    "            combined = pd.concat([previous_imputed, group_data])\n",
    "\n",
    "        try:\n",
    "            if hasattr(imputer, \"fit_transform\"):\n",
    "                combined_imputed = imputer.fit_transform(combined)\n",
    "                combined_imputed = pd.DataFrame(combined_imputed, columns=combined.columns, index=combined.index)\n",
    "            else:\n",
    "                combined_imputed = imputer(combined, random_state=random_state)\n",
    "        except TypeError:\n",
    "            combined_imputed = imputer(combined)\n",
    "\n",
    "        group_imputed = combined_imputed.loc[idx].clip(lower=global_min, upper=global_max, axis=1)\n",
    "\n",
    "        imputed_df.loc[idx] = group_imputed\n",
    "        method_log.loc[idx] = method_name\n",
    "\n",
    "        previous_imputed = pd.concat([previous_imputed, group_imputed]) if previous_imputed is not None else group_imputed.copy()\n",
    "\n",
    "        group_label = f\"{int(lower_bound * 100)}%–{int(upper_bound * 100)}%\"\n",
    "        group_names.append(group_label)\n",
    "        cumulative_total += len(group_data)\n",
    "        cumulative_rows.append(cumulative_total)\n",
    "        method_names_actual.append(method_name)\n",
    "\n",
    "    if imputed_df.isnull().values.any():\n",
    "        raise ValueError(\"NaNs remain after hierarchical imputation!\")\n",
    "\n",
    "    # === Plot ===\n",
    "    output_dir = \"figures/CIR-16\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    unique_methods = list(set(method_names_actual))\n",
    "    palette = sns.color_palette(\"tab10\", n_colors=len(unique_methods))\n",
    "    method_color_map = {method: palette[i] for i, method in enumerate(unique_methods)}\n",
    "    colors = [method_color_map[m] for m in method_names_actual]\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.barh(\n",
    "        y=range(1, len(cumulative_rows) + 1),\n",
    "        width=cumulative_rows,\n",
    "        color=colors,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "\n",
    "    plt.yticks(ticks=range(1, len(group_names) + 1), labels=group_names)\n",
    "    plt.title(f\"Cumulative Rows Used for Imputation by Group - {dataset_name}\", fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"Missingness Range\", fontsize=12)\n",
    "    plt.xlabel(\"Cumulative Rows Used\", fontsize=12)\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "    legend_handles = [Patch(color=color, label=method) for method, color in method_color_map.items()]\n",
    "    plt.legend(handles=legend_handles, title=\"Imputation Method\", loc=\"lower right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # === Save plot with dataset-specific name ===\n",
<<<<<<< HEAD
    "    if dataset_name and plot_id is not None:\n",
    "        filename = f\"{dataset_name}_seq_{plot_id:02d}_cumulative_imputation_rows.png\"\n",
    "    elif dataset_name:\n",
    "        filename = f\"{dataset_name}_cumulative_imputation_rows.png\"\n",
=======
    "    if dataset_name:\n",
    "        filename = f\"{dataset_name}_simple_lstm_cumulative_imputation_rows.png\"\n",
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
    "    else:\n",
    "        filename = \"cumulative_imputation_rows.png\"\n",
    "\n",
    "\n",
    "    plt.savefig(os.path.join(output_dir, filename), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return (imputed_df, method_log) if return_method_log else imputed_df"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": 18,
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
   "id": "54f71b5a-005b-49de-81a4-bc9159a32981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imputer(method_name, registry):\n",
    "    imputer = registry.get(method_name)\n",
    "    if imputer is None:\n",
    "        raise ValueError(f\"Method '{method_name}' not found or not implemented.\")\n",
    "    if hasattr(imputer, \"fit\") and hasattr(imputer, \"transform\"):\n",
    "        return copy.deepcopy(imputer)\n",
    "    return imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f810aa0b-a5ae-4ae0-b571-7216e1bcf54f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2025-05-07 23:58:52,886 - INFO - [#0] Imputing: o4_X_train with ['mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean']\n",
      "2025-05-07 23:58:53,259 - INFO - Group 1 (0.00, 0.10] -> mean | 12 rows\n",
      "2025-05-07 23:58:53,308 - INFO - Group 2 (0.10, 0.20] -> mean | 2124 rows\n",
      "2025-05-07 23:58:53,394 - INFO - Group 3 (0.20, 0.30] -> mean | 6048 rows\n",
      "2025-05-07 23:58:53,608 - INFO - Group 4 (0.30, 0.40] -> mean | 9876 rows\n",
      "2025-05-07 23:58:53,917 - INFO - Group 5 (0.40, 0.50] -> mean | 10116 rows\n",
      "2025-05-07 23:58:54,302 - INFO - Group 6 (0.50, 0.60] -> mean | 1092 rows\n",
      "2025-05-07 23:58:54,567 - INFO - Group 7 (0.60, 0.70] -> mean | 108 rows\n",
      "2025-05-07 23:58:54,836 - INFO - Group 8 (0.70, 0.80] -> mean | 1164 rows\n",
      "2025-05-07 23:58:55,138 - INFO - Group 9 (0.80, 0.90] -> mean | 84 rows\n",
      "2025-05-07 23:59:02,863 - INFO - Saved: CSV/exports/CIR-16/impute/o4_X_train_seq_00.csv\n",
      "2025-05-07 23:59:02,863 - INFO - [#0] Imputing: o4_X_validate with ['mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean']\n",
      "2025-05-07 23:59:02,933 - INFO - Group 1 (0.00, 0.10] -> mean | 12 rows\n",
      "2025-05-07 23:59:02,959 - INFO - Group 2 (0.10, 0.20] -> mean | 216 rows\n",
      "2025-05-07 23:59:02,999 - INFO - Group 3 (0.20, 0.30] -> mean | 600 rows\n",
      "2025-05-07 23:59:03,039 - INFO - Group 4 (0.30, 0.40] -> mean | 1380 rows\n",
      "2025-05-07 23:59:03,086 - INFO - Group 5 (0.40, 0.50] -> mean | 1284 rows\n",
      "2025-05-07 23:59:03,150 - INFO - Group 6 (0.50, 0.60] -> mean | 132 rows\n",
      "2025-05-07 23:59:03,213 - INFO - Group 7 (0.60, 0.70] -> mean | 48 rows\n",
      "2025-05-07 23:59:03,277 - INFO - Group 8 (0.70, 0.80] -> mean | 156 rows\n",
      "2025-05-07 23:59:04,885 - INFO - Saved: CSV/exports/CIR-16/impute/o4_X_validate_seq_00.csv\n",
      "2025-05-07 23:59:04,885 - INFO - [#0] Imputing: o4_X_test with ['mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean']\n",
      "2025-05-07 23:59:04,981 - INFO - Group 2 (0.10, 0.20] -> mean | 312 rows\n",
      "2025-05-07 23:59:05,013 - INFO - Group 3 (0.20, 0.30] -> mean | 744 rows\n",
      "2025-05-07 23:59:05,045 - INFO - Group 4 (0.30, 0.40] -> mean | 1224 rows\n",
      "2025-05-07 23:59:05,108 - INFO - Group 5 (0.40, 0.50] -> mean | 1188 rows\n",
      "2025-05-07 23:59:05,176 - INFO - Group 6 (0.50, 0.60] -> mean | 168 rows\n",
      "2025-05-07 23:59:05,283 - INFO - Group 8 (0.70, 0.80] -> mean | 180 rows\n",
      "2025-05-07 23:59:05,346 - INFO - Group 9 (0.80, 0.90] -> mean | 12 rows\n",
      "2025-05-07 23:59:07,505 - INFO - Saved: CSV/exports/CIR-16/impute/o4_X_test_seq_00.csv\n",
      "2025-05-07 23:59:07,505 - INFO - [#0] Imputing: o4_X_external with ['mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean', 'mean']\n",
      "2025-05-07 23:59:08,582 - INFO - Group 2 (0.10, 0.20] -> mean | 144 rows\n",
      "2025-05-07 23:59:08,642 - INFO - Group 3 (0.20, 0.30] -> mean | 1962 rows\n",
      "2025-05-07 23:59:08,738 - INFO - Group 4 (0.30, 0.40] -> mean | 10468 rows\n",
      "2025-05-07 23:59:09,027 - INFO - Group 5 (0.40, 0.50] -> mean | 23776 rows\n",
      "2025-05-07 23:59:09,603 - INFO - Group 6 (0.50, 0.60] -> mean | 14578 rows\n",
      "2025-05-07 23:59:10,197 - INFO - Group 7 (0.60, 0.70] -> mean | 3000 rows\n",
      "2025-05-07 23:59:10,667 - INFO - Group 8 (0.70, 0.80] -> mean | 3216 rows\n",
      "2025-05-07 23:59:11,160 - INFO - Group 9 (0.80, 0.90] -> mean | 1536 rows\n",
      "2025-05-07 23:59:26,748 - INFO - Saved: CSV/exports/CIR-16/impute/o4_X_external_seq_00.csv\n",
      "2025-05-07 23:59:26,748 - INFO - [#1] Imputing: o4_X_train with ['median', 'median', 'median', 'median', 'median', 'median', 'median', 'median', 'median', 'median']\n",
      "2025-05-07 23:59:27,150 - INFO - Group 1 (0.00, 0.10] -> median | 12 rows\n",
      "2025-05-07 23:59:27,209 - INFO - Group 2 (0.10, 0.20] -> median | 2124 rows\n",
      "2025-05-07 23:59:27,320 - INFO - Group 3 (0.20, 0.30] -> median | 6048 rows\n",
      "2025-05-07 23:59:27,639 - INFO - Group 4 (0.30, 0.40] -> median | 9876 rows\n",
      "2025-05-07 23:59:28,291 - INFO - Group 5 (0.40, 0.50] -> median | 10116 rows\n",
      "2025-05-07 23:59:29,530 - INFO - Group 6 (0.50, 0.60] -> median | 1092 rows\n",
      "2025-05-07 23:59:30,609 - INFO - Group 7 (0.60, 0.70] -> median | 108 rows\n",
      "2025-05-07 23:59:31,686 - INFO - Group 8 (0.70, 0.80] -> median | 1164 rows\n",
      "2025-05-07 23:59:32,836 - INFO - Group 9 (0.80, 0.90] -> median | 84 rows\n",
      "2025-05-07 23:59:41,848 - INFO - Saved: CSV/exports/CIR-16/impute/o4_X_train_seq_01.csv\n",
      "2025-05-07 23:59:41,848 - INFO - [#1] Imputing: o4_X_validate with ['median', 'median', 'median', 'median', 'median', 'median', 'median', 'median', 'median', 'median']\n",
      "2025-05-07 23:59:41,904 - INFO - Group 1 (0.00, 0.10] -> median | 12 rows\n",
      "2025-05-07 23:59:41,920 - INFO - Group 2 (0.10, 0.20] -> median | 216 rows\n",
      "2025-05-07 23:59:41,952 - INFO - Group 3 (0.20, 0.30] -> median | 600 rows\n",
      "2025-05-07 23:59:42,000 - INFO - Group 4 (0.30, 0.40] -> median | 1380 rows\n",
      "2025-05-07 23:59:42,080 - INFO - Group 5 (0.40, 0.50] -> median | 1284 rows\n",
      "2025-05-07 23:59:42,176 - INFO - Group 6 (0.50, 0.60] -> median | 132 rows\n",
      "2025-05-07 23:59:42,272 - INFO - Group 7 (0.60, 0.70] -> median | 48 rows\n",
      "2025-05-07 23:59:42,366 - INFO - Group 8 (0.70, 0.80] -> median | 156 rows\n",
      "2025-05-07 23:59:44,668 - INFO - Saved: CSV/exports/CIR-16/impute/o4_X_validate_seq_01.csv\n",
      "2025-05-07 23:59:44,683 - INFO - [#1] Imputing: o4_X_test with ['median', 'median', 'median', 'median', 'median', 'median', 'median', 'median', 'median', 'median']\n",
      "2025-05-07 23:59:44,842 - INFO - Group 2 (0.10, 0.20] -> median | 312 rows\n",
      "2025-05-07 23:59:44,953 - INFO - Group 3 (0.20, 0.30] -> median | 744 rows\n",
      "2025-05-07 23:59:45,076 - INFO - Group 4 (0.30, 0.40] -> median | 1224 rows\n",
      "2025-05-07 23:59:45,222 - INFO - Group 5 (0.40, 0.50] -> median | 1188 rows\n",
      "2025-05-07 23:59:45,396 - INFO - Group 6 (0.50, 0.60] -> median | 168 rows\n",
      "2025-05-07 23:59:45,664 - INFO - Group 8 (0.70, 0.80] -> median | 180 rows\n",
      "2025-05-07 23:59:45,904 - INFO - Group 9 (0.80, 0.90] -> median | 12 rows\n",
      "2025-05-07 23:59:47,849 - INFO - Saved: CSV/exports/CIR-16/impute/o4_X_test_seq_01.csv\n",
      "2025-05-07 23:59:47,858 - INFO - [#1] Imputing: o4_X_external with ['median', 'median', 'median', 'median', 'median', 'median', 'median', 'median', 'median', 'median']\n",
      "2025-05-07 23:59:48,569 - INFO - Group 2 (0.10, 0.20] -> median | 144 rows\n",
      "2025-05-07 23:59:48,659 - INFO - Group 3 (0.20, 0.30] -> median | 1962 rows\n",
      "2025-05-07 23:59:48,809 - INFO - Group 4 (0.30, 0.40] -> median | 10468 rows\n",
      "2025-05-07 23:59:49,414 - INFO - Group 5 (0.40, 0.50] -> median | 23776 rows\n",
      "2025-05-07 23:59:50,461 - INFO - Group 6 (0.50, 0.60] -> median | 14578 rows\n",
      "2025-05-07 23:59:52,164 - INFO - Group 7 (0.60, 0.70] -> median | 3000 rows\n",
      "2025-05-07 23:59:54,323 - INFO - Group 8 (0.70, 0.80] -> median | 3216 rows\n",
      "2025-05-07 23:59:57,219 - INFO - Group 9 (0.80, 0.90] -> median | 1536 rows\n",
      "2025-05-08 00:00:15,202 - INFO - Saved: CSV/exports/CIR-16/impute/o4_X_external_seq_01.csv\n",
      "2025-05-08 00:00:15,204 - INFO - [#2] Imputing: o4_X_train with ['iterative_simple', 'iterative_simple', 'iterative_simple', 'iterative_simple', 'iterative_simple', 'iterative_simple', 'iterative_simple', 'iterative_simple', 'iterative_simple', 'iterative_simple']\n",
      "2025-05-08 00:00:15,480 - INFO - Group 1 (0.00, 0.10] -> iterative_simple | 12 rows\n",
      "2025-05-08 00:00:19,922 - INFO - Group 2 (0.10, 0.20] -> iterative_simple | 2124 rows\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     imputed_df, method_log \u001b[38;5;241m=\u001b[39m hierarchical_impute_dynamic(\n\u001b[0;32m     60\u001b[0m         df\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m     61\u001b[0m         thresholds\u001b[38;5;241m=\u001b[39mthresholds,\n\u001b[0;32m     62\u001b[0m         method_names\u001b[38;5;241m=\u001b[39mmethod_names,\n\u001b[0;32m     63\u001b[0m         method_registry\u001b[38;5;241m=\u001b[39mimputer_registry,\n\u001b[0;32m     64\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     65\u001b[0m         return_method_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     66\u001b[0m         dataset_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m     67\u001b[0m         plot_id\u001b[38;5;241m=\u001b[39midx\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     70\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV/exports/CIR-16/impute/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_seq_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m     imputed_df\u001b[38;5;241m.\u001b[39mto_csv(output_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[28], line 79\u001b[0m, in \u001b[0;36mhierarchical_impute_dynamic\u001b[1;34m(df, thresholds, method_names, method_registry, random_state, return_method_log, dataset_name, plot_id)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(imputer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 79\u001b[0m         combined_imputed \u001b[38;5;241m=\u001b[39m imputer\u001b[38;5;241m.\u001b[39mfit_transform(combined)\n\u001b[0;32m     80\u001b[0m         combined_imputed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(combined_imputed, columns\u001b[38;5;241m=\u001b[39mcombined\u001b[38;5;241m.\u001b[39mcolumns, index\u001b[38;5;241m=\u001b[39mcombined\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:859\u001b[0m, in \u001b[0;36mIterativeImputer.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat_idx \u001b[38;5;129;01min\u001b[39;00m ordered_idx:\n\u001b[0;32m    856\u001b[0m     neighbor_feat_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_neighbor_feat_idx(\n\u001b[0;32m    857\u001b[0m         n_features, feat_idx, abs_corr_mat\n\u001b[0;32m    858\u001b[0m     )\n\u001b[1;32m--> 859\u001b[0m     Xt, estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impute_one_feature(\n\u001b[0;32m    860\u001b[0m         Xt,\n\u001b[0;32m    861\u001b[0m         mask_missing_values,\n\u001b[0;32m    862\u001b[0m         feat_idx,\n\u001b[0;32m    863\u001b[0m         neighbor_feat_idx,\n\u001b[0;32m    864\u001b[0m         estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    865\u001b[0m         fit_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    866\u001b[0m         params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[0;32m    867\u001b[0m     )\n\u001b[0;32m    868\u001b[0m     estimator_triplet \u001b[38;5;241m=\u001b[39m _ImputerTriplet(\n\u001b[0;32m    869\u001b[0m         feat_idx, neighbor_feat_idx, estimator\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimputation_sequence_\u001b[38;5;241m.\u001b[39mappend(estimator_triplet)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:427\u001b[0m, in \u001b[0;36mIterativeImputer._impute_one_feature\u001b[1;34m(self, X_filled, mask_missing_values, feat_idx, neighbor_feat_idx, estimator, fit_mode, params)\u001b[0m\n\u001b[0;32m    417\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m _safe_indexing(\n\u001b[0;32m    418\u001b[0m         _safe_indexing(X_filled, neighbor_feat_idx, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;241m~\u001b[39mmissing_row_mask,\n\u001b[0;32m    420\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    422\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m _safe_indexing(\n\u001b[0;32m    423\u001b[0m         _safe_indexing(X_filled, feat_idx, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;241m~\u001b[39mmissing_row_mask,\n\u001b[0;32m    425\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    426\u001b[0m     )\n\u001b[1;32m--> 427\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# if no missing values, don't predict\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(missing_row_mask) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    479\u001b[0m ]\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    488\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    489\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    490\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    491\u001b[0m )(\n\u001b[0;32m    492\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    493\u001b[0m         t,\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    495\u001b[0m         X,\n\u001b[0;32m    496\u001b[0m         y,\n\u001b[0;32m    497\u001b[0m         sample_weight,\n\u001b[0;32m    498\u001b[0m         i,\n\u001b[0;32m    499\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    500\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    501\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    502\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    503\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    504\u001b[0m     )\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    506\u001b[0m )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:197\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    190\u001b[0m         X,\n\u001b[0;32m    191\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    195\u001b[0m     )\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    198\u001b[0m         X,\n\u001b[0;32m    199\u001b[0m         y,\n\u001b[0;32m    200\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    201\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    202\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    203\u001b[0m     )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "methods_all = [\"mean\", \"median\", \"iterative_simple\", \"iterative_function\", \"xgboost\", \"gan\", \"lstm\", \"rnn\"]\n",
    "\n",
    "# Define all datasets\n",
    "datasets = [\n",
    "    \"o4_X_train\", \"o4_X_validate\", \"o4_X_test\", \"o4_X_external\"\n",
    "]\n",
    "\n",
    "# Define the fixed sequence builder\n",
    "def build_sequences():\n",
    "    sequences = []\n",
    "\n",
    "    # Stage A: Run each method alone\n",
    "    for method in methods_all:\n",
    "        sequences.append([method] * 10)\n",
    "\n",
    "    # Stage B: Fix KNN for 1 part, cycle rest 9\n",
    "    fixed_1 = [\"knn\"] * 1\n",
    "    for method in methods_all:\n",
    "        if method != \"knn\":\n",
    "            sequences.append(fixed_1 + [method] * 9)\n",
    "\n",
    "    # Stage C onward: Incrementally grow the fixed prefix\n",
    "    base = [\"knn\"] * 1 + [\"iterative_simple\"] * 2\n",
    "    sequences.append(base + [\"mean\"] * 7)\n",
    "    sequences.append(base + [\"median\"] * 7)\n",
    "    sequences.append(base + [\"xgboost\"] * 7)\n",
    "    sequences.append(base + [\"gan\"] * 7)\n",
    "    sequences.append(base + [\"lstm\"] * 7)\n",
    "    sequences.append(base + [\"rnn\"] * 7)\n",
    "\n",
    "    # Add LSTM to base\n",
    "    base += [\"lstm\"] * 2\n",
    "    sequences.append(base + [\"mean\"] * 5)\n",
    "    sequences.append(base + [\"gan\"] * 5)\n",
    "    sequences.append(base + [\"rnn\"] * 5)\n",
    "\n",
    "    # Add RNN to base\n",
    "    base += [\"rnn\"] * 2\n",
    "    sequences.append(base + [\"gan\"] * 3)\n",
    "\n",
    "    return sequences\n",
    "\n",
    "# Run all sequences\n",
    "sequences = build_sequences()\n",
    "\n",
    "# Loop through all method combinations\n",
    "for idx, method_names in enumerate(sequences):\n",
    "    thresholds = [0.10] * len(method_names)\n",
    "\n",
    "    for name in datasets:\n",
    "        logging.info(f\"[#{idx}] Imputing: {name} with {method_names}\")\n",
    "\n",
    "        df = globals().get(name)\n",
    "        if df is None or not isinstance(df, pd.DataFrame):\n",
    "            logging.info(f\"Skipping {name} (not found or not a DataFrame)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            imputed_df, method_log = hierarchical_impute_dynamic(\n",
    "                df=df,\n",
    "                thresholds=thresholds,\n",
    "                method_names=method_names,\n",
    "                method_registry=imputer_registry,\n",
    "                random_state=0,\n",
    "                return_method_log=True,\n",
    "                dataset_name=name,\n",
    "                plot_id=idx\n",
    "            )\n",
    "\n",
    "            output_path = f\"CSV/exports/CIR-16/impute/{name}_seq_{idx:02d}.csv\"\n",
    "            imputed_df.to_csv(output_path, index=False)\n",
    "            logging.info(f\"Saved: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Failed to impute {name} at step #{idx}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92901377-2464-44c8-b32c-523e76f9f154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc015c8b-a7e0-4522-8c20-febfedc95e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea67f34-a2da-463c-aeaa-a541246513d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce724ba3-4aa6-4e76-af68-bf80f04280b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd840ba-2344-4200-987d-16e6963bf2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe0bab-327a-452f-a202-922feabd9eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e2f0f-8a04-4fa6-a38a-6ff175d4a6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b14e4c-e12c-4e0c-9c65-b56f14f96ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a895abb-36b2-49f8-aad1-0cc5bba4d93f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c291b-07c6-4a89-877f-e4d0bfba5dad",
   "metadata": {},
   "outputs": [],
=======
      "2025-05-07 18:43:16,451 - INFO - Imputing: o4_X_train\n",
      "2025-05-07 18:43:16,853 - INFO - Group 1 (0.00, 0.10] -> lstm | 12 rows\n",
      "2025-05-07 18:43:20,894 - INFO - [LSTM Epoch 0] Loss: 0.1909\n",
      "2025-05-07 18:43:29,062 - INFO - [LSTM Epoch 100] Loss: 0.0279\n",
      "2025-05-07 18:43:36,952 - INFO - [LSTM Epoch 200] Loss: 0.0224\n",
      "2025-05-07 18:43:44,989 - INFO - [LSTM Epoch 300] Loss: 0.0214\n",
      "2025-05-07 18:43:52,910 - INFO - [LSTM Epoch 400] Loss: 0.0205\n",
      "2025-05-07 18:44:01,000 - INFO - [LSTM Epoch 500] Loss: 0.0203\n",
      "2025-05-07 18:44:08,937 - INFO - [LSTM Epoch 600] Loss: 0.0202\n",
      "2025-05-07 18:44:17,043 - INFO - [LSTM Epoch 700] Loss: 0.0202\n",
      "2025-05-07 18:44:24,995 - INFO - [LSTM Epoch 800] Loss: 0.0202\n",
      "2025-05-07 18:44:33,084 - INFO - [LSTM Epoch 900] Loss: 0.0201\n",
      "2025-05-07 18:44:40,967 - INFO - [LSTM Epoch 999] Loss: 0.0201\n",
      "2025-05-07 18:44:41,669 - INFO - Group 2 (0.10, 0.20] -> lstm | 2124 rows\n",
      "2025-05-07 18:44:42,942 - INFO - [LSTM Epoch 0] Loss: 0.0419\n",
      "2025-05-07 18:45:23,118 - INFO - [LSTM Epoch 100] Loss: 0.0069\n",
      "2025-05-07 18:46:02,877 - INFO - [LSTM Epoch 200] Loss: 0.0055\n",
      "2025-05-07 18:46:43,287 - INFO - [LSTM Epoch 300] Loss: 0.0049\n",
      "2025-05-07 18:47:23,643 - INFO - [LSTM Epoch 400] Loss: 0.0047\n",
      "2025-05-07 18:48:04,180 - INFO - [LSTM Epoch 500] Loss: 0.0046\n",
      "2025-05-07 18:48:44,155 - INFO - [LSTM Epoch 600] Loss: 0.0046\n",
      "2025-05-07 18:49:24,445 - INFO - [LSTM Epoch 700] Loss: 0.0045\n",
      "2025-05-07 18:50:04,355 - INFO - [LSTM Epoch 800] Loss: 0.0045\n",
      "2025-05-07 18:50:44,746 - INFO - [LSTM Epoch 900] Loss: 0.0044\n",
      "2025-05-07 18:51:24,422 - INFO - [LSTM Epoch 999] Loss: 0.0044\n",
      "2025-05-07 18:51:25,540 - INFO - Group 3 (0.20, 0.30] -> lstm | 6048 rows\n",
      "2025-05-07 18:51:27,909 - INFO - [LSTM Epoch 0] Loss: 0.0078\n",
      "2025-05-07 18:53:45,382 - INFO - [LSTM Epoch 100] Loss: 0.0041\n",
      "2025-05-07 18:55:59,356 - INFO - [LSTM Epoch 200] Loss: 0.0040\n",
      "2025-05-07 18:58:14,610 - INFO - [LSTM Epoch 300] Loss: 0.0039\n",
      "2025-05-07 19:00:32,657 - INFO - [LSTM Epoch 400] Loss: 0.0039\n",
      "2025-05-07 19:02:49,626 - INFO - [LSTM Epoch 500] Loss: 0.0038\n",
      "2025-05-07 19:05:05,744 - INFO - [LSTM Epoch 600] Loss: 0.0038\n",
      "2025-05-07 19:07:21,022 - INFO - [LSTM Epoch 700] Loss: 0.0038\n",
      "2025-05-07 19:09:35,193 - INFO - [LSTM Epoch 800] Loss: 0.0038\n",
      "2025-05-07 19:11:49,239 - INFO - [LSTM Epoch 900] Loss: 0.0038\n",
      "2025-05-07 19:14:02,528 - INFO - [LSTM Epoch 999] Loss: 0.0038\n",
      "2025-05-07 19:14:03,947 - INFO - Group 4 (0.30, 0.40] -> lstm | 9876 rows\n",
      "2025-05-07 19:14:07,874 - INFO - [LSTM Epoch 0] Loss: 0.0041\n",
      "2025-05-07 19:18:53,760 - INFO - [LSTM Epoch 100] Loss: 0.0034\n",
      "2025-05-07 19:23:42,421 - INFO - [LSTM Epoch 200] Loss: 0.0033\n",
      "2025-05-07 19:28:32,544 - INFO - [LSTM Epoch 300] Loss: 0.0033\n",
      "2025-05-07 19:33:44,535 - INFO - [LSTM Epoch 400] Loss: 0.0033\n",
      "2025-05-07 19:38:40,034 - INFO - [LSTM Epoch 500] Loss: 0.0033\n",
      "2025-05-07 19:43:44,475 - INFO - [LSTM Epoch 600] Loss: 0.0033\n",
      "2025-05-07 19:49:11,211 - INFO - [LSTM Epoch 700] Loss: 0.0033\n",
      "2025-05-07 19:54:38,195 - INFO - [LSTM Epoch 800] Loss: 0.0033\n",
      "2025-05-07 19:59:48,239 - INFO - [LSTM Epoch 900] Loss: 0.0033\n",
      "2025-05-07 20:04:32,427 - INFO - [LSTM Epoch 999] Loss: 0.0033\n",
      "2025-05-07 20:04:34,970 - INFO - Group 5 (0.40, 0.50] -> lstm | 10116 rows\n",
      "2025-05-07 20:04:40,296 - INFO - [LSTM Epoch 0] Loss: 0.0031\n",
      "2025-05-07 20:11:51,555 - INFO - [LSTM Epoch 100] Loss: 0.0029\n",
      "2025-05-07 20:18:54,847 - INFO - [LSTM Epoch 200] Loss: 0.0028\n",
      "2025-05-07 20:25:58,099 - INFO - [LSTM Epoch 300] Loss: 0.0028\n",
      "2025-05-07 20:32:55,705 - INFO - [LSTM Epoch 400] Loss: 0.0028\n",
      "2025-05-07 20:39:57,539 - INFO - [LSTM Epoch 500] Loss: 0.0028\n",
      "2025-05-07 20:46:54,782 - INFO - [LSTM Epoch 600] Loss: 0.0028\n",
      "2025-05-07 20:53:55,720 - INFO - [LSTM Epoch 700] Loss: 0.0028\n",
      "2025-05-07 21:00:51,614 - INFO - [LSTM Epoch 800] Loss: 0.0028\n",
      "2025-05-07 21:07:50,838 - INFO - [LSTM Epoch 900] Loss: 0.0028\n",
      "2025-05-07 21:14:48,687 - INFO - [LSTM Epoch 999] Loss: 0.0028\n",
      "2025-05-07 21:14:52,065 - INFO - Group 6 (0.50, 0.60] -> lstm | 1092 rows\n",
      "2025-05-07 21:14:57,513 - INFO - [LSTM Epoch 0] Loss: 0.0028\n",
      "2025-05-07 21:22:17,747 - INFO - [LSTM Epoch 100] Loss: 0.0028\n",
      "2025-05-07 21:29:33,959 - INFO - [LSTM Epoch 200] Loss: 0.0028\n",
      "2025-05-07 21:36:52,555 - INFO - [LSTM Epoch 300] Loss: 0.0028\n",
      "2025-05-07 21:44:17,023 - INFO - [LSTM Epoch 400] Loss: 0.0028\n",
      "2025-05-07 21:51:36,588 - INFO - [LSTM Epoch 500] Loss: 0.0028\n",
      "2025-05-07 21:58:59,841 - INFO - [LSTM Epoch 600] Loss: 0.0028\n",
      "2025-05-07 22:06:26,872 - INFO - [LSTM Epoch 700] Loss: 0.0028\n",
      "2025-05-07 22:13:50,705 - INFO - [LSTM Epoch 800] Loss: 0.0028\n",
      "2025-05-07 22:21:12,604 - INFO - [LSTM Epoch 900] Loss: 0.0028\n",
      "2025-05-07 22:28:29,598 - INFO - [LSTM Epoch 999] Loss: 0.0028\n",
      "2025-05-07 22:28:32,800 - INFO - Group 7 (0.60, 0.70] -> lstm | 108 rows\n",
      "2025-05-07 22:28:38,399 - INFO - [LSTM Epoch 0] Loss: 0.0028\n",
      "2025-05-07 22:36:09,337 - INFO - [LSTM Epoch 100] Loss: 0.0027\n",
      "2025-05-07 22:43:42,794 - INFO - [LSTM Epoch 200] Loss: 0.0028\n",
      "2025-05-07 22:51:27,495 - INFO - [LSTM Epoch 300] Loss: 0.0027\n",
      "2025-05-07 22:59:16,116 - INFO - [LSTM Epoch 400] Loss: 0.0027\n",
      "2025-05-07 23:06:59,019 - INFO - [LSTM Epoch 500] Loss: 0.0027\n",
      "2025-05-07 23:14:40,584 - INFO - [LSTM Epoch 600] Loss: 0.0027\n"
     ]
    }
   ],
>>>>>>> 153739aa1612aa808173f075b1c5b5c3666c585d
   "source": [
    "\"\"\"\n",
    "Methods we define and can use:\n",
    "mean, median, knn, iterative_simple, iterative_function, xgboost, gan, lstm, rnn\n",
    "\"\"\"\n",
    "\n",
    "# List of dataset names to impute\n",
    "datasets = [\n",
    "    #\"o1_X_train\", \"o1_X_validate\", \"o1_X_test\", \"o1_X_external\",\n",
    "    #\"o2_X_train\", \"o2_X_validate\", \"o2_X_test\", \"o2_X_external\",\n",
    "    #\"o3_X_train\", \"o3_X_validate\", \"o3_X_test\", \"o3_X_external\",\n",
    "    \"o4_X_train\", \"o4_X_validate\", \"o4_X_test\", \"o4_X_external\"\n",
    "]\n",
    "\n",
    "\n",
    "# Define thresholds and corresponding methods\n",
    "thresholds = [0.10] * 10\n",
    "#method_names = [\"knn\"] * 2 + [\"iterative\"] * 4 + [\"lstm\"] * 4 + [\"rnn\"] * 4 + [\"gan\"] * 6\n",
    "\n",
    "method_names = [\"lstm\"] * 10\n",
    "\n",
    "# Loop through and apply imputation\n",
    "for name in datasets:\n",
    "    logging.info(f\"Imputing: {name}\")\n",
    "    df = globals().get(name)\n",
    "\n",
    "    if df is None or not isinstance(df, pd.DataFrame):\n",
    "        logging.info(f\"Skipping {name} (not found or not a DataFrame)\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        imputed_df, method_log = hierarchical_impute_dynamic(\n",
    "            df=df,\n",
    "            thresholds=thresholds,\n",
    "            method_names=method_names,\n",
    "            method_registry=imputer_registry,\n",
    "            random_state=0,\n",
    "            return_method_log=True,\n",
    "            dataset_name=name\n",
    "        )\n",
    "\n",
    "        output_path = f\"CSV/exports/CIR-16/impute/{name}_simple_lstm_imputed_.csv\"\n",
    "        imputed_df.to_csv(output_path, index=False)\n",
    "        logging.info(f\"Saved: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Failed to impute {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7dede5-8a94-4dd5-8a35-5c9adb2c1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "output_path = \"CSV/exports/CIR-16/impute/\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "imputed_df.to_csv(os.path.join(output_path, f\"{file_name}.csv\"), index=False)\n",
    "method_log.to_csv(os.path.join(output_path, f\"{file_name}_method_log.csv\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7748f-385a-417a-a605-814f2a09008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"small_data = o4_X_train.iloc[:1000, :]  # picking rows\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
