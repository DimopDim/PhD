{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ed3e02-f081-456b-856f-a09ce46d1f96",
   "metadata": {},
   "source": [
    "# CIR-03: Hierarchical Imputation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6e9d4-78c6-4011-b711-c666178fa491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, Dense, Concatenate, GRU, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09007333-6aa8-45a4-8236-9a4b891fd7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial logger setup\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Global variable to hold the active file handler\n",
    "current_file_handler = None\n",
    "\n",
    "# Create the stream handler\n",
    "stream_handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "def switch_log_file(filename):\n",
    "    global current_file_handler\n",
    "\n",
    "    # If a file handler already exists, remove and close it\n",
    "    if current_file_handler:\n",
    "        logger.removeHandler(current_file_handler)\n",
    "        current_file_handler.close()\n",
    "\n",
    "    # Create a new file handler\n",
    "    current_file_handler = logging.FileHandler(filename)\n",
    "    current_file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(current_file_handler)\n",
    "\n",
    "    logger.info(f\"Switched logging to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cfd680-ccfb-4b55-b717-3ab5b6cf4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-2.log')\n",
    "logger.info(\"This is being logged to CIR-2.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b582432-2054-46b9-8452-70037a2f0b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "data_path = \"../04_ANN/CSV/exports/split_set/without_multiple_rows\"\n",
    "all_files = os.listdir(data_path)\n",
    "\n",
    "logging.info(\"+++++++++++++++++CIR-2+++++++++++++++++++++++++\")\n",
    "logging.info(\"Start Loading Dataframes.\")\n",
    "\n",
    "# Load CSVs into a dictionary of dataframes\n",
    "dataframes = {}\n",
    "for file in all_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        var_name = file.replace(\".csv\", \"\").replace(\"-\", \"_\")\n",
    "        logging.info(f\"Loading... -> {file}\")\n",
    "        dataframes[var_name] = pd.read_csv(os.path.join(data_path, file)).astype('float32')\n",
    "\n",
    "# Log loaded datasets\n",
    "for var_name, df in dataframes.items():\n",
    "    globals()[var_name] = df\n",
    "    logging.info(f\"{var_name} loaded successfully with shape {df.shape}\")\n",
    "logging.info(\"Load Complete.\")\n",
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d3072-f6a8-4100-adc2-5cc68a031614",
   "metadata": {},
   "source": [
    "# CIR-14: Implement Row Segmentation by Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216f073-770a-482b-98f7-2f56fbca1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-14.log')\n",
    "logger.info(\"This is being logged to CIR-14.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3dfc4a-aaa2-4c81-837c-5daad8897160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Segments the dataframe rows into categories based on the percentage of missing values.\n",
    "\"\"\"\n",
    "def segment_rows_by_missingness(df: pd.DataFrame):\n",
    "    row_missing_perc = df.isnull().mean(axis=1)\n",
    "\n",
    "    segments = {\n",
    "        'very_low_missing 0% < 20%': df[(row_missing_perc <= 0.20)],\n",
    "        'low_missing 21% <= 40%': df[(row_missing_perc > 0.20) & (row_missing_perc <= 0.40)],\n",
    "        'moderate_missing 41% <= 60%': df[(row_missing_perc > 0.40) & (row_missing_perc <= 0.60)],\n",
    "        'high_missing > 60%': df[(row_missing_perc > 0.60)]\n",
    "    }\n",
    "\n",
    "    row_indices = {\n",
    "        name: segment.index.tolist() for name, segment in segments.items()\n",
    "    }\n",
    "\n",
    "    return segments, row_indices, row_missing_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2138fc7-4ae4-450e-90ef-7142d331d211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Segment and log all X_ dataframes\n",
    "logging.info(\"---------------\")\n",
    "for var_name, df in dataframes.items():\n",
    "    if not var_name.startswith(\"o\") or \"_X_\" not in var_name:\n",
    "        continue  # Skip non-feature or target datasets\n",
    "\n",
    "    logging.info(f\"Segmenting rows by missingness for: {var_name}\")\n",
    "    logging.info(f\"{var_name} - Total rows: {df.shape[0]}\")\n",
    "\n",
    "    segments, row_indices, row_missing_perc = segment_rows_by_missingness(df)\n",
    "\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        logging.info(f\"{var_name} - {segment_name}: {len(segment_df)} rows\")\n",
    "    logging.info(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c545a-d22c-4b99-ad2e-0eb4c8be5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "base_plot_path = \"figures/CIR-14\"\n",
    "os.makedirs(base_plot_path, exist_ok=True)\n",
    "\n",
    "# Seaborn aesthetic settings\n",
    "sns.set(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "\n",
    "for var_name, df in dataframes.items():\n",
    "    if not var_name.startswith(\"o\") or \"_X_\" not in var_name:\n",
    "        continue  # Skip targets\n",
    "\n",
    "    logging.info(f\"Processing missing distribution plot for {var_name}\")\n",
    "\n",
    "    # Calculate row-wise missingness\n",
    "    row_missing_perc = df.isnull().mean(axis=1)\n",
    "    segments, _, _ = segment_rows_by_missingness(df)\n",
    "\n",
    "    # Prepare summary box content\n",
    "    summary_text = (\n",
    "        f\"Total rows: {len(df):,}\\n\"\n",
    "        f\"Very low (â‰¤20%): {len(segments['very_low_missing 0% < 20%']):,}\\n\"\n",
    "        f\"Low (21â€“40%): {len(segments['low_missing 21% <= 40%']):,}\\n\"\n",
    "        f\"Moderate (41â€“60%): {len(segments['moderate_missing 41% <= 60%']):,}\\n\"\n",
    "        f\"High (>60%): {len(segments['high_missing > 60%']):,}\"\n",
    "    )\n",
    "\n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    sns.histplot(row_missing_perc, bins=20, kde=True, color='#2c7fb8', edgecolor='black', ax=ax)\n",
    "\n",
    "    # Customize titles and labels\n",
    "    ax.set_title(f\"Row-wise Missing Value Distribution\\n{var_name}\", fontsize=18, fontweight='bold')\n",
    "    ax.set_xlabel(\"Proportion of Missing Values\", fontsize=15)\n",
    "    ax.set_ylabel(\"Number of Rows\", fontsize=15)\n",
    "\n",
    "    # Add summary box to top-right\n",
    "    ax.text(\n",
    "        0.99, 0.95, summary_text,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=12,\n",
    "        verticalalignment='top',\n",
    "        horizontalalignment='right',\n",
    "        bbox=dict(boxstyle=\"round,pad=0.4\", facecolor='whitesmoke', alpha=0.85, edgecolor='gray')\n",
    "    )\n",
    "\n",
    "    # Add grid with transparency\n",
    "    ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.6)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "    # Optional: Add watermark tag\n",
    "    ax.text(0.01, 0.01, \"CIR-14\", transform=ax.transAxes,\n",
    "            fontsize=10, color='gray', alpha=0.7, ha='left', va='bottom')\n",
    "\n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plot_filename = os.path.join(base_plot_path, f\"{var_name}_missing_distribution.png\")\n",
    "    fig.savefig(plot_filename, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    logging.info(f\"Saved professional missingness plot to {plot_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1f44c-cae0-48dd-b990-cfdbfc2f545d",
   "metadata": {},
   "source": [
    "# CIR-15: Register Multiple Imputation Methods\n",
    "## mean, median, knn, iterative, xgboost, gan, LSTM, RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab32877-0301-43d6-a194-648de0569bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-15.log')\n",
    "logger.info(\"This is being logged to CIR-15.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc04268-628f-4419-a0b7-ccf4fd1f4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Impute missing values using XGBoost regression for each column independently.\n",
    "\"\"\"\n",
    "def xgboost_imputer(df, random_state=0):\n",
    "    df_imputed = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() == 0:\n",
    "            continue  # Skip fully observed columns\n",
    "\n",
    "        # Split rows with and without missing values in this column\n",
    "        not_null_idx = df[col].notnull()\n",
    "        null_idx = df[col].isnull()\n",
    "\n",
    "        X_train = df.loc[not_null_idx].drop(columns=[col])\n",
    "        y_train = df.loc[not_null_idx, col]\n",
    "        X_pred = df.loc[null_idx].drop(columns=[col])\n",
    "\n",
    "        # Skip if nothing to predict\n",
    "        if X_pred.empty:\n",
    "            continue\n",
    "\n",
    "        # Drop columns that are completely NaN\n",
    "        X_train = X_train.dropna(axis=1, how='all')\n",
    "        X_pred = X_pred[X_train.columns]  # keep same columns\n",
    "\n",
    "        # Fill remaining NaNs with column means (simple fallback)\n",
    "        X_train = X_train.fillna(X_train.mean())\n",
    "        X_pred = X_pred.fillna(X_train.mean())\n",
    "\n",
    "        # Train XGBoost model\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=random_state,\n",
    "            verbosity=0\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_pred)\n",
    "\n",
    "        # Impute predicted values\n",
    "        df_imputed.loc[null_idx, col] = y_pred\n",
    "\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba2c679-1f08-49ed-a8c6-6789ddcc6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Impute missing values using an LSTM autoencoder.\n",
    "Works best for dense rows (e.g., <40% missing).\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Impute missing values using an LSTM autoencoder.\n",
    "Works best for dense rows (e.g., <40% missing).\n",
    "\"\"\"\n",
    "\n",
    "# Cache the model outside the function (top-level variable)\n",
    "_lstm_model = None\n",
    "\n",
    "def lstm_imputer(df, random_state=0, epochs=30, batch_size=64):\n",
    "    global _lstm_model\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    idx = df_copy.index\n",
    "    cols = df_copy.columns\n",
    "\n",
    "    # Fill missing values and normalize\n",
    "    df_filled = df_copy.fillna(df_copy.mean())\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = scaler.fit_transform(df_filled)\n",
    "    X = df_scaled.reshape((df_scaled.shape[0], 1, df_scaled.shape[1]))\n",
    "    input_dim = X.shape[2]\n",
    "\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Only build the model once\n",
    "    if _lstm_model is None:\n",
    "        input_layer = Input(shape=(1, input_dim))\n",
    "        encoded = LSTM(64, activation=\"relu\", return_sequences=False)(input_layer)\n",
    "        repeated = RepeatVector(1)(encoded)\n",
    "        decoded = LSTM(input_dim, activation=\"sigmoid\", return_sequences=True)(repeated)\n",
    "        _lstm_model = Model(inputs=input_layer, outputs=decoded)\n",
    "        _lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\")\n",
    "\n",
    "    # Train and log loss\n",
    "    for epoch in range(epochs):\n",
    "        history = _lstm_model.fit(X, X, epochs=1, batch_size=batch_size, verbose=0)\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            logging.info(f\"[LSTM Epoch {epoch}] Loss: {history.history['loss'][0]:.4f}\")\n",
    "\n",
    "    # Predict and inverse transform\n",
    "    X_imputed = _lstm_model.predict(X, verbose=0)\n",
    "    df_imputed_array = scaler.inverse_transform(X_imputed[:, 0, :])\n",
    "    df_imputed = pd.DataFrame(df_imputed_array, columns=cols, index=idx)\n",
    "\n",
    "    # Only fill missing values\n",
    "    for col in cols:\n",
    "        missing_mask = df[col].isnull()\n",
    "        df_copy.loc[missing_mask, col] = df_imputed.loc[missing_mask, col]\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa26cf-96ef-45ee-ae57-ac70868f3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GAN-style imputer for missing data based on GAIN.\n",
    "Arguments:\n",
    "    df (pd.DataFrame): Input dataframe with missing values.\n",
    "    random_state (int): Seed for reproducibility.\n",
    "    epochs (int): Number of training iterations.\n",
    "    batch_size (int): Batch size for training.\n",
    "Returns:\n",
    "    pd.DataFrame: Imputed dataframe.\n",
    "\"\"\"\n",
    "def gan_imputer(df, random_state=0, epochs=1000, batch_size=128):\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    cols = df_copy.columns\n",
    "    idx = df_copy.index\n",
    "\n",
    "    # ===== Step 1: Normalize & Create mask =====\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = scaler.fit_transform(df_copy.fillna(0))  # Fill NA with 0 for scaling\n",
    "    mask = ~df_copy.isnull().values  # 1 where observed, 0 where missing\n",
    "\n",
    "    data_dim = df_scaled.shape[1]\n",
    "    \n",
    "    # ===== Step 2: Generator =====\n",
    "    def build_generator():\n",
    "        inputs = Input(shape=(data_dim * 2,))\n",
    "        x = Dense(128, activation='relu')(inputs)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dense(data_dim, activation='sigmoid')(x)\n",
    "        return Model(inputs, x)\n",
    "\n",
    "    # ===== Step 3: Discriminator =====\n",
    "    def build_discriminator():\n",
    "        inputs = Input(shape=(data_dim * 2,))\n",
    "        x = Dense(128, activation='relu')(inputs)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dense(data_dim, activation='sigmoid')(x)\n",
    "        return Model(inputs, x)\n",
    "\n",
    "    G = build_generator()\n",
    "    D = build_discriminator()\n",
    "    G.compile(loss='binary_crossentropy', optimizer=Adam(0.001))\n",
    "    D.compile(loss='binary_crossentropy', optimizer=Adam(0.001))\n",
    "\n",
    "    # ===== Step 4: Training =====\n",
    "    for epoch in range(epochs):\n",
    "        # === Consistent batch size to avoid retracing ===\n",
    "        if df_scaled.shape[0] < batch_size:\n",
    "            repeat_factor = int(np.ceil(batch_size / df_scaled.shape[0]))\n",
    "            X_batch = np.tile(df_scaled, (repeat_factor, 1))[:batch_size]\n",
    "            M_batch = np.tile(mask, (repeat_factor, 1))[:batch_size]\n",
    "        else:\n",
    "            batch_idx = np.random.choice(df_scaled.shape[0], batch_size, replace=False)\n",
    "            X_batch = df_scaled[batch_idx]\n",
    "            M_batch = mask[batch_idx]\n",
    "\n",
    "        Z_batch = np.random.uniform(0, 0.01, size=X_batch.shape)\n",
    "        X_hat = M_batch * X_batch + (1 - M_batch) * Z_batch\n",
    "        G_input = np.concatenate([X_hat, M_batch], axis=1)\n",
    "\n",
    "        G_sample = G.predict(G_input, verbose=0)\n",
    "        X_fake = M_batch * X_batch + (1 - M_batch) * G_sample\n",
    "\n",
    "        D_input_real = np.concatenate([X_batch, M_batch], axis=1)\n",
    "        D_input_fake = np.concatenate([X_fake, M_batch], axis=1)\n",
    "\n",
    "        D_loss_real = D.train_on_batch(D_input_real, M_batch)\n",
    "        D_loss_fake = D.train_on_batch(D_input_fake, M_batch)\n",
    "\n",
    "        # === Train Generator ===\n",
    "        G_loss = G.train_on_batch(G_input, M_batch)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            logging.info(f\"[{epoch}] D_loss: {(D_loss_real + D_loss_fake) / 2:.4f} | G_loss: {G_loss:.4f}\")\n",
    "\n",
    "    # ===== Step 5: Imputation =====\n",
    "    Z_full = np.random.uniform(0, 0.01, size=df_scaled.shape)\n",
    "    X_hat_full = mask * df_scaled + (1 - mask) * Z_full\n",
    "    G_input_full = np.concatenate([X_hat_full, mask], axis=1)\n",
    "\n",
    "    G_imputed = G.predict(G_input_full, verbose=0)\n",
    "    X_final = mask * df_scaled + (1 - mask) * G_imputed\n",
    "\n",
    "    df_imputed_array = scaler.inverse_transform(X_final)\n",
    "    df_imputed = pd.DataFrame(df_imputed_array, columns=cols, index=idx)\n",
    "\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8aae06-9847-4169-9fd2-df67e8727d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Impute missing values using a GRU-based autoencoder.\n",
    "\"\"\"\n",
    "\n",
    "# Cache model to avoid retracing\n",
    "_rnn_model = None\n",
    "\n",
    "\"\"\"\n",
    "Parameter patience enables the early stopping if the training doesn't produce\n",
    "better MAE than the previews 10 or what number we put predictions.\n",
    "\n",
    "The mask_ratio controls the percentage of the known values are hidden in order\n",
    "to predict them.\n",
    "    - Low (0.05â€“0.1) Only a few known values are hidden per row.\n",
    "      Training is conservative but may not learn well.\n",
    "    - Medium (0.2â€“0.3) Balanced â€” enough challenge for learning while\n",
    "      preserving input context.\n",
    "    - High (0.4â€“0.5+) Very challenging â€” model must infer much of the input,\n",
    "      good for robustness, risky for small data.\n",
    "\"\"\"\n",
    "def rnn_imputer(df, random_state=0, epochs=1000, batch_size=64, mask_ratio=0.2, patience=10):\n",
    "    global _rnn_model\n",
    "\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    idx = df_copy.index\n",
    "    cols = df_copy.columns\n",
    "\n",
    "    # Step 1: Fill initial NaNs with column mean\n",
    "    df_filled = df_copy.fillna(df_copy.mean())\n",
    "\n",
    "    # Step 2: Scale\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = scaler.fit_transform(df_filled)\n",
    "\n",
    "    # Save original\n",
    "    X_original = df_scaled.copy()\n",
    "    input_dim = X_original.shape[1]\n",
    "\n",
    "    # Step 3: Build model once\n",
    "    if _rnn_model is None:\n",
    "        input_layer = Input(shape=(1, input_dim))\n",
    "        encoded = GRU(64, activation='relu', return_sequences=False)(input_layer)\n",
    "        encoded = Dropout(0.2)(encoded)  # ðŸ§¬ Dropout added\n",
    "        repeated = RepeatVector(1)(encoded)\n",
    "        decoded = GRU(input_dim, activation='sigmoid', return_sequences=True)(repeated)\n",
    "        _rnn_model = Model(inputs=input_layer, outputs=decoded)\n",
    "        _rnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "    # Step 4: Training loop with dynamic masking + early stopping\n",
    "    best_mae = float(\"inf\")\n",
    "    best_weights = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # === ðŸŒ€ Dynamic masking each epoch ===\n",
    "        mask = (np.random.rand(*X_original.shape) < mask_ratio)\n",
    "        X_masked = X_original.copy()\n",
    "        X_masked[mask] = 0\n",
    "\n",
    "        X_input = X_masked.reshape((X_masked.shape[0], 1, X_masked.shape[1]))\n",
    "        X_target = X_original.reshape((X_original.shape[0], 1, X_original.shape[1]))\n",
    "        loss_mask = tf.convert_to_tensor(mask.reshape((mask.shape[0], 1, mask.shape[1])), dtype=tf.float32)\n",
    "\n",
    "        # === Custom training step ===\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = _rnn_model(tf.convert_to_tensor(X_input, dtype=tf.float32))\n",
    "            loss = tf.reduce_sum(tf.square((preds - X_target) * loss_mask)) / tf.reduce_sum(loss_mask)\n",
    "\n",
    "        grads = tape.gradient(loss, _rnn_model.trainable_variables)\n",
    "        _rnn_model.optimizer.apply_gradients(zip(grads, _rnn_model.trainable_variables))\n",
    "\n",
    "        # === Benchmark MAE ===\n",
    "        mae = tf.reduce_sum(tf.abs((preds - X_target) * loss_mask)) / tf.reduce_sum(loss_mask)\n",
    "\n",
    "        # === Early stopping check ===\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_weights = _rnn_model.get_weights()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                logging.info(f\"[RNN Epoch {epoch}] Early stopping triggered. Best MAE: {best_mae.numpy():.4f}\")\n",
    "                break\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            logging.info(f\"[RNN Epoch {epoch}] Masked Loss MSE: {loss.numpy():.4f} | MAE: {mae.numpy():.4f}\")\n",
    "\n",
    "    # Restore best weights\n",
    "    if best_weights is not None:\n",
    "        _rnn_model.set_weights(best_weights)\n",
    "\n",
    "    # Step 5: Predict (impute)\n",
    "    X_pred = _rnn_model.predict(X_original.reshape((X_original.shape[0], 1, input_dim)), verbose=0)\n",
    "    X_imputed_array = scaler.inverse_transform(X_pred[:, 0, :])\n",
    "    df_imputed = pd.DataFrame(X_imputed_array, columns=cols, index=idx)\n",
    "\n",
    "    # Step 6: Replace only real missing values\n",
    "    for col in cols:\n",
    "        missing_mask = df[col].isnull()\n",
    "        df_copy.loc[missing_mask, col] = df_imputed.loc[missing_mask, col]\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58511d-c1c9-42e2-91b1-f802a7a5f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tee class to redirect output to both stdout and logging ---\n",
    "class Tee:\n",
    "    def __init__(self, *files, use_logging=False):\n",
    "        self.files = files\n",
    "        self.use_logging = use_logging\n",
    "\n",
    "    def write(self, obj):\n",
    "        for f in self.files:\n",
    "            f.write(obj)\n",
    "            f.flush()\n",
    "        if self.use_logging:\n",
    "            for line in obj.rstrip().splitlines():\n",
    "                logging.info(line)\n",
    "\n",
    "    def flush(self):\n",
    "        for f in self.files:\n",
    "            f.flush()\n",
    "\n",
    "# --- Iterative Imputer Function ---\n",
    "def impute_with_iterative(input_df, method, output_path, n_iter, log_verbose_file_path=None):\n",
    "    logging.info(f\"Starting Iterative Imputer with method={method} on input DataFrame of shape {input_df.shape}.\")\n",
    "\n",
    "    data_copy = input_df.copy()\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Estimator selection\n",
    "    if method == \"ExtraTrees\":\n",
    "        estimator = ExtraTreesRegressor(n_estimators=10, random_state=0, n_jobs=-1)\n",
    "    elif method == \"HistGradientBoosting\":\n",
    "        estimator = HistGradientBoostingRegressor(random_state=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}. Use 'ExtraTrees' or 'HistGradientBoosting'.\")\n",
    "\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=estimator,\n",
    "        max_iter=n_iter,\n",
    "        random_state=0,\n",
    "        verbose=2,\n",
    "        sample_posterior=False\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if log_verbose_file_path is not None:\n",
    "        os.makedirs(os.path.dirname(log_verbose_file_path), exist_ok=True)\n",
    "        original_stdout = sys.stdout\n",
    "        with open(log_verbose_file_path, \"w\") as log_file:\n",
    "            sys.stdout = Tee(sys.__stdout__, log_file, use_logging=True)\n",
    "            try:\n",
    "                imputed_array = imputer.fit_transform(data_copy)\n",
    "            finally:\n",
    "                sys.stdout = original_stdout\n",
    "    else:\n",
    "        sys.stdout = Tee(sys.__stdout__, use_logging=True)\n",
    "        try:\n",
    "            imputed_array = imputer.fit_transform(data_copy)\n",
    "        finally:\n",
    "            sys.stdout = sys.__stdout__\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    # Retain original index to avoid downstream assignment errors\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=data_copy.columns, index=data_copy.index)\n",
    "    imputed_df.to_csv(output_path, index=False)\n",
    "\n",
    "    logging.info(f\"Imputation completed in {runtime:.2f} seconds.\")\n",
    "    logging.info(f\"Number of NaNs after imputation: {np.isnan(imputed_df.values).sum()}\")\n",
    "    logging.info(f\"Imputed dataset saved at {output_path}\")\n",
    "\n",
    "    #describe_output_path = output_path.replace(\".csv\", \"_describe.csv\")\n",
    "    #imputed_df.describe().to_csv(describe_output_path)\n",
    "    #logging.info(f\"Basic statistics saved at {describe_output_path}\")\n",
    "\n",
    "    return imputed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89dd2f-e455-433a-923a-5d1fe18f51b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Registry of Imputation Methods ---\n",
    "imputer_registry = {\n",
    "    \"mean\": SimpleImputer(strategy=\"mean\"),\n",
    "    \"median\": SimpleImputer(strategy=\"median\"),\n",
    "    \"knn\": KNNImputer(n_neighbors=5, weights=\"uniform\"),\n",
    "    \"iterative_function\": lambda df: impute_with_iterative(\n",
    "        input_df=df,\n",
    "        method=\"ExtraTrees\",\n",
    "        output_path=\"imputed_outputs/tmp.csv\",  # dummy or default path\n",
    "        n_iter=20\n",
    "    ),\n",
    "    \"iterative_simple\": IterativeImputer(estimator=ExtraTreesRegressor(n_estimators=10, random_state=42),\n",
    "                                   max_iter=10, random_state=42),\n",
    "    \"xgboost\": xgboost_imputer,\n",
    "    \"gan\": gan_imputer,\n",
    "    \"lstm\": lstm_imputer,\n",
    "    \"rnn\": rnn_imputer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a2c90-d2f9-4d3b-8732-6d27bdcbcbbc",
   "metadata": {},
   "source": [
    "# CIR-16: Build Core Hierarchical Controller Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed8030-eb6b-42a3-b014-54289367ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-16.log')\n",
    "logger.info(\"This is being logged to CIR-16.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d226725-059e-4e89-bafe-e269e768bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dynamic hierarchical imputer using cumulative row-wise missingness and assigned methods.\n",
    "\n",
    "Parameters:\n",
    "    df (pd.DataFrame): Dataset with missing values.\n",
    "    thresholds (list): List of group widths (must sum to ~1.0).\n",
    "    method_names (list): List of method names (must match thresholds).\n",
    "    method_registry (dict): Registered methods with keys as names and values as callables or sklearn objects.\n",
    "    random_state (int): Random seed for reproducibility.\n",
    "    return_method_log (bool): Return pd.Series logging method used per row.\n",
    "\n",
    "Returns:\n",
    "    imputed_df (pd.DataFrame)\n",
    "    method_log (pd.Series) â€” only if return_method_log=True\n",
    "\"\"\"\n",
    "\n",
    "def hierarchical_impute_dynamic(\n",
    "    df,\n",
    "    thresholds,\n",
    "    method_names,\n",
    "    method_registry,\n",
    "    random_state=0,\n",
    "    return_method_log=False,\n",
    "    dataset_name=None\n",
    "):\n",
    "    if len(thresholds) != len(method_names):\n",
    "        raise ValueError(\"The number of thresholds must match the number of methods.\")\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"missing_pct\"] = df_copy.isnull().mean(axis=1)\n",
    "    cols = df_copy.columns.drop(\"missing_pct\")\n",
    "\n",
    "    global_means = df_copy[cols].mean().fillna(0)\n",
    "    global_min = df_copy[cols].min()\n",
    "    global_max = df_copy[cols].max()\n",
    "\n",
    "    imputed_df = pd.DataFrame(index=df_copy.index, columns=cols)\n",
    "    method_log = pd.Series(index=df_copy.index, dtype=\"object\")\n",
    "\n",
    "    cum_thresholds = np.cumsum(thresholds)\n",
    "    if not np.isclose(cum_thresholds[-1], 1.0):\n",
    "        raise ValueError(\"Thresholds must sum to 1.0\")\n",
    "\n",
    "    previous_imputed = None\n",
    "\n",
    "    # For visualization\n",
    "    group_names = []\n",
    "    cumulative_rows = []\n",
    "    method_names_actual = []\n",
    "    cumulative_total = 0\n",
    "\n",
    "    for i, upper_bound in enumerate(cum_thresholds):\n",
    "        lower_bound = cum_thresholds[i - 1] if i > 0 else 0.0\n",
    "        idx = df_copy.index[\n",
    "            (df_copy[\"missing_pct\"] > lower_bound) & (df_copy[\"missing_pct\"] <= upper_bound)\n",
    "        ]\n",
    "        group_data = df_copy.loc[idx, cols].copy()\n",
    "\n",
    "        for col in group_data.columns:\n",
    "            if group_data[col].isnull().all():\n",
    "                group_data[col] = global_means[col]\n",
    "\n",
    "        if group_data.empty:\n",
    "            continue\n",
    "\n",
    "        method_name = method_names[i]\n",
    "        logging.info(f\"Group {i+1} ({lower_bound:.2f}, {upper_bound:.2f}] -> {method_name} | {len(group_data)} rows\")\n",
    "\n",
    "        imputer = get_imputer(method_name, method_registry)\n",
    "\n",
    "        if previous_imputed is None:\n",
    "            combined = group_data\n",
    "        else:\n",
    "            combined = pd.concat([previous_imputed, group_data])\n",
    "\n",
    "        try:\n",
    "            if hasattr(imputer, \"fit_transform\"):\n",
    "                combined_imputed = imputer.fit_transform(combined)\n",
    "                combined_imputed = pd.DataFrame(combined_imputed, columns=combined.columns, index=combined.index)\n",
    "            else:\n",
    "                combined_imputed = imputer(combined, random_state=random_state)\n",
    "        except TypeError:\n",
    "            combined_imputed = imputer(combined)\n",
    "\n",
    "        group_imputed = combined_imputed.loc[idx].clip(lower=global_min, upper=global_max, axis=1)\n",
    "\n",
    "        imputed_df.loc[idx] = group_imputed\n",
    "        method_log.loc[idx] = method_name\n",
    "\n",
    "        previous_imputed = pd.concat([previous_imputed, group_imputed]) if previous_imputed is not None else group_imputed.copy()\n",
    "\n",
    "        group_label = f\"{int(lower_bound * 100)}%â€“{int(upper_bound * 100)}%\"\n",
    "        group_names.append(group_label)\n",
    "        cumulative_total += len(group_data)\n",
    "        cumulative_rows.append(cumulative_total)\n",
    "        method_names_actual.append(method_name)\n",
    "\n",
    "    if imputed_df.isnull().values.any():\n",
    "        raise ValueError(\"NaNs remain after hierarchical imputation!\")\n",
    "\n",
    "    # === Plot ===\n",
    "    output_dir = \"figures/CIR-16\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    unique_methods = list(set(method_names_actual))\n",
    "    palette = sns.color_palette(\"tab10\", n_colors=len(unique_methods))\n",
    "    method_color_map = {method: palette[i] for i, method in enumerate(unique_methods)}\n",
    "    colors = [method_color_map[m] for m in method_names_actual]\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.barh(\n",
    "        y=range(1, len(cumulative_rows) + 1),\n",
    "        width=cumulative_rows,\n",
    "        color=colors,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "\n",
    "    plt.yticks(ticks=range(1, len(group_names) + 1), labels=group_names)\n",
    "    plt.title(f\"Cumulative Rows Used for Imputation by Group - {dataset_name}\", fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"Missingness Range\", fontsize=12)\n",
    "    plt.xlabel(\"Cumulative Rows Used\", fontsize=12)\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "    legend_handles = [Patch(color=color, label=method) for method, color in method_color_map.items()]\n",
    "    plt.legend(handles=legend_handles, title=\"Imputation Method\", loc=\"lower right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # === Save plot with dataset-specific name ===\n",
    "    if dataset_name:\n",
    "        filename = f\"{dataset_name}_simple_rnn_cumulative_imputation_rows.png\"\n",
    "    else:\n",
    "        filename = \"cumulative_imputation_rows.png\"\n",
    "\n",
    "    plt.savefig(os.path.join(output_dir, filename), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return (imputed_df, method_log) if return_method_log else imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f71b5a-005b-49de-81a4-bc9159a32981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imputer(method_name, registry):\n",
    "    imputer = registry.get(method_name)\n",
    "    if imputer is None:\n",
    "        raise ValueError(f\"Method '{method_name}' not found or not implemented.\")\n",
    "    if hasattr(imputer, \"fit\") and hasattr(imputer, \"transform\"):\n",
    "        return copy.deepcopy(imputer)\n",
    "    return imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c291b-07c6-4a89-877f-e4d0bfba5dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Methods we define and can use:\n",
    "mean, median, knn, iterative_simple, iterative_function, xgboost, gan, lstm, rnn\n",
    "\"\"\"\n",
    "\n",
    "# List of dataset names to impute\n",
    "datasets = [\n",
    "    #\"o1_X_train\", \"o1_X_validate\", \"o1_X_test\", \"o1_X_external\",\n",
    "    #\"o2_X_train\", \"o2_X_validate\", \"o2_X_test\", \"o2_X_external\",\n",
    "    #\"o3_X_train\", \"o3_X_validate\", \"o3_X_test\", \"o3_X_external\",\n",
    "    \"o4_X_train\", \"o4_X_validate\", \"o4_X_test\", \"o4_X_external\"\n",
    "]\n",
    "\n",
    "\n",
    "# Define thresholds and corresponding methods\n",
    "thresholds = [0.10] * 10\n",
    "#method_names = [\"knn\"] * 2 + [\"iterative\"] * 4 + [\"lstm\"] * 4 + [\"rnn\"] * 4 + [\"gan\"] * 6\n",
    "\n",
    "method_names = [\"rnn\"] * 10\n",
    "\n",
    "# Loop through and apply imputation\n",
    "for name in datasets:\n",
    "    logging.info(f\"Imputing: {name}\")\n",
    "    df = globals().get(name)\n",
    "\n",
    "    if df is None or not isinstance(df, pd.DataFrame):\n",
    "        logging.info(f\"Skipping {name} (not found or not a DataFrame)\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        imputed_df, method_log = hierarchical_impute_dynamic(\n",
    "            df=df,\n",
    "            thresholds=thresholds,\n",
    "            method_names=method_names,\n",
    "            method_registry=imputer_registry,\n",
    "            random_state=0,\n",
    "            return_method_log=True,\n",
    "            dataset_name=name\n",
    "        )\n",
    "\n",
    "        output_path = f\"CSV/exports/CIR-16/impute/{name}_simple_iterative_imputed_.csv\"\n",
    "        imputed_df.to_csv(output_path, index=False)\n",
    "        logging.info(f\"Saved: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Failed to impute {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7dede5-8a94-4dd5-8a35-5c9adb2c1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "output_path = \"CSV/exports/CIR-16/impute/\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "imputed_df.to_csv(os.path.join(output_path, f\"{file_name}.csv\"), index=False)\n",
    "method_log.to_csv(os.path.join(output_path, f\"{file_name}_method_log.csv\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7748f-385a-417a-a605-814f2a09008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"small_data = o4_X_train.iloc[:1000, :]  # picking rows\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
