{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ed3e02-f081-456b-856f-a09ce46d1f96",
   "metadata": {},
   "source": [
    "# CIR-03: Hierarchical Imputation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e6e9d4-78c6-4011-b711-c666178fa491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09007333-6aa8-45a4-8236-9a4b891fd7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial logger setup\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Global variable to hold the active file handler\n",
    "current_file_handler = None\n",
    "\n",
    "# Create the stream handler\n",
    "stream_handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "def switch_log_file(filename):\n",
    "    global current_file_handler\n",
    "\n",
    "    # If a file handler already exists, remove and close it\n",
    "    if current_file_handler:\n",
    "        logger.removeHandler(current_file_handler)\n",
    "        current_file_handler.close()\n",
    "\n",
    "    # Create a new file handler\n",
    "    current_file_handler = logging.FileHandler(filename)\n",
    "    current_file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(current_file_handler)\n",
    "\n",
    "    logger.info(f\"Switched logging to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5cfd680-ccfb-4b55-b717-3ab5b6cf4688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 01:54:57,201 - INFO - Switched logging to logs/CIR-2.log\n",
      "2025-05-04 01:54:57,204 - INFO - This is being logged to CIR-2.log\n"
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-2.log')\n",
    "logger.info(\"This is being logged to CIR-2.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b582432-2054-46b9-8452-70037a2f0b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 01:54:57,217 - INFO - +++++++++++++++++CIR-2+++++++++++++++++++++++++\n",
      "2025-05-04 01:54:57,218 - INFO - Start Loading Dataframes.\n",
      "2025-05-04 01:54:57,219 - INFO - Loading... -> o1_X_external.csv\n",
      "2025-05-04 01:55:04,447 - INFO - Loading... -> o1_X_test.csv\n",
      "2025-05-04 01:55:05,056 - INFO - Loading... -> o1_X_train.csv\n",
      "2025-05-04 01:55:09,174 - INFO - Loading... -> o1_X_validate.csv\n",
      "2025-05-04 01:55:09,683 - INFO - Loading... -> o1_y_external_los.csv\n",
      "2025-05-04 01:55:09,727 - INFO - Loading... -> o1_y_external_mortality.csv\n",
      "2025-05-04 01:55:09,755 - INFO - Loading... -> o1_y_test_los.csv\n",
      "2025-05-04 01:55:09,765 - INFO - Loading... -> o1_y_test_mortality.csv\n",
      "2025-05-04 01:55:09,769 - INFO - Loading... -> o1_y_train_los.csv\n",
      "2025-05-04 01:55:09,805 - INFO - Loading... -> o1_y_train_mortality.csv\n",
      "2025-05-04 01:55:09,822 - INFO - Loading... -> o1_y_validate_los.csv\n",
      "2025-05-04 01:55:09,832 - INFO - Loading... -> o1_y_validate_mortality.csv\n",
      "2025-05-04 01:55:09,836 - INFO - Loading... -> o2_X_external.csv\n",
      "2025-05-04 01:55:13,473 - INFO - Loading... -> o2_X_test.csv\n",
      "2025-05-04 01:55:13,744 - INFO - Loading... -> o2_X_train.csv\n",
      "2025-05-04 01:55:15,760 - INFO - Loading... -> o2_X_validate.csv\n",
      "2025-05-04 01:55:16,029 - INFO - Loading... -> o2_y_external_los.csv\n",
      "2025-05-04 01:55:16,051 - INFO - Loading... -> o2_y_external_mortality.csv\n",
      "2025-05-04 01:55:16,067 - INFO - Loading... -> o2_y_test_los.csv\n",
      "2025-05-04 01:55:16,073 - INFO - Loading... -> o2_y_test_mortality.csv\n",
      "2025-05-04 01:55:16,077 - INFO - Loading... -> o2_y_train_los.csv\n",
      "2025-05-04 01:55:16,097 - INFO - Loading... -> o2_y_train_mortality.csv\n",
      "2025-05-04 01:55:16,107 - INFO - Loading... -> o2_y_validate_los.csv\n",
      "2025-05-04 01:55:16,112 - INFO - Loading... -> o2_y_validate_mortality.csv\n",
      "2025-05-04 01:55:16,116 - INFO - Loading... -> o3_X_external.csv\n",
      "2025-05-04 01:55:18,455 - INFO - Loading... -> o3_X_test.csv\n",
      "2025-05-04 01:55:18,650 - INFO - Loading... -> o3_X_train.csv\n",
      "2025-05-04 01:55:20,009 - INFO - Loading... -> o3_X_validate.csv\n",
      "2025-05-04 01:55:20,201 - INFO - Loading... -> o3_y_external_los.csv\n",
      "2025-05-04 01:55:20,218 - INFO - Loading... -> o3_y_external_mortality.csv\n",
      "2025-05-04 01:55:20,231 - INFO - Loading... -> o3_y_test_los.csv\n",
      "2025-05-04 01:55:20,236 - INFO - Loading... -> o3_y_test_mortality.csv\n",
      "2025-05-04 01:55:20,239 - INFO - Loading... -> o3_y_train_los.csv\n",
      "2025-05-04 01:55:20,255 - INFO - Loading... -> o3_y_train_mortality.csv\n",
      "2025-05-04 01:55:20,263 - INFO - Loading... -> o3_y_validate_los.csv\n",
      "2025-05-04 01:55:20,267 - INFO - Loading... -> o3_y_validate_mortality.csv\n",
      "2025-05-04 01:55:20,270 - INFO - Loading... -> o4_X_external.csv\n",
      "2025-05-04 01:55:22,049 - INFO - Loading... -> o4_X_test.csv\n",
      "2025-05-04 01:55:22,206 - INFO - Loading... -> o4_X_train.csv\n",
      "2025-05-04 01:55:23,194 - INFO - Loading... -> o4_X_validate.csv\n",
      "2025-05-04 01:55:23,341 - INFO - Loading... -> o4_y_external_los.csv\n",
      "2025-05-04 01:55:23,357 - INFO - Loading... -> o4_y_external_mortality.csv\n",
      "2025-05-04 01:55:23,366 - INFO - Loading... -> o4_y_test_los.csv\n",
      "2025-05-04 01:55:23,370 - INFO - Loading... -> o4_y_test_mortality.csv\n",
      "2025-05-04 01:55:23,373 - INFO - Loading... -> o4_y_train_los.csv\n",
      "2025-05-04 01:55:23,387 - INFO - Loading... -> o4_y_train_mortality.csv\n",
      "2025-05-04 01:55:23,392 - INFO - Loading... -> o4_y_validate_los.csv\n",
      "2025-05-04 01:55:23,396 - INFO - Loading... -> o4_y_validate_mortality.csv\n",
      "2025-05-04 01:55:23,399 - INFO - o1_X_external loaded successfully with shape (234720, 345)\n",
      "2025-05-04 01:55:23,400 - INFO - o1_X_test loaded successfully with shape (15312, 345)\n",
      "2025-05-04 01:55:23,401 - INFO - o1_X_train loaded successfully with shape (122496, 345)\n",
      "2025-05-04 01:55:23,402 - INFO - o1_X_validate loaded successfully with shape (15312, 345)\n",
      "2025-05-04 01:55:23,403 - INFO - o1_y_external_los loaded successfully with shape (234720, 1)\n",
      "2025-05-04 01:55:23,403 - INFO - o1_y_external_mortality loaded successfully with shape (234720, 1)\n",
      "2025-05-04 01:55:23,405 - INFO - o1_y_test_los loaded successfully with shape (15312, 1)\n",
      "2025-05-04 01:55:23,405 - INFO - o1_y_test_mortality loaded successfully with shape (15312, 1)\n",
      "2025-05-04 01:55:23,406 - INFO - o1_y_train_los loaded successfully with shape (122496, 1)\n",
      "2025-05-04 01:55:23,407 - INFO - o1_y_train_mortality loaded successfully with shape (122496, 1)\n",
      "2025-05-04 01:55:23,408 - INFO - o1_y_validate_los loaded successfully with shape (15312, 1)\n",
      "2025-05-04 01:55:23,409 - INFO - o1_y_validate_mortality loaded successfully with shape (15312, 1)\n",
      "2025-05-04 01:55:23,410 - INFO - o2_X_external loaded successfully with shape (117360, 345)\n",
      "2025-05-04 01:55:23,411 - INFO - o2_X_test loaded successfully with shape (7656, 345)\n",
      "2025-05-04 01:55:23,412 - INFO - o2_X_train loaded successfully with shape (61248, 345)\n",
      "2025-05-04 01:55:23,413 - INFO - o2_X_validate loaded successfully with shape (7656, 345)\n",
      "2025-05-04 01:55:23,414 - INFO - o2_y_external_los loaded successfully with shape (117360, 1)\n",
      "2025-05-04 01:55:23,415 - INFO - o2_y_external_mortality loaded successfully with shape (117360, 1)\n",
      "2025-05-04 01:55:23,416 - INFO - o2_y_test_los loaded successfully with shape (7656, 1)\n",
      "2025-05-04 01:55:23,417 - INFO - o2_y_test_mortality loaded successfully with shape (7656, 1)\n",
      "2025-05-04 01:55:23,417 - INFO - o2_y_train_los loaded successfully with shape (61248, 1)\n",
      "2025-05-04 01:55:23,418 - INFO - o2_y_train_mortality loaded successfully with shape (61248, 1)\n",
      "2025-05-04 01:55:23,419 - INFO - o2_y_validate_los loaded successfully with shape (7656, 1)\n",
      "2025-05-04 01:55:23,420 - INFO - o2_y_validate_mortality loaded successfully with shape (7656, 1)\n",
      "2025-05-04 01:55:23,420 - INFO - o3_X_external loaded successfully with shape (78240, 345)\n",
      "2025-05-04 01:55:23,421 - INFO - o3_X_test loaded successfully with shape (5104, 345)\n",
      "2025-05-04 01:55:23,422 - INFO - o3_X_train loaded successfully with shape (40832, 345)\n",
      "2025-05-04 01:55:23,424 - INFO - o3_X_validate loaded successfully with shape (5104, 345)\n",
      "2025-05-04 01:55:23,424 - INFO - o3_y_external_los loaded successfully with shape (78240, 1)\n",
      "2025-05-04 01:55:23,425 - INFO - o3_y_external_mortality loaded successfully with shape (78240, 1)\n",
      "2025-05-04 01:55:23,426 - INFO - o3_y_test_los loaded successfully with shape (5104, 1)\n",
      "2025-05-04 01:55:23,427 - INFO - o3_y_test_mortality loaded successfully with shape (5104, 1)\n",
      "2025-05-04 01:55:23,428 - INFO - o3_y_train_los loaded successfully with shape (40832, 1)\n",
      "2025-05-04 01:55:23,429 - INFO - o3_y_train_mortality loaded successfully with shape (40832, 1)\n",
      "2025-05-04 01:55:23,430 - INFO - o3_y_validate_los loaded successfully with shape (5104, 1)\n",
      "2025-05-04 01:55:23,431 - INFO - o3_y_validate_mortality loaded successfully with shape (5104, 1)\n",
      "2025-05-04 01:55:23,432 - INFO - o4_X_external loaded successfully with shape (58680, 345)\n",
      "2025-05-04 01:55:23,433 - INFO - o4_X_test loaded successfully with shape (3828, 345)\n",
      "2025-05-04 01:55:23,433 - INFO - o4_X_train loaded successfully with shape (30624, 345)\n",
      "2025-05-04 01:55:23,434 - INFO - o4_X_validate loaded successfully with shape (3828, 345)\n",
      "2025-05-04 01:55:23,435 - INFO - o4_y_external_los loaded successfully with shape (58680, 1)\n",
      "2025-05-04 01:55:23,436 - INFO - o4_y_external_mortality loaded successfully with shape (58680, 1)\n",
      "2025-05-04 01:55:23,437 - INFO - o4_y_test_los loaded successfully with shape (3828, 1)\n",
      "2025-05-04 01:55:23,438 - INFO - o4_y_test_mortality loaded successfully with shape (3828, 1)\n",
      "2025-05-04 01:55:23,439 - INFO - o4_y_train_los loaded successfully with shape (30624, 1)\n",
      "2025-05-04 01:55:23,440 - INFO - o4_y_train_mortality loaded successfully with shape (30624, 1)\n",
      "2025-05-04 01:55:23,441 - INFO - o4_y_validate_los loaded successfully with shape (3828, 1)\n",
      "2025-05-04 01:55:23,442 - INFO - o4_y_validate_mortality loaded successfully with shape (3828, 1)\n",
      "2025-05-04 01:55:23,442 - INFO - Load Complete.\n",
      "2025-05-04 01:55:23,443 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "data_path = \"../04_ANN/CSV/exports/split_set/without_multiple_rows\"\n",
    "all_files = os.listdir(data_path)\n",
    "\n",
    "logging.info(\"+++++++++++++++++CIR-2+++++++++++++++++++++++++\")\n",
    "logging.info(\"Start Loading Dataframes.\")\n",
    "\n",
    "# Load CSVs into a dictionary of dataframes\n",
    "dataframes = {}\n",
    "for file in all_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        var_name = file.replace(\".csv\", \"\").replace(\"-\", \"_\")\n",
    "        logging.info(f\"Loading... -> {file}\")\n",
    "        dataframes[var_name] = pd.read_csv(os.path.join(data_path, file)).astype('float32')\n",
    "\n",
    "# Log loaded datasets\n",
    "for var_name, df in dataframes.items():\n",
    "    globals()[var_name] = df\n",
    "    logging.info(f\"{var_name} loaded successfully with shape {df.shape}\")\n",
    "logging.info(\"Load Complete.\")\n",
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d3072-f6a8-4100-adc2-5cc68a031614",
   "metadata": {},
   "source": [
    "# CIR-14: Implement Row Segmentation by Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d216f073-770a-482b-98f7-2f56fbca1e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 01:55:23,452 - INFO - Switched logging to logs/CIR-14.log\n",
      "2025-05-04 01:55:23,454 - INFO - This is being logged to CIR-14.log\n"
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-14.log')\n",
    "logger.info(\"This is being logged to CIR-14.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a3dfc4a-aaa2-4c81-837c-5daad8897160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Segments the dataframe rows into categories based on the percentage of missing values.\n",
    "\"\"\"\n",
    "def segment_rows_by_missingness(df: pd.DataFrame):\n",
    "    row_missing_perc = df.isnull().mean(axis=1)\n",
    "\n",
    "    segments = {\n",
    "        'very_low_missing 0% < 20%': df[(row_missing_perc <= 0.20)],\n",
    "        'low_missing 21% <= 40%': df[(row_missing_perc > 0.20) & (row_missing_perc <= 0.40)],\n",
    "        'moderate_missing 41% <= 60%': df[(row_missing_perc > 0.40) & (row_missing_perc <= 0.60)],\n",
    "        'high_missing > 60%': df[(row_missing_perc > 0.60)]\n",
    "    }\n",
    "\n",
    "    row_indices = {\n",
    "        name: segment.index.tolist() for name, segment in segments.items()\n",
    "    }\n",
    "\n",
    "    return segments, row_indices, row_missing_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2138fc7-4ae4-450e-90ef-7142d331d211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 01:55:23,476 - INFO - ---------------\n",
      "2025-05-04 01:55:23,477 - INFO - Segmenting rows by missingness for: o1_X_external\n",
      "2025-05-04 01:55:23,478 - INFO - o1_X_external - Total rows: 234720\n",
      "2025-05-04 01:55:23,934 - INFO - o1_X_external - very_low_missing 0% < 20%: 576 rows\n",
      "2025-05-04 01:55:23,935 - INFO - o1_X_external - low_missing 21% <= 40%: 49729 rows\n",
      "2025-05-04 01:55:23,936 - INFO - o1_X_external - moderate_missing 41% <= 60%: 153407 rows\n",
      "2025-05-04 01:55:23,938 - INFO - o1_X_external - high_missing > 60%: 31008 rows\n",
      "2025-05-04 01:55:23,939 - INFO - ---------------\n",
      "2025-05-04 01:55:23,939 - INFO - Segmenting rows by missingness for: o1_X_test\n",
      "2025-05-04 01:55:23,940 - INFO - o1_X_test - Total rows: 15312\n",
      "2025-05-04 01:55:24,002 - INFO - o1_X_test - very_low_missing 0% < 20%: 1248 rows\n",
      "2025-05-04 01:55:24,003 - INFO - o1_X_test - low_missing 21% <= 40%: 7872 rows\n",
      "2025-05-04 01:55:24,004 - INFO - o1_X_test - moderate_missing 41% <= 60%: 5424 rows\n",
      "2025-05-04 01:55:24,005 - INFO - o1_X_test - high_missing > 60%: 768 rows\n",
      "2025-05-04 01:55:24,006 - INFO - ---------------\n",
      "2025-05-04 01:55:24,007 - INFO - Segmenting rows by missingness for: o1_X_train\n",
      "2025-05-04 01:55:24,007 - INFO - o1_X_train - Total rows: 122496\n",
      "2025-05-04 01:55:24,240 - INFO - o1_X_train - very_low_missing 0% < 20%: 8544 rows\n",
      "2025-05-04 01:55:24,242 - INFO - o1_X_train - low_missing 21% <= 40%: 63696 rows\n",
      "2025-05-04 01:55:24,243 - INFO - o1_X_train - moderate_missing 41% <= 60%: 44832 rows\n",
      "2025-05-04 01:55:24,244 - INFO - o1_X_train - high_missing > 60%: 5424 rows\n",
      "2025-05-04 01:55:24,244 - INFO - ---------------\n",
      "2025-05-04 01:55:24,245 - INFO - Segmenting rows by missingness for: o1_X_validate\n",
      "2025-05-04 01:55:24,246 - INFO - o1_X_validate - Total rows: 15312\n",
      "2025-05-04 01:55:24,294 - INFO - o1_X_validate - very_low_missing 0% < 20%: 912 rows\n",
      "2025-05-04 01:55:24,295 - INFO - o1_X_validate - low_missing 21% <= 40%: 7920 rows\n",
      "2025-05-04 01:55:24,296 - INFO - o1_X_validate - moderate_missing 41% <= 60%: 5664 rows\n",
      "2025-05-04 01:55:24,297 - INFO - o1_X_validate - high_missing > 60%: 816 rows\n",
      "2025-05-04 01:55:24,298 - INFO - ---------------\n",
      "2025-05-04 01:55:24,299 - INFO - Segmenting rows by missingness for: o2_X_external\n",
      "2025-05-04 01:55:24,300 - INFO - o2_X_external - Total rows: 117360\n",
      "2025-05-04 01:55:24,529 - INFO - o2_X_external - very_low_missing 0% < 20%: 288 rows\n",
      "2025-05-04 01:55:24,530 - INFO - o2_X_external - low_missing 21% <= 40%: 24865 rows\n",
      "2025-05-04 01:55:24,530 - INFO - o2_X_external - moderate_missing 41% <= 60%: 76703 rows\n",
      "2025-05-04 01:55:24,532 - INFO - o2_X_external - high_missing > 60%: 15504 rows\n",
      "2025-05-04 01:55:24,533 - INFO - ---------------\n",
      "2025-05-04 01:55:24,534 - INFO - Segmenting rows by missingness for: o2_X_test\n",
      "2025-05-04 01:55:24,535 - INFO - o2_X_test - Total rows: 7656\n",
      "2025-05-04 01:55:24,567 - INFO - o2_X_test - very_low_missing 0% < 20%: 624 rows\n",
      "2025-05-04 01:55:24,568 - INFO - o2_X_test - low_missing 21% <= 40%: 3936 rows\n",
      "2025-05-04 01:55:24,569 - INFO - o2_X_test - moderate_missing 41% <= 60%: 2712 rows\n",
      "2025-05-04 01:55:24,570 - INFO - o2_X_test - high_missing > 60%: 384 rows\n",
      "2025-05-04 01:55:24,571 - INFO - ---------------\n",
      "2025-05-04 01:55:24,572 - INFO - Segmenting rows by missingness for: o2_X_train\n",
      "2025-05-04 01:55:24,572 - INFO - o2_X_train - Total rows: 61248\n",
      "2025-05-04 01:55:24,694 - INFO - o2_X_train - very_low_missing 0% < 20%: 4272 rows\n",
      "2025-05-04 01:55:24,695 - INFO - o2_X_train - low_missing 21% <= 40%: 31848 rows\n",
      "2025-05-04 01:55:24,696 - INFO - o2_X_train - moderate_missing 41% <= 60%: 22416 rows\n",
      "2025-05-04 01:55:24,698 - INFO - o2_X_train - high_missing > 60%: 2712 rows\n",
      "2025-05-04 01:55:24,699 - INFO - ---------------\n",
      "2025-05-04 01:55:24,700 - INFO - Segmenting rows by missingness for: o2_X_validate\n",
      "2025-05-04 01:55:24,701 - INFO - o2_X_validate - Total rows: 7656\n",
      "2025-05-04 01:55:24,728 - INFO - o2_X_validate - very_low_missing 0% < 20%: 456 rows\n",
      "2025-05-04 01:55:24,729 - INFO - o2_X_validate - low_missing 21% <= 40%: 3960 rows\n",
      "2025-05-04 01:55:24,730 - INFO - o2_X_validate - moderate_missing 41% <= 60%: 2832 rows\n",
      "2025-05-04 01:55:24,731 - INFO - o2_X_validate - high_missing > 60%: 408 rows\n",
      "2025-05-04 01:55:24,732 - INFO - ---------------\n",
      "2025-05-04 01:55:24,733 - INFO - Segmenting rows by missingness for: o3_X_external\n",
      "2025-05-04 01:55:24,734 - INFO - o3_X_external - Total rows: 78240\n",
      "2025-05-04 01:55:24,894 - INFO - o3_X_external - very_low_missing 0% < 20%: 192 rows\n",
      "2025-05-04 01:55:24,895 - INFO - o3_X_external - low_missing 21% <= 40%: 16579 rows\n",
      "2025-05-04 01:55:24,895 - INFO - o3_X_external - moderate_missing 41% <= 60%: 51133 rows\n",
      "2025-05-04 01:55:24,896 - INFO - o3_X_external - high_missing > 60%: 10336 rows\n",
      "2025-05-04 01:55:24,898 - INFO - ---------------\n",
      "2025-05-04 01:55:24,898 - INFO - Segmenting rows by missingness for: o3_X_test\n",
      "2025-05-04 01:55:24,899 - INFO - o3_X_test - Total rows: 5104\n",
      "2025-05-04 01:55:24,922 - INFO - o3_X_test - very_low_missing 0% < 20%: 416 rows\n",
      "2025-05-04 01:55:24,923 - INFO - o3_X_test - low_missing 21% <= 40%: 2624 rows\n",
      "2025-05-04 01:55:24,924 - INFO - o3_X_test - moderate_missing 41% <= 60%: 1808 rows\n",
      "2025-05-04 01:55:24,926 - INFO - o3_X_test - high_missing > 60%: 256 rows\n",
      "2025-05-04 01:55:24,926 - INFO - ---------------\n",
      "2025-05-04 01:55:24,927 - INFO - Segmenting rows by missingness for: o3_X_train\n",
      "2025-05-04 01:55:24,928 - INFO - o3_X_train - Total rows: 40832\n",
      "2025-05-04 01:55:25,017 - INFO - o3_X_train - very_low_missing 0% < 20%: 2848 rows\n",
      "2025-05-04 01:55:25,018 - INFO - o3_X_train - low_missing 21% <= 40%: 21232 rows\n",
      "2025-05-04 01:55:25,019 - INFO - o3_X_train - moderate_missing 41% <= 60%: 14944 rows\n",
      "2025-05-04 01:55:25,020 - INFO - o3_X_train - high_missing > 60%: 1808 rows\n",
      "2025-05-04 01:55:25,021 - INFO - ---------------\n",
      "2025-05-04 01:55:25,022 - INFO - Segmenting rows by missingness for: o3_X_validate\n",
      "2025-05-04 01:55:25,023 - INFO - o3_X_validate - Total rows: 5104\n",
      "2025-05-04 01:55:25,043 - INFO - o3_X_validate - very_low_missing 0% < 20%: 304 rows\n",
      "2025-05-04 01:55:25,044 - INFO - o3_X_validate - low_missing 21% <= 40%: 2640 rows\n",
      "2025-05-04 01:55:25,046 - INFO - o3_X_validate - moderate_missing 41% <= 60%: 1888 rows\n",
      "2025-05-04 01:55:25,046 - INFO - o3_X_validate - high_missing > 60%: 272 rows\n",
      "2025-05-04 01:55:25,047 - INFO - ---------------\n",
      "2025-05-04 01:55:25,048 - INFO - Segmenting rows by missingness for: o4_X_external\n",
      "2025-05-04 01:55:25,049 - INFO - o4_X_external - Total rows: 58680\n",
      "2025-05-04 01:55:25,166 - INFO - o4_X_external - very_low_missing 0% < 20%: 144 rows\n",
      "2025-05-04 01:55:25,167 - INFO - o4_X_external - low_missing 21% <= 40%: 12430 rows\n",
      "2025-05-04 01:55:25,168 - INFO - o4_X_external - moderate_missing 41% <= 60%: 38354 rows\n",
      "2025-05-04 01:55:25,169 - INFO - o4_X_external - high_missing > 60%: 7752 rows\n",
      "2025-05-04 01:55:25,170 - INFO - ---------------\n",
      "2025-05-04 01:55:25,171 - INFO - Segmenting rows by missingness for: o4_X_test\n",
      "2025-05-04 01:55:25,172 - INFO - o4_X_test - Total rows: 3828\n",
      "2025-05-04 01:55:25,190 - INFO - o4_X_test - very_low_missing 0% < 20%: 312 rows\n",
      "2025-05-04 01:55:25,191 - INFO - o4_X_test - low_missing 21% <= 40%: 1968 rows\n",
      "2025-05-04 01:55:25,193 - INFO - o4_X_test - moderate_missing 41% <= 60%: 1356 rows\n",
      "2025-05-04 01:55:25,193 - INFO - o4_X_test - high_missing > 60%: 192 rows\n",
      "2025-05-04 01:55:25,195 - INFO - ---------------\n",
      "2025-05-04 01:55:25,196 - INFO - Segmenting rows by missingness for: o4_X_train\n",
      "2025-05-04 01:55:25,197 - INFO - o4_X_train - Total rows: 30624\n",
      "2025-05-04 01:55:25,266 - INFO - o4_X_train - very_low_missing 0% < 20%: 2136 rows\n",
      "2025-05-04 01:55:25,267 - INFO - o4_X_train - low_missing 21% <= 40%: 15924 rows\n",
      "2025-05-04 01:55:25,268 - INFO - o4_X_train - moderate_missing 41% <= 60%: 11208 rows\n",
      "2025-05-04 01:55:25,269 - INFO - o4_X_train - high_missing > 60%: 1356 rows\n",
      "2025-05-04 01:55:25,270 - INFO - ---------------\n",
      "2025-05-04 01:55:25,271 - INFO - Segmenting rows by missingness for: o4_X_validate\n",
      "2025-05-04 01:55:25,272 - INFO - o4_X_validate - Total rows: 3828\n",
      "2025-05-04 01:55:25,287 - INFO - o4_X_validate - very_low_missing 0% < 20%: 228 rows\n",
      "2025-05-04 01:55:25,288 - INFO - o4_X_validate - low_missing 21% <= 40%: 1980 rows\n",
      "2025-05-04 01:55:25,288 - INFO - o4_X_validate - moderate_missing 41% <= 60%: 1416 rows\n",
      "2025-05-04 01:55:25,290 - INFO - o4_X_validate - high_missing > 60%: 204 rows\n",
      "2025-05-04 01:55:25,291 - INFO - ---------------\n"
     ]
    }
   ],
   "source": [
    "# Segment and log all X_ dataframes\n",
    "logging.info(\"---------------\")\n",
    "for var_name, df in dataframes.items():\n",
    "    if not var_name.startswith(\"o\") or \"_X_\" not in var_name:\n",
    "        continue  # Skip non-feature or target datasets\n",
    "\n",
    "    logging.info(f\"Segmenting rows by missingness for: {var_name}\")\n",
    "    logging.info(f\"{var_name} - Total rows: {df.shape[0]}\")\n",
    "\n",
    "    segments, row_indices, row_missing_perc = segment_rows_by_missingness(df)\n",
    "\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        logging.info(f\"{var_name} - {segment_name}: {len(segment_df)} rows\")\n",
    "    logging.info(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "528c545a-d22c-4b99-ad2e-0eb4c8be5beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 01:55:25,313 - INFO - Processing missing distribution plot for o1_X_external\n",
      "2025-05-04 01:55:28,165 - INFO - Saved professional missingness plot to figures/CIR-14\\o1_X_external_missing_distribution.png\n",
      "2025-05-04 01:55:28,166 - INFO - Processing missing distribution plot for o1_X_test\n",
      "2025-05-04 01:55:29,062 - INFO - Saved professional missingness plot to figures/CIR-14\\o1_X_test_missing_distribution.png\n",
      "2025-05-04 01:55:29,063 - INFO - Processing missing distribution plot for o1_X_train\n",
      "2025-05-04 01:55:30,797 - INFO - Saved professional missingness plot to figures/CIR-14\\o1_X_train_missing_distribution.png\n",
      "2025-05-04 01:55:30,798 - INFO - Processing missing distribution plot for o1_X_validate\n",
      "2025-05-04 01:55:31,641 - INFO - Saved professional missingness plot to figures/CIR-14\\o1_X_validate_missing_distribution.png\n",
      "2025-05-04 01:55:31,642 - INFO - Processing missing distribution plot for o2_X_external\n",
      "2025-05-04 01:55:33,320 - INFO - Saved professional missingness plot to figures/CIR-14\\o2_X_external_missing_distribution.png\n",
      "2025-05-04 01:55:33,321 - INFO - Processing missing distribution plot for o2_X_test\n",
      "2025-05-04 01:55:34,132 - INFO - Saved professional missingness plot to figures/CIR-14\\o2_X_test_missing_distribution.png\n",
      "2025-05-04 01:55:34,133 - INFO - Processing missing distribution plot for o2_X_train\n",
      "2025-05-04 01:55:35,359 - INFO - Saved professional missingness plot to figures/CIR-14\\o2_X_train_missing_distribution.png\n",
      "2025-05-04 01:55:35,360 - INFO - Processing missing distribution plot for o2_X_validate\n",
      "2025-05-04 01:55:36,195 - INFO - Saved professional missingness plot to figures/CIR-14\\o2_X_validate_missing_distribution.png\n",
      "2025-05-04 01:55:36,196 - INFO - Processing missing distribution plot for o3_X_external\n",
      "2025-05-04 01:55:37,599 - INFO - Saved professional missingness plot to figures/CIR-14\\o3_X_external_missing_distribution.png\n",
      "2025-05-04 01:55:37,600 - INFO - Processing missing distribution plot for o3_X_test\n",
      "2025-05-04 01:55:38,413 - INFO - Saved professional missingness plot to figures/CIR-14\\o3_X_test_missing_distribution.png\n",
      "2025-05-04 01:55:38,414 - INFO - Processing missing distribution plot for o3_X_train\n",
      "2025-05-04 01:55:39,469 - INFO - Saved professional missingness plot to figures/CIR-14\\o3_X_train_missing_distribution.png\n",
      "2025-05-04 01:55:39,470 - INFO - Processing missing distribution plot for o3_X_validate\n",
      "2025-05-04 01:55:40,252 - INFO - Saved professional missingness plot to figures/CIR-14\\o3_X_validate_missing_distribution.png\n",
      "2025-05-04 01:55:40,252 - INFO - Processing missing distribution plot for o4_X_external\n",
      "2025-05-04 01:55:41,635 - INFO - Saved professional missingness plot to figures/CIR-14\\o4_X_external_missing_distribution.png\n",
      "2025-05-04 01:55:41,636 - INFO - Processing missing distribution plot for o4_X_test\n",
      "2025-05-04 01:55:42,401 - INFO - Saved professional missingness plot to figures/CIR-14\\o4_X_test_missing_distribution.png\n",
      "2025-05-04 01:55:42,402 - INFO - Processing missing distribution plot for o4_X_train\n",
      "2025-05-04 01:55:43,359 - INFO - Saved professional missingness plot to figures/CIR-14\\o4_X_train_missing_distribution.png\n",
      "2025-05-04 01:55:43,360 - INFO - Processing missing distribution plot for o4_X_validate\n",
      "2025-05-04 01:55:44,119 - INFO - Saved professional missingness plot to figures/CIR-14\\o4_X_validate_missing_distribution.png\n"
     ]
    }
   ],
   "source": [
    "# Ensure output directory exists\n",
    "base_plot_path = \"figures/CIR-14\"\n",
    "os.makedirs(base_plot_path, exist_ok=True)\n",
    "\n",
    "# Seaborn aesthetic settings\n",
    "sns.set(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "\n",
    "for var_name, df in dataframes.items():\n",
    "    if not var_name.startswith(\"o\") or \"_X_\" not in var_name:\n",
    "        continue  # Skip targets\n",
    "\n",
    "    logging.info(f\"Processing missing distribution plot for {var_name}\")\n",
    "\n",
    "    # Calculate row-wise missingness\n",
    "    row_missing_perc = df.isnull().mean(axis=1)\n",
    "    segments, _, _ = segment_rows_by_missingness(df)\n",
    "\n",
    "    # Prepare summary box content\n",
    "    summary_text = (\n",
    "        f\"Total rows: {len(df):,}\\n\"\n",
    "        f\"Very low (≤20%): {len(segments['very_low_missing 0% < 20%']):,}\\n\"\n",
    "        f\"Low (21–40%): {len(segments['low_missing 21% <= 40%']):,}\\n\"\n",
    "        f\"Moderate (41–60%): {len(segments['moderate_missing 41% <= 60%']):,}\\n\"\n",
    "        f\"High (>60%): {len(segments['high_missing > 60%']):,}\"\n",
    "    )\n",
    "\n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    sns.histplot(row_missing_perc, bins=20, kde=True, color='#2c7fb8', edgecolor='black', ax=ax)\n",
    "\n",
    "    # Customize titles and labels\n",
    "    ax.set_title(f\"Row-wise Missing Value Distribution\\n{var_name}\", fontsize=18, fontweight='bold')\n",
    "    ax.set_xlabel(\"Proportion of Missing Values\", fontsize=15)\n",
    "    ax.set_ylabel(\"Number of Rows\", fontsize=15)\n",
    "\n",
    "    # Add summary box to top-right\n",
    "    ax.text(\n",
    "        0.99, 0.95, summary_text,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=12,\n",
    "        verticalalignment='top',\n",
    "        horizontalalignment='right',\n",
    "        bbox=dict(boxstyle=\"round,pad=0.4\", facecolor='whitesmoke', alpha=0.85, edgecolor='gray')\n",
    "    )\n",
    "\n",
    "    # Add grid with transparency\n",
    "    ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.6)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "    # Optional: Add watermark tag\n",
    "    ax.text(0.01, 0.01, \"CIR-14\", transform=ax.transAxes,\n",
    "            fontsize=10, color='gray', alpha=0.7, ha='left', va='bottom')\n",
    "\n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plot_filename = os.path.join(base_plot_path, f\"{var_name}_missing_distribution.png\")\n",
    "    fig.savefig(plot_filename, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    logging.info(f\"Saved professional missingness plot to {plot_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1f44c-cae0-48dd-b990-cfdbfc2f545d",
   "metadata": {},
   "source": [
    "# CIR-15: Register Multiple Imputation Methods\n",
    "## mean, median, knn, iterative, xgboost, gan, LSTM, RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ab32877-0301-43d6-a194-648de0569bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 01:55:44,130 - INFO - Switched logging to logs/CIR-15.log\n",
      "2025-05-04 01:55:44,131 - INFO - This is being logged to CIR-15.log\n"
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-15.log')\n",
    "logger.info(\"This is being logged to CIR-15.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fc04268-628f-4419-a0b7-ccf4fd1f4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_imputer(df, random_state=0):\n",
    "    \"\"\"\n",
    "    Impute missing values using XGBoost regression for each column independently.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with missing values.\n",
    "        random_state (int): Random seed.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Imputed DataFrame.\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() == 0:\n",
    "            continue  # Skip fully observed columns\n",
    "\n",
    "        # Split rows with and without missing values in this column\n",
    "        not_null_idx = df[col].notnull()\n",
    "        null_idx = df[col].isnull()\n",
    "\n",
    "        X_train = df.loc[not_null_idx].drop(columns=[col])\n",
    "        y_train = df.loc[not_null_idx, col]\n",
    "        X_pred = df.loc[null_idx].drop(columns=[col])\n",
    "\n",
    "        # Skip if nothing to predict\n",
    "        if X_pred.empty:\n",
    "            continue\n",
    "\n",
    "        # Drop columns that are completely NaN\n",
    "        X_train = X_train.dropna(axis=1, how='all')\n",
    "        X_pred = X_pred[X_train.columns]  # keep same columns\n",
    "\n",
    "        # Fill remaining NaNs with column means (simple fallback)\n",
    "        X_train = X_train.fillna(X_train.mean())\n",
    "        X_pred = X_pred.fillna(X_train.mean())\n",
    "\n",
    "        # Train XGBoost model\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=random_state,\n",
    "            verbosity=0\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_pred)\n",
    "\n",
    "        # Impute predicted values\n",
    "        df_imputed.loc[null_idx, col] = y_pred\n",
    "\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba2c679-1f08-49ed-a8c6-6789ddcc6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_imputer(df, random_state=0):\n",
    "    \"\"\"\n",
    "    Impute missing values using an LSTM autoencoder.\n",
    "    Works best for dense rows (e.g., <40% missing).\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    idx = df_copy.index\n",
    "    cols = df_copy.columns\n",
    "\n",
    "    # Fill initial missing values with column means\n",
    "    df_filled = df_copy.fillna(df_copy.mean())\n",
    "\n",
    "    # Scale values to 0-1 for neural network stability\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = scaler.fit_transform(df_filled)\n",
    "\n",
    "    # Reshape to 3D [samples, timesteps, features]\n",
    "    # We'll treat each row as a \"sequence\" with 1 timestep\n",
    "    X = df_scaled.reshape((df_scaled.shape[0], 1, df_scaled.shape[1]))\n",
    "\n",
    "    # LSTM autoencoder\n",
    "    input_dim = X.shape[2]\n",
    "    input_layer = Input(shape=(1, input_dim))\n",
    "    encoded = LSTM(64, activation=\"relu\", return_sequences=False)(input_layer)\n",
    "    repeated = RepeatVector(1)(encoded)\n",
    "    decoded = LSTM(input_dim, activation=\"sigmoid\", return_sequences=True)(repeated)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\")\n",
    "\n",
    "    # Train\n",
    "    autoencoder.fit(X, X, epochs=30, batch_size=64, verbose=0)\n",
    "\n",
    "    # Predict\n",
    "    X_imputed = autoencoder.predict(X, verbose=0)\n",
    "\n",
    "    # Reshape back and inverse scale\n",
    "    df_imputed_array = scaler.inverse_transform(X_imputed[:, 0, :])\n",
    "    df_imputed = pd.DataFrame(df_imputed_array, columns=cols, index=idx)\n",
    "\n",
    "    # Replace only originally missing values\n",
    "    for col in cols:\n",
    "        missing_mask = df[col].isnull()\n",
    "        df_copy.loc[missing_mask, col] = df_imputed.loc[missing_mask, col]\n",
    "\n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8aae06-9847-4169-9fd2-df67e8727d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Placeholder Functions for Custom Imputation Models ---\n",
    "\n",
    "def gan_imputer(df):\n",
    "    \"\"\"\n",
    "    Placeholder for GAN-based imputation.\n",
    "    Should return imputed DataFrame.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"GAN imputer not yet implemented.\")\n",
    "\n",
    "def rnn_imputer(df):\n",
    "    \"\"\"\n",
    "    Placeholder for RNN-based imputation.\n",
    "    Should return imputed DataFrame.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"RNN imputer not yet implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef89dd2f-e455-433a-923a-5d1fe18f51b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Registry of Imputation Methods ---\n",
    "imputer_registry = {\n",
    "    \"mean\": SimpleImputer(strategy=\"mean\"),\n",
    "    \"median\": SimpleImputer(strategy=\"median\"),\n",
    "    \"knn\": KNNImputer(n_neighbors=5, weights=\"uniform\"),\n",
    "    \"iterative\": IterativeImputer(estimator=ExtraTreesRegressor(n_estimators=10, random_state=42),\n",
    "                                   max_iter=10, random_state=42),\n",
    "    \"xgboost\": xgboost_imputer,\n",
    "    \"gan\": gan_imputer,\n",
    "    \"lstm\": lstm_imputer,\n",
    "    \"rnn\": rnn_imputer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a2c90-d2f9-4d3b-8732-6d27bdcbcbbc",
   "metadata": {},
   "source": [
    "# CIR-16: Build Core Hierarchical Controller Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d226725-059e-4e89-bafe-e269e768bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_impute_dynamic(\n",
    "    df,\n",
    "    thresholds,\n",
    "    method_names,\n",
    "    method_registry,\n",
    "    random_state=0,\n",
    "    return_method_log=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Dynamic hierarchical imputer using cumulative row-wise missingness and assigned methods.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Dataset with missing values.\n",
    "        thresholds (list): List of group widths (must sum to ~1.0).\n",
    "        method_names (list): List of method names (must match thresholds).\n",
    "        method_registry (dict): Registered methods with keys as names and values as callables or sklearn objects.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "        return_method_log (bool): Return pd.Series logging method used per row.\n",
    "\n",
    "    Returns:\n",
    "        imputed_df (pd.DataFrame)\n",
    "        method_log (pd.Series) — only if return_method_log=True\n",
    "    \"\"\"\n",
    "    if len(thresholds) != len(method_names):\n",
    "        raise ValueError(\"The number of thresholds must match the number of methods.\")\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"missing_pct\"] = df_copy.isnull().mean(axis=1)\n",
    "    cols = df_copy.columns.drop(\"missing_pct\")\n",
    "\n",
    "    global_means = df_copy[cols].mean().fillna(0)\n",
    "    global_min = df_copy[cols].min()\n",
    "    global_max = df_copy[cols].max()\n",
    "\n",
    "    imputed_df = pd.DataFrame(index=df_copy.index, columns=cols)\n",
    "    method_log = pd.Series(index=df_copy.index, dtype=\"object\")\n",
    "\n",
    "    cum_thresholds = np.cumsum(thresholds)\n",
    "    if not np.isclose(cum_thresholds[-1], 1.0):\n",
    "        raise ValueError(\"Thresholds must sum to 1.0\")\n",
    "\n",
    "    previous_imputed = None\n",
    "\n",
    "    for i, upper_bound in enumerate(cum_thresholds):\n",
    "        lower_bound = cum_thresholds[i - 1] if i > 0 else 0.0\n",
    "        idx = df_copy.index[\n",
    "            (df_copy[\"missing_pct\"] > lower_bound) & (df_copy[\"missing_pct\"] <= upper_bound)\n",
    "        ]\n",
    "        group_data = df_copy.loc[idx, cols].copy()\n",
    "\n",
    "        for col in group_data.columns:\n",
    "            if group_data[col].isnull().all():\n",
    "                group_data[col] = global_means[col]\n",
    "\n",
    "        if group_data.empty:\n",
    "            continue\n",
    "\n",
    "        method_name = method_names[i]\n",
    "        logging.info(f\"Group {i+1} ({lower_bound:.2f}, {upper_bound:.2f}] -> {method_name} | {len(group_data)} rows\")\n",
    "\n",
    "        imputer = get_imputer(method_name, method_registry)\n",
    "\n",
    "        if previous_imputed is None:\n",
    "            combined = group_data\n",
    "        else:\n",
    "            combined = pd.concat([previous_imputed, group_data])\n",
    "\n",
    "        try:\n",
    "            if hasattr(imputer, \"fit_transform\"):\n",
    "                combined_imputed = imputer.fit_transform(combined)\n",
    "                combined_imputed = pd.DataFrame(combined_imputed, columns=combined.columns, index=combined.index)\n",
    "            else:\n",
    "                combined_imputed = imputer(combined, random_state=random_state)\n",
    "        except TypeError:\n",
    "            combined_imputed = imputer(combined)\n",
    "\n",
    "        group_imputed = combined_imputed.loc[idx].clip(lower=global_min, upper=global_max, axis=1)\n",
    "\n",
    "        imputed_df.loc[idx] = group_imputed\n",
    "        method_log.loc[idx] = method_name\n",
    "\n",
    "        previous_imputed = pd.concat([previous_imputed, group_imputed]) if previous_imputed is not None else group_imputed.copy()\n",
    "\n",
    "    if imputed_df.isnull().values.any():\n",
    "        raise ValueError(\"NaNs remain after hierarchical imputation!\")\n",
    "\n",
    "    return (imputed_df, method_log) if return_method_log else imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54f71b5a-005b-49de-81a4-bc9159a32981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imputer(method_name, registry):\n",
    "    imputer = registry.get(method_name)\n",
    "    if imputer is None:\n",
    "        raise ValueError(f\"Method '{method_name}' not found or not implemented.\")\n",
    "    if hasattr(imputer, \"fit\") and hasattr(imputer, \"transform\"):\n",
    "        return copy.deepcopy(imputer)\n",
    "    return imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c291b-07c6-4a89-877f-e4d0bfba5dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 01:55:44,758 - INFO - Group 2 (0.04, 0.08] -> lstm | 12 rows\n",
      "2025-05-04 01:55:50,117 - INFO - Group 3 (0.08, 0.12] -> lstm | 120 rows\n",
      "2025-05-04 01:55:56,383 - INFO - Group 4 (0.12, 0.16] -> lstm | 420 rows\n",
      "2025-05-04 01:56:04,338 - INFO - Group 5 (0.16, 0.20] -> lstm | 1584 rows\n",
      "2025-05-04 01:56:20,084 - INFO - Group 6 (0.20, 0.24] -> lstm | 2016 rows\n",
      "2025-05-04 01:56:48,297 - INFO - Group 7 (0.24, 0.28] -> lstm | 2880 rows\n",
      "2025-05-04 01:57:27,183 - INFO - Group 8 (0.28, 0.32] -> lstm | 3276 rows\n",
      "2025-05-04 01:58:18,496 - INFO - Group 9 (0.32, 0.36] -> lstm | 4560 rows\n",
      "2025-05-04 01:59:31,041 - INFO - Group 10 (0.36, 0.40] -> lstm | 3192 rows\n",
      "2025-05-04 02:01:09,579 - INFO - Group 11 (0.40, 0.44] -> xgboost | 3072 rows\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.04] * 25\n",
    "method_names = [\"lstm\"] * 10 + [\"xgboost\"] * 15\n",
    "\n",
    "#method_names = [\"iterative\"] * 3 + [\"lstm\"] * 12 + [\"xgboost\"] * 10\n",
    "\n",
    "imputed_df, method_log = hierarchical_impute_dynamic(\n",
    "    df=o4_X_train, # define the dataset\n",
    "    thresholds=thresholds,\n",
    "    method_names=method_names,\n",
    "    method_registry=imputer_registry,\n",
    "    random_state=0,\n",
    "    return_method_log=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7dede5-8a94-4dd5-8a35-5c9adb2c1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"../CSV/exports/CIR-16/impute/o1_lstm-15_xgboost-10/\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "imputed_df.to_csv(os.path.join(output_path, \"o4_X_train_imputed.csv\"), index=False)\n",
    "method_log.to_csv(os.path.join(output_path, \"o4_X_train_method_log.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24909d4-11a2-41ea-b82a-f464b98820b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Tracking for Visualization ---\n",
    "group_names = []\n",
    "rows_per_group = []\n",
    "cumulative_rows = []\n",
    "\n",
    "cumulative_total = 0\n",
    "\n",
    "# --- Loop through each group ---\n",
    "for i, upper_bound in enumerate(cum_thresholds):\n",
    "    lower_bound = cum_thresholds[i - 1] if i > 0 else 0.0\n",
    "    idx = df_copy.index[\n",
    "        (df_copy[\"missing_pct\"] > lower_bound) & (df_copy[\"missing_pct\"] <= upper_bound)\n",
    "    ]\n",
    "    group_data = df_copy.loc[idx, cols].copy()\n",
    "\n",
    "    group_count = len(group_data)\n",
    "    cumulative_total += group_count\n",
    "\n",
    "    group_names.append(f\"Group {i+1}\\n({lower_bound:.2f}-{upper_bound:.2f})\")\n",
    "    rows_per_group.append(group_count)\n",
    "    cumulative_rows.append(cumulative_total)\n",
    "\n",
    "    # [.. your existing imputation logic goes here ..]\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(cumulative_rows) + 1), cumulative_rows, marker='o', linestyle='-', color='blue')\n",
    "plt.xticks(ticks=range(1, len(group_names) + 1), labels=group_names, rotation=45, ha='right')\n",
    "plt.title(\"Cumulative Rows Used for Imputation Across Groups\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Group (Missingness Range)\", fontsize=12)\n",
    "plt.ylabel(\"Cumulative Rows Used\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "\n",
    "# --- Save the figure ---\n",
    "output_dir = \"figures/CIR-16\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "plt.savefig(os.path.join(output_dir, \"cumulative_imputation_rows.png\"), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7748f-385a-417a-a605-814f2a09008d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
