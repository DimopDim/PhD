{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdebed6-b39e-4d34-b4c3-bda11f83ab2a",
   "metadata": {},
   "source": [
    "# CIR-2: Baseline Imputation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448fce81-0731-4ad1-8f7f-f80619f29358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8893061-af8a-4097-9c7a-e254f73c15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial logger setup\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Global variable to hold the active file handler\n",
    "current_file_handler = None\n",
    "\n",
    "# Create the stream handler\n",
    "stream_handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "def switch_log_file(filename):\n",
    "    global current_file_handler\n",
    "\n",
    "    # If a file handler already exists, remove and close it\n",
    "    if current_file_handler:\n",
    "        logger.removeHandler(current_file_handler)\n",
    "        current_file_handler.close()\n",
    "\n",
    "    # Create a new file handler\n",
    "    current_file_handler = logging.FileHandler(filename)\n",
    "    current_file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(current_file_handler)\n",
    "\n",
    "    logger.info(f\"Switched logging to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bdf90b6-c578-4594-94ce-70255ad091af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 12:14:26,093 - INFO - Switched logging to logs/CIR-2.log\n",
      "2025-05-03 12:14:26,096 - INFO - This is being logged to CIR-2.log\n"
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-2.log')\n",
    "logger.info(\"This is being logged to CIR-2.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6883c12-3115-4f33-a5bc-143d140e7b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 12:14:26,109 - INFO - +++++++++++++++++CIR-2+++++++++++++++++++++++++\n",
      "2025-05-03 12:14:26,111 - INFO - Start Loading Dataframes.\n",
      "2025-05-03 12:14:26,112 - INFO - Loading... -> o1_X_external.csv\n",
      "2025-05-03 12:14:33,391 - INFO - Loading... -> o1_X_test.csv\n",
      "2025-05-03 12:14:33,974 - INFO - Loading... -> o1_X_train.csv\n",
      "2025-05-03 12:14:38,428 - INFO - Loading... -> o1_X_validate.csv\n",
      "2025-05-03 12:14:39,000 - INFO - Loading... -> o1_y_external_los.csv\n",
      "2025-05-03 12:14:39,050 - INFO - Loading... -> o1_y_external_mortality.csv\n",
      "2025-05-03 12:14:39,084 - INFO - Loading... -> o1_y_test_los.csv\n",
      "2025-05-03 12:14:39,096 - INFO - Loading... -> o1_y_test_mortality.csv\n",
      "2025-05-03 12:14:39,101 - INFO - Loading... -> o1_y_train_los.csv\n",
      "2025-05-03 12:14:39,150 - INFO - Loading... -> o1_y_train_mortality.csv\n",
      "2025-05-03 12:14:39,169 - INFO - Loading... -> o1_y_validate_los.csv\n",
      "2025-05-03 12:14:39,180 - INFO - Loading... -> o1_y_validate_mortality.csv\n",
      "2025-05-03 12:14:39,185 - INFO - Loading... -> o2_X_external.csv\n",
      "2025-05-03 12:14:43,100 - INFO - Loading... -> o2_X_test.csv\n",
      "2025-05-03 12:14:43,398 - INFO - Loading... -> o2_X_train.csv\n",
      "2025-05-03 12:14:45,611 - INFO - Loading... -> o2_X_validate.csv\n",
      "2025-05-03 12:14:45,891 - INFO - Loading... -> o2_y_external_los.csv\n",
      "2025-05-03 12:14:45,914 - INFO - Loading... -> o2_y_external_mortality.csv\n",
      "2025-05-03 12:14:45,931 - INFO - Loading... -> o2_y_test_los.csv\n",
      "2025-05-03 12:14:45,938 - INFO - Loading... -> o2_y_test_mortality.csv\n",
      "2025-05-03 12:14:45,942 - INFO - Loading... -> o2_y_train_los.csv\n",
      "2025-05-03 12:14:45,963 - INFO - Loading... -> o2_y_train_mortality.csv\n",
      "2025-05-03 12:14:45,975 - INFO - Loading... -> o2_y_validate_los.csv\n",
      "2025-05-03 12:14:45,981 - INFO - Loading... -> o2_y_validate_mortality.csv\n",
      "2025-05-03 12:14:45,984 - INFO - Loading... -> o3_X_external.csv\n",
      "2025-05-03 12:14:48,420 - INFO - Loading... -> o3_X_test.csv\n",
      "2025-05-03 12:14:48,612 - INFO - Loading... -> o3_X_train.csv\n",
      "2025-05-03 12:14:50,048 - INFO - Loading... -> o3_X_validate.csv\n",
      "2025-05-03 12:14:50,248 - INFO - Loading... -> o3_y_external_los.csv\n",
      "2025-05-03 12:14:50,266 - INFO - Loading... -> o3_y_external_mortality.csv\n",
      "2025-05-03 12:14:50,280 - INFO - Loading... -> o3_y_test_los.csv\n",
      "2025-05-03 12:14:50,285 - INFO - Loading... -> o3_y_test_mortality.csv\n",
      "2025-05-03 12:14:50,288 - INFO - Loading... -> o3_y_train_los.csv\n",
      "2025-05-03 12:14:50,306 - INFO - Loading... -> o3_y_train_mortality.csv\n",
      "2025-05-03 12:14:50,315 - INFO - Loading... -> o3_y_validate_los.csv\n",
      "2025-05-03 12:14:50,320 - INFO - Loading... -> o3_y_validate_mortality.csv\n",
      "2025-05-03 12:14:50,324 - INFO - Loading... -> o4_X_external.csv\n",
      "2025-05-03 12:14:52,191 - INFO - Loading... -> o4_X_test.csv\n",
      "2025-05-03 12:14:52,339 - INFO - Loading... -> o4_X_train.csv\n",
      "2025-05-03 12:14:53,439 - INFO - Loading... -> o4_X_validate.csv\n",
      "2025-05-03 12:14:53,595 - INFO - Loading... -> o4_y_external_los.csv\n",
      "2025-05-03 12:14:53,611 - INFO - Loading... -> o4_y_external_mortality.csv\n",
      "2025-05-03 12:14:53,621 - INFO - Loading... -> o4_y_test_los.csv\n",
      "2025-05-03 12:14:53,625 - INFO - Loading... -> o4_y_test_mortality.csv\n",
      "2025-05-03 12:14:53,629 - INFO - Loading... -> o4_y_train_los.csv\n",
      "2025-05-03 12:14:53,643 - INFO - Loading... -> o4_y_train_mortality.csv\n",
      "2025-05-03 12:14:53,650 - INFO - Loading... -> o4_y_validate_los.csv\n",
      "2025-05-03 12:14:53,654 - INFO - Loading... -> o4_y_validate_mortality.csv\n",
      "2025-05-03 12:14:53,657 - INFO - o1_X_external loaded successfully with shape (234720, 345)\n",
      "2025-05-03 12:14:53,658 - INFO - o1_X_test loaded successfully with shape (15312, 345)\n",
      "2025-05-03 12:14:53,659 - INFO - o1_X_train loaded successfully with shape (122496, 345)\n",
      "2025-05-03 12:14:53,660 - INFO - o1_X_validate loaded successfully with shape (15312, 345)\n",
      "2025-05-03 12:14:53,661 - INFO - o1_y_external_los loaded successfully with shape (234720, 1)\n",
      "2025-05-03 12:14:53,661 - INFO - o1_y_external_mortality loaded successfully with shape (234720, 1)\n",
      "2025-05-03 12:14:53,662 - INFO - o1_y_test_los loaded successfully with shape (15312, 1)\n",
      "2025-05-03 12:14:53,663 - INFO - o1_y_test_mortality loaded successfully with shape (15312, 1)\n",
      "2025-05-03 12:14:53,664 - INFO - o1_y_train_los loaded successfully with shape (122496, 1)\n",
      "2025-05-03 12:14:53,665 - INFO - o1_y_train_mortality loaded successfully with shape (122496, 1)\n",
      "2025-05-03 12:14:53,665 - INFO - o1_y_validate_los loaded successfully with shape (15312, 1)\n",
      "2025-05-03 12:14:53,666 - INFO - o1_y_validate_mortality loaded successfully with shape (15312, 1)\n",
      "2025-05-03 12:14:53,668 - INFO - o2_X_external loaded successfully with shape (117360, 345)\n",
      "2025-05-03 12:14:53,669 - INFO - o2_X_test loaded successfully with shape (7656, 345)\n",
      "2025-05-03 12:14:53,670 - INFO - o2_X_train loaded successfully with shape (61248, 345)\n",
      "2025-05-03 12:14:53,670 - INFO - o2_X_validate loaded successfully with shape (7656, 345)\n",
      "2025-05-03 12:14:53,671 - INFO - o2_y_external_los loaded successfully with shape (117360, 1)\n",
      "2025-05-03 12:14:53,672 - INFO - o2_y_external_mortality loaded successfully with shape (117360, 1)\n",
      "2025-05-03 12:14:53,673 - INFO - o2_y_test_los loaded successfully with shape (7656, 1)\n",
      "2025-05-03 12:14:53,674 - INFO - o2_y_test_mortality loaded successfully with shape (7656, 1)\n",
      "2025-05-03 12:14:53,675 - INFO - o2_y_train_los loaded successfully with shape (61248, 1)\n",
      "2025-05-03 12:14:53,676 - INFO - o2_y_train_mortality loaded successfully with shape (61248, 1)\n",
      "2025-05-03 12:14:53,677 - INFO - o2_y_validate_los loaded successfully with shape (7656, 1)\n",
      "2025-05-03 12:14:53,678 - INFO - o2_y_validate_mortality loaded successfully with shape (7656, 1)\n",
      "2025-05-03 12:14:53,679 - INFO - o3_X_external loaded successfully with shape (78240, 345)\n",
      "2025-05-03 12:14:53,679 - INFO - o3_X_test loaded successfully with shape (5104, 345)\n",
      "2025-05-03 12:14:53,680 - INFO - o3_X_train loaded successfully with shape (40832, 345)\n",
      "2025-05-03 12:14:53,683 - INFO - o3_X_validate loaded successfully with shape (5104, 345)\n",
      "2025-05-03 12:14:53,683 - INFO - o3_y_external_los loaded successfully with shape (78240, 1)\n",
      "2025-05-03 12:14:53,684 - INFO - o3_y_external_mortality loaded successfully with shape (78240, 1)\n",
      "2025-05-03 12:14:53,685 - INFO - o3_y_test_los loaded successfully with shape (5104, 1)\n",
      "2025-05-03 12:14:53,686 - INFO - o3_y_test_mortality loaded successfully with shape (5104, 1)\n",
      "2025-05-03 12:14:53,687 - INFO - o3_y_train_los loaded successfully with shape (40832, 1)\n",
      "2025-05-03 12:14:53,688 - INFO - o3_y_train_mortality loaded successfully with shape (40832, 1)\n",
      "2025-05-03 12:14:53,689 - INFO - o3_y_validate_los loaded successfully with shape (5104, 1)\n",
      "2025-05-03 12:14:53,690 - INFO - o3_y_validate_mortality loaded successfully with shape (5104, 1)\n",
      "2025-05-03 12:14:53,691 - INFO - o4_X_external loaded successfully with shape (58680, 345)\n",
      "2025-05-03 12:14:53,692 - INFO - o4_X_test loaded successfully with shape (3828, 345)\n",
      "2025-05-03 12:14:53,692 - INFO - o4_X_train loaded successfully with shape (30624, 345)\n",
      "2025-05-03 12:14:53,693 - INFO - o4_X_validate loaded successfully with shape (3828, 345)\n",
      "2025-05-03 12:14:53,694 - INFO - o4_y_external_los loaded successfully with shape (58680, 1)\n",
      "2025-05-03 12:14:53,694 - INFO - o4_y_external_mortality loaded successfully with shape (58680, 1)\n",
      "2025-05-03 12:14:53,695 - INFO - o4_y_test_los loaded successfully with shape (3828, 1)\n",
      "2025-05-03 12:14:53,696 - INFO - o4_y_test_mortality loaded successfully with shape (3828, 1)\n",
      "2025-05-03 12:14:53,698 - INFO - o4_y_train_los loaded successfully with shape (30624, 1)\n",
      "2025-05-03 12:14:53,699 - INFO - o4_y_train_mortality loaded successfully with shape (30624, 1)\n",
      "2025-05-03 12:14:53,700 - INFO - o4_y_validate_los loaded successfully with shape (3828, 1)\n",
      "2025-05-03 12:14:53,700 - INFO - o4_y_validate_mortality loaded successfully with shape (3828, 1)\n",
      "2025-05-03 12:14:53,701 - INFO - Load Complete.\n",
      "2025-05-03 12:14:53,702 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "data_path = \"../04_ANN/CSV/exports/split_set/without_multiple_rows\"\n",
    "all_files = os.listdir(data_path)\n",
    "\n",
    "logging.info(\"+++++++++++++++++CIR-2+++++++++++++++++++++++++\")\n",
    "logging.info(\"Start Loading Dataframes.\")\n",
    "\n",
    "# Load CSVs into a dictionary of dataframes\n",
    "dataframes = {}\n",
    "for file in all_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        var_name = file.replace(\".csv\", \"\").replace(\"-\", \"_\")\n",
    "        logging.info(f\"Loading... -> {file}\")\n",
    "        dataframes[var_name] = pd.read_csv(os.path.join(data_path, file)).astype('float32')\n",
    "\n",
    "# Log loaded datasets\n",
    "for var_name, df in dataframes.items():\n",
    "    globals()[var_name] = df\n",
    "    logging.info(f\"{var_name} loaded successfully with shape {df.shape}\")\n",
    "logging.info(\"Load Complete.\")\n",
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53cf697-daa1-4e6e-b0a9-868bd8a5dad0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CIR-23: Implement Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da711f0-95b4-451c-9cf9-a64e59f55617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tee class to duplicate stdout\n",
    "class Tee:\n",
    "    def __init__(self, *files):\n",
    "        self.files = files\n",
    "    def write(self, obj):\n",
    "        for f in self.files:\n",
    "            f.write(obj)\n",
    "            f.flush()\n",
    "    def flush(self):\n",
    "        for f in self.files:\n",
    "            f.flush()\n",
    "\n",
    "def impute_with_iterative(input_df, method, output_path, n_iter, log_verbose_file_path=None):\n",
    "    logging.info(f\"Starting Iterative Imputer with method={method} on input DataFrame of shape {input_df.shape}.\")\n",
    "\n",
    "    data_copy = input_df.copy()\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Estimator selection\n",
    "    if method == \"ExtraTrees\":\n",
    "        estimator = ExtraTreesRegressor(n_estimators=10, random_state=0, n_jobs=-1)\n",
    "    elif method == \"HistGradientBoosting\":\n",
    "        estimator = HistGradientBoostingRegressor(random_state=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}. Use 'ExtraTrees', or 'HistGradientBoosting'.\")\n",
    "\n",
    "    # IterativeImputer\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=estimator,\n",
    "        max_iter=n_iter,\n",
    "        random_state=0,\n",
    "        verbose=2,\n",
    "        sample_posterior=False\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Handle logging duplication\n",
    "    if log_verbose_file_path is not None:\n",
    "        log_dir = os.path.dirname(log_verbose_file_path)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "        original_stdout = sys.stdout  # Save original stdout\n",
    "        with open(log_verbose_file_path, \"w\") as log_file:\n",
    "            sys.stdout = Tee(sys.__stdout__, log_file)\n",
    "\n",
    "            try:\n",
    "                # Fit and transform\n",
    "                imputed_array = imputer.fit_transform(data_copy)\n",
    "            finally:\n",
    "                sys.stdout = original_stdout  # Restore stdout\n",
    "    else:\n",
    "        # If no logging redirection\n",
    "        imputed_array = imputer.fit_transform(data_copy)\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    # Save imputed data\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=data_copy.columns)\n",
    "    imputed_df.to_csv(output_path, index=False)\n",
    "\n",
    "    # Logging\n",
    "    logging.info(f\"Imputation completed in {runtime:.2f} seconds.\")\n",
    "    nan_count = np.isnan(imputed_df.values).sum()\n",
    "    logging.info(f\"Number of NaNs after imputation: {nan_count}\")\n",
    "    logging.info(f\"Imputed dataset saved at {output_path}\")\n",
    "    logging.info(f\"Basic statistics after imputation:\\n{imputed_df.describe()}\")\n",
    "\n",
    "    # Save full describe output to a separate CSV file\n",
    "    # The logging.info could not represent all the statistics\n",
    "    describe_output_path = output_path.replace(\".csv\", \"_describe.csv\")\n",
    "    imputed_df.describe().to_csv(describe_output_path)\n",
    "    logging.info(f\"Basic statistics saved at {describe_output_path}\")\n",
    "\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862069df-0de8-46a8-928f-a5f7e9c740cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create log file\n",
    "switch_log_file('logs/CIR-23_ExtraTrees.log')\n",
    "logger.info(\"This is being logged to CIR-23_ExtraTrees.log\")\n",
    "\n",
    "# ExtraTrees estimator\n",
    "impute_with_iterative(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o4_X_train_Iterative_ExtraTrees.csv\",\n",
    "    method=\"ExtraTrees\",\n",
    "    n_iter=3,\n",
    "    log_verbose_file_path=\"logs/CIR-23_ExtraTrees.log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375708b2-e3d1-46fa-b09b-5aae7fbd541f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create log file\n",
    "switch_log_file('logs/CIR-23_HistGradientBoosting.log')\n",
    "logger.info(\"This is being logged to CIR-23_HistGradientBoosting.log\")\n",
    "\n",
    "# HistGradientBoosting estimator\n",
    "impute_with_iterative(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o4_X_train_Iterative_HistGradientBoosting.csv\",\n",
    "    method=\"HistGradientBoosting\",\n",
    "    n_iter=20,\n",
    "    log_verbose_file_path=\"logs/CIR-23_HistGradientBoosting.log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559bc1d-0c9d-4d88-a66e-2fb65c657564",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CIR-24 Implement Knn Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478cbfd2-4332-4b48-8583-a7107c75359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-24.log')\n",
    "logger.info(\"This is being logged to CIR-24.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a615dd-e0af-435a-b4a0-1dcc85c9ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_knn(input_df, output_path, n_neighbors=5, weights=\"uniform\"):\n",
    "\n",
    "    logging.info(f\"Starting KNN Imputer on input DataFrame of shape {input_df.shape}.\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    data_copy = input_df.copy()\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create and apply KNNImputer\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors, weights=weights)\n",
    "    imputed_array = imputer.fit_transform(data_copy)\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    # Convert to DataFrame and cast to float32\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=data_copy.columns).astype(np.float32)\n",
    "    imputed_df.to_csv(output_path, index=False)\n",
    "\n",
    "    # Logging\n",
    "    logging.info(f\"KNN Imputation completed in {runtime:.2f} seconds.\")\n",
    "    nan_count = np.isnan(imputed_df.values).sum()\n",
    "    logging.info(f\"Number of NaNs after imputation: {nan_count}\")\n",
    "    logging.info(f\"Imputed dataset saved at {output_path}\")\n",
    "\n",
    "    # Save .describe() summary\n",
    "    describe_output_path = output_path.replace(\".csv\", \"_describe.csv\")\n",
    "    imputed_df.describe().to_csv(describe_output_path)\n",
    "    logging.info(f\"Basic statistics saved at {describe_output_path}\")\n",
    "\n",
    "    return imputed_df\n",
    "\n",
    "# Run KNN Imputer\n",
    "o4_X_train_knn_imputed = impute_with_knn(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/02_knn/o4_X_train_KNN_distance.csv\",\n",
    "    n_neighbors=5,\n",
    "    weights=\"distance\"  # \"uniform\" or \"distance\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc0ecf5-4cdb-428f-bffe-9a2c5ec1abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KNN Imputer\n",
    "o4_X_train_knn_imputed = impute_with_knn(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/02_knn/o4_X_train_KNN_distance.csv\",\n",
    "    n_neighbors=5,\n",
    "    weights=\"distance\"  # \"uniform\" or \"distance\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd301a-613a-4220-a4db-80956ebdf998",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CIR-25: Implement Mean - Median Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d009ec-b7b3-4fc8-9994-b80fb11d3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-25.log')\n",
    "logger.info(\"This is being logged to CIR-25.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c6cf6-0746-4a85-a013-d92b089ad03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_simple(input_df, strategy, output_path):\n",
    "\n",
    "    logging.info(f\"Starting {strategy.capitalize()} Imputer on input DataFrame of shape {input_df.shape}.\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    data_copy = input_df.copy()\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    imputed_array = imputer.fit_transform(data_copy)\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=data_copy.columns).astype(np.float32)\n",
    "    imputed_df.to_csv(output_path, index=False)\n",
    "\n",
    "    logging.info(f\"{strategy.capitalize()} Imputation completed in {runtime:.2f} seconds.\")\n",
    "    logging.info(f\"Number of NaNs after imputation: {np.isnan(imputed_df.values).sum()}\")\n",
    "    logging.info(f\"Imputed dataset saved at {output_path}\")\n",
    "\n",
    "    # Save statistics\n",
    "    describe_output_path = output_path.replace(\".csv\", \"_describe.csv\")\n",
    "    imputed_df.describe().to_csv(describe_output_path)\n",
    "    logging.info(f\"Basic statistics saved at {describe_output_path}\")\n",
    "\n",
    "    return imputed_df, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1af88-9619-48c0-a0c0-ed5d981a7075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Imputation\n",
    "mean_imputed, runtime_mean = impute_with_simple(\n",
    "    input_df=o4_X_train,\n",
    "    strategy=\"mean\",\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/03_mean/o4_X_train_Mean.csv\"\n",
    ")\n",
    "\n",
    "# Median Imputation\n",
    "median_imputed, runtime_median = impute_with_simple(\n",
    "    input_df=o4_X_train,\n",
    "    strategy=\"median\",\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/04_median/o4_X_train_Median.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258f36a-aaea-4318-8dac-f79500b7641c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Histogram Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3cdae-d76e-4378-87c6-24a8adec3581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-24.log')\n",
    "logger.info(\"This is being logged to CIR-24.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f7d4e-16ff-4c9d-aa90-efa3ba076a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(name):\n",
    "    # Replace characters that are not safe in filenames\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name)\n",
    "\n",
    "def plot_before_after_distributions(input_df_before, input_df_after, output_folder, sample_features=None):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    features = list(input_df_before.columns)\n",
    "\n",
    "    # If sample_features is None ➔ use all features\n",
    "    if sample_features is None:\n",
    "        sampled_features = features\n",
    "    else:\n",
    "        sampled_features = random.sample(features, min(sample_features, len(features)))\n",
    "\n",
    "    logging.info(f\"Plotting distributions for features: {sampled_features}\")\n",
    "\n",
    "    for feature in sampled_features:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(input_df_before[feature].dropna(), bins=50, alpha=0.5, label='Before Imputation')\n",
    "        plt.hist(input_df_after[feature].dropna(), bins=50, alpha=0.5, label='After Imputation')\n",
    "\n",
    "        plt.title(f'Distribution of {feature}')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # ✅ Use sanitized feature name for the filename\n",
    "        safe_feature_name = sanitize_filename(feature)\n",
    "        plot_path = os.path.join(output_folder, f\"{safe_feature_name}_distribution_comparison.png\")\n",
    "\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "\n",
    "    logging.info(f\"Distribution plots saved in {output_folder}\")\n",
    "\n",
    "\n",
    "def check_extreme_values(input_df_before, input_df_after, threshold=5.0):\n",
    "    \"\"\"\n",
    "    Check for extreme outliers introduced after imputation.\n",
    "    \"\"\"\n",
    "    suspicious_features = []\n",
    "\n",
    "    for feature in input_df_before.columns:\n",
    "        before_max = input_df_before[feature].max()\n",
    "        before_min = input_df_before[feature].min()\n",
    "        after_max = input_df_after[feature].max()\n",
    "        after_min = input_df_after[feature].min()\n",
    "\n",
    "        if before_max != 0 and (after_max > threshold * before_max or after_max < before_max / threshold):\n",
    "            suspicious_features.append((feature, 'max', before_max, after_max))\n",
    "        if before_min != 0 and (after_min < before_min / threshold or after_min > threshold * before_min):\n",
    "            suspicious_features.append((feature, 'min', before_min, after_min))\n",
    "\n",
    "    suspicious_df = pd.DataFrame(\n",
    "        suspicious_features, \n",
    "        columns=[\"Feature\", \"Type\", \"Before_Value\", \"After_Value\"]\n",
    "    )\n",
    "\n",
    "    if not suspicious_df.empty:\n",
    "        logging.warning(f\"Found {len(suspicious_df)} suspicious extreme values after imputation!\")\n",
    "\n",
    "    return suspicious_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f9e35-ed9a-40b4-9f1c-cfc039c9c6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# ExtraTrees\n",
    "#switch_log_file('logs/CIR-27.log')\n",
    "#logger.info(\"This is being logged to CIR-24.log\")\n",
    "#data_imputed = pd.read_csv(f\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o4_X_train_Iterative_ExtraTrees.csv\")\n",
    "#method_imputed = \"ExtraTrees\"\n",
    "#dataset = o4_X_train\n",
    "#dataset_name = \"o4_X_train\"\n",
    "\n",
    "# HistGradientBoosting\n",
    "#switch_log_file('logs/CIR-27.log')\n",
    "#logger.info(\"This is being logged to CIR-24.log\")\n",
    "#data_imputed = pd.read_csv(f\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o4_X_train_Iterative_HistGradientBoosting.csv\")\n",
    "#method_imputed = \"HistGradientBoosting\"\n",
    "#dataset = o4_X_train\n",
    "#dataset_name = \"o4_X_train\"\n",
    "\n",
    "# knn\n",
    "#switch_log_file('logs/CIR-24.log')\n",
    "#logger.info(\"This is being logged to CIR-24.log\")\n",
    "#data_imputed = pd.read_csv(f\"CSV/exports/CRI-02/o1_impute_baselines/02_knn/o4_X_train_KNN_distance.csv\")\n",
    "#method_imputed = \"knn_distance\"\n",
    "#dataset = o4_X_train\n",
    "#dataset_name = \"o4_X_train\"\n",
    "\n",
    "# mean\n",
    "#switch_log_file('logs/CIR-25.log')\n",
    "#logger.info(\"This is being logged to CIR-25.log\")\n",
    "#data_imputed = pd.read_csv(f\"CSV/exports/CRI-02/o1_impute_baselines/03_mean/o4_X_train_Mean.csv\")\n",
    "#method_imputed = \"Mean\"\n",
    "#dataset = o4_X_train\n",
    "#dataset_name = \"o4_X_train\"\n",
    "\n",
    "# Median\n",
    "switch_log_file('logs/CIR-25.log')\n",
    "logger.info(\"This is being logged to CIR-25.log\")\n",
    "data_imputed = pd.read_csv(f\"CSV/exports/CRI-02/o1_impute_baselines/04_median/o4_X_train_Median.csv\")\n",
    "method_imputed = \"Median\"\n",
    "dataset = o4_X_train\n",
    "dataset_name = \"o4_X_train\"\n",
    "\n",
    "# Plot distributions before vs after\n",
    "plot_before_after_distributions(\n",
    "    input_df_before=dataset,\n",
    "    input_df_after=data_imputed,\n",
    "    output_folder=f\"figures/CRI-27/o1_impute_baselines/01_iterative/{method_imputed}/\",\n",
    ")\n",
    "\n",
    "# Check for extreme values\n",
    "extreme_values_df = check_extreme_values(\n",
    "    input_df_before=dataset,\n",
    "    input_df_after=data_imputed,\n",
    "    threshold=5.0\n",
    ")\n",
    "\n",
    "# Save report if needed\n",
    "\n",
    "output_folder = \"CSV/exports/CRI-02/00_extreme_values/\"\n",
    "os.makedirs(output_folder, exist_ok=True) \n",
    "\n",
    "if not extreme_values_df.empty:\n",
    "    output_path = os.path.join(output_folder, f\"{dataset_name}_{method_imputed}_extreme_values.csv\")\n",
    "    extreme_values_df.to_csv(output_path, index=False)\n",
    "    logging.info(\"Extreme values report saved.\")\n",
    "else:\n",
    "    logging.info(\"No suspicious extreme values detected.\")\n",
    "\n",
    "logging.info(\"+++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56fde7-c52d-4e5d-b2a3-a13b2da81b40",
   "metadata": {},
   "source": [
    "# Comparison\n",
    "## Compaire imputation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8a62c12-38b2-4051-890f-19ba23fc8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_path = os.path.join(\"CSV\", \"exports\", \"CRI-02\", \"o1_impute_baselines\", \"03_mean\", \"o4_X_train_Mean.csv\")\n",
    "median_path = os.path.join(\"CSV\", \"exports\", \"CRI-02\", \"o1_impute_baselines\", \"04_median\", \"o4_X_train_Median.csv\")\n",
    "knn_distance = os.path.join(\"CSV\", \"exports\", \"CRI-02\", \"o1_impute_baselines\", \"02_knn\", \"o4_X_train_KNN_distance.csv\")\n",
    "knn_uniform = os.path.join(\"CSV\", \"exports\", \"CRI-02\", \"o1_impute_baselines\", \"02_knn\", \"o4_X_train_KNN_distance.csv\")\n",
    "iter_extratrees_path = os.path.join(\"CSV\", \"exports\", \"CRI-02\", \"o1_impute_baselines\", \"01_iterative\", \"o4_X_train_Iterative_ExtraTrees.csv\")\n",
    "iter_hgb_path = os.path.join(\"CSV\", \"exports\", \"CRI-02\", \"o1_impute_baselines\", \"01_iterative\", \"o4_X_train_Iterative_HistGradientBoosting.csv\")\n",
    "\n",
    "# Load original and imputed datasets\n",
    "#o4_X_train = pd.read_csv(mean_path).astype(np.float32)\n",
    "o4_X_train_Mean = pd.read_csv(mean_path).astype(np.float32)\n",
    "o4_X_train_Median = pd.read_csv(median_path).astype(np.float32)\n",
    "o4_X_train_KNN_distance = pd.read_csv(knn_distance).astype(np.float32)\n",
    "o4_X_train_KNN_uniform = pd.read_csv(knn_uniform).astype(np.float32)\n",
    "o4_X_train_Iterative_ExtraTrees = pd.read_csv(iter_extratrees_path).astype(np.float32)\n",
    "o4_X_train_Iterative_HistGradientBoosting = pd.read_csv(iter_hgb_path).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bfecee0-7ba0-448f-b8a8-b57b4ba78611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_imputation_results(original_df, imputed_df, method_name, extreme_values_path):\n",
    "    # Detect where original data had NaNs\n",
    "    mask_missing = original_df.isna()\n",
    "\n",
    "    # Calculate difference only at missing locations\n",
    "    diff = np.abs(imputed_df - original_df)\n",
    "    mean_change = diff[mask_missing].mean().mean()\n",
    "\n",
    "    # Count remaining NaNs\n",
    "    nan_count = np.isnan(imputed_df.values).sum()\n",
    "\n",
    "    # Load extreme values file and count\n",
    "    if os.path.exists(extreme_values_path):\n",
    "        extreme_df = pd.read_csv(extreme_values_path)\n",
    "        n_extreme = len(extreme_df)\n",
    "    else:\n",
    "        n_extreme = 0\n",
    "        logging.warning(f\"Extreme values file not found for method {method_name}: {extreme_values_path}\")\n",
    "\n",
    "    return {\n",
    "        \"method\": method_name,\n",
    "        \"nan_count\": nan_count,\n",
    "        \"mean_change\": round(mean_change, 4),\n",
    "        \"n_extreme_values\": n_extreme\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cb2c875-ed40-40e5-92c2-ea98c8620883",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "base_extreme_dir = os.path.join(\"CSV\", \"exports\", \"CRI-02\", \"00_extreme_values\")\n",
    "\n",
    "results.append(summarize_imputation_results(\n",
    "    o4_X_train, o4_X_train_Mean, \"Mean\",\n",
    "    os.path.join(base_extreme_dir, \"o4_X_train_Mean_extreme_values.csv\")\n",
    "))\n",
    "\n",
    "results.append(summarize_imputation_results(\n",
    "    o4_X_train, o4_X_train_Median, \"Median\",\n",
    "    os.path.join(base_extreme_dir, \"o4_X_train_Median_extreme_values.csv\")\n",
    "))\n",
    "\n",
    "results.append(summarize_imputation_results(\n",
    "    o4_X_train, o4_X_train_KNN_distance, \"KNN (distance)\",\n",
    "    os.path.join(base_extreme_dir, \"o4_X_train_knn_distance_extreme_values.csv\")\n",
    "))\n",
    "\n",
    "results.append(summarize_imputation_results(\n",
    "    o4_X_train, o4_X_train_KNN_uniform, \"KNN (uniform)\",\n",
    "    os.path.join(base_extreme_dir, \"o4_X_train_knn_uniform_extreme_values.csv\")\n",
    "))\n",
    "\n",
    "results.append(summarize_imputation_results(\n",
    "    o4_X_train, o4_X_train_Iterative_ExtraTrees, \"Iterative (ExtraTrees)\",\n",
    "    os.path.join(base_extreme_dir, \"o4_X_train_ExtraTrees_extreme_values.csv\")\n",
    "))\n",
    "\n",
    "results.append(summarize_imputation_results(\n",
    "    o4_X_train, o4_X_train_Iterative_HistGradientBoosting, \"Iterative (HistGradientBoosting)\",\n",
    "    os.path.join(base_extreme_dir, \"o4_X_train_HistGradientBoosting_extreme_values.csv\")\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71adb497-ff30-41c8-a201-fc17f323e16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>nan_count</th>\n",
       "      <th>mean_change</th>\n",
       "      <th>n_extreme_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Median</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN (distance)</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN (uniform)</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iterative (ExtraTrees)</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Iterative (HistGradientBoosting)</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             method  nan_count  mean_change  n_extreme_values\n",
       "0                              Mean          0          NaN                17\n",
       "1                            Median          0          NaN                17\n",
       "2                    KNN (distance)          0          NaN                17\n",
       "3                     KNN (uniform)          0          NaN                17\n",
       "4            Iterative (ExtraTrees)          0          NaN                17\n",
       "5  Iterative (HistGradientBoosting)          0          NaN                47"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the list of dictionaries into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by mean change (optional)\n",
    "results_df = results_df.sort_values(by=\"mean_change\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Display\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec5a4ea-9961-4b01-a487-3682c0e95d9a",
   "metadata": {},
   "source": [
    "# Testing Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be49b3-1296-4d56-8929-759a26e94a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small subset of the dataframe (for faster testing)\n",
    "small_data = o4_X_train.iloc[:450, :]  # pick first 50 features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
