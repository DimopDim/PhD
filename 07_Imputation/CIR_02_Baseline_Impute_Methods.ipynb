{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdebed6-b39e-4d34-b4c3-bda11f83ab2a",
   "metadata": {},
   "source": [
    "# CIR-2 Baseline Imputation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448fce81-0731-4ad1-8f7f-f80619f29358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8893061-af8a-4097-9c7a-e254f73c15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial logger setup\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Global variable to hold the active file handler\n",
    "current_file_handler = None\n",
    "\n",
    "# Create the stream handler (to console)\n",
    "stream_handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "def switch_log_file(filename):\n",
    "    global current_file_handler\n",
    "\n",
    "    # If a file handler already exists, remove and close it\n",
    "    if current_file_handler:\n",
    "        logger.removeHandler(current_file_handler)\n",
    "        current_file_handler.close()\n",
    "\n",
    "    # Create a new file handler\n",
    "    current_file_handler = logging.FileHandler(filename)\n",
    "    current_file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(current_file_handler)\n",
    "\n",
    "    logger.info(f\"Switched logging to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bdf90b6-c578-4594-94ce-70255ad091af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 21:46:53,575 - INFO - Switched logging to logs/CIR-2.log\n",
      "2025-04-28 21:46:53,578 - INFO - This is being logged to CIR-2.log\n"
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-2.log')\n",
    "logger.info(\"This is being logged to CIR-2.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6883c12-3115-4f33-a5bc-143d140e7b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 21:46:53,592 - INFO - +++++++++++++++++CIR-2+++++++++++++++++++++++++\n",
      "2025-04-28 21:46:53,593 - INFO - Start Loading Dataframes.\n",
      "2025-04-28 21:46:53,594 - INFO - Loading... -> o1_X_external.csv\n",
      "2025-04-28 21:47:00,730 - INFO - Loading... -> o1_X_test.csv\n",
      "2025-04-28 21:47:01,299 - INFO - Loading... -> o1_X_train.csv\n",
      "2025-04-28 21:47:05,432 - INFO - Loading... -> o1_X_validate.csv\n",
      "2025-04-28 21:47:05,969 - INFO - Loading... -> o1_y_external_los.csv\n",
      "2025-04-28 21:47:06,014 - INFO - Loading... -> o1_y_external_mortality.csv\n",
      "2025-04-28 21:47:06,043 - INFO - Loading... -> o1_y_test_los.csv\n",
      "2025-04-28 21:47:06,053 - INFO - Loading... -> o1_y_test_mortality.csv\n",
      "2025-04-28 21:47:06,058 - INFO - Loading... -> o1_y_train_los.csv\n",
      "2025-04-28 21:47:06,096 - INFO - Loading... -> o1_y_train_mortality.csv\n",
      "2025-04-28 21:47:06,113 - INFO - Loading... -> o1_y_validate_los.csv\n",
      "2025-04-28 21:47:06,124 - INFO - Loading... -> o1_y_validate_mortality.csv\n",
      "2025-04-28 21:47:06,128 - INFO - Loading... -> o2_X_external.csv\n",
      "2025-04-28 21:47:09,801 - INFO - Loading... -> o2_X_test.csv\n",
      "2025-04-28 21:47:10,156 - INFO - Loading... -> o2_X_train.csv\n",
      "2025-04-28 21:47:12,328 - INFO - Loading... -> o2_X_validate.csv\n",
      "2025-04-28 21:47:12,608 - INFO - Loading... -> o2_y_external_los.csv\n",
      "2025-04-28 21:47:12,630 - INFO - Loading... -> o2_y_external_mortality.csv\n",
      "2025-04-28 21:47:12,647 - INFO - Loading... -> o2_y_test_los.csv\n",
      "2025-04-28 21:47:12,653 - INFO - Loading... -> o2_y_test_mortality.csv\n",
      "2025-04-28 21:47:12,657 - INFO - Loading... -> o2_y_train_los.csv\n",
      "2025-04-28 21:47:12,678 - INFO - Loading... -> o2_y_train_mortality.csv\n",
      "2025-04-28 21:47:12,688 - INFO - Loading... -> o2_y_validate_los.csv\n",
      "2025-04-28 21:47:12,694 - INFO - Loading... -> o2_y_validate_mortality.csv\n",
      "2025-04-28 21:47:12,697 - INFO - Loading... -> o3_X_external.csv\n",
      "2025-04-28 21:47:15,053 - INFO - Loading... -> o3_X_test.csv\n",
      "2025-04-28 21:47:15,243 - INFO - Loading... -> o3_X_train.csv\n",
      "2025-04-28 21:47:16,577 - INFO - Loading... -> o3_X_validate.csv\n",
      "2025-04-28 21:47:16,779 - INFO - Loading... -> o3_y_external_los.csv\n",
      "2025-04-28 21:47:16,795 - INFO - Loading... -> o3_y_external_mortality.csv\n",
      "2025-04-28 21:47:16,809 - INFO - Loading... -> o3_y_test_los.csv\n",
      "2025-04-28 21:47:16,813 - INFO - Loading... -> o3_y_test_mortality.csv\n",
      "2025-04-28 21:47:16,817 - INFO - Loading... -> o3_y_train_los.csv\n",
      "2025-04-28 21:47:16,832 - INFO - Loading... -> o3_y_train_mortality.csv\n",
      "2025-04-28 21:47:16,839 - INFO - Loading... -> o3_y_validate_los.csv\n",
      "2025-04-28 21:47:16,844 - INFO - Loading... -> o3_y_validate_mortality.csv\n",
      "2025-04-28 21:47:16,848 - INFO - Loading... -> o4_X_external.csv\n",
      "2025-04-28 21:47:18,661 - INFO - Loading... -> o4_X_test.csv\n",
      "2025-04-28 21:47:18,814 - INFO - Loading... -> o4_X_train.csv\n",
      "2025-04-28 21:47:19,862 - INFO - Loading... -> o4_X_validate.csv\n",
      "2025-04-28 21:47:20,018 - INFO - Loading... -> o4_y_external_los.csv\n",
      "2025-04-28 21:47:20,032 - INFO - Loading... -> o4_y_external_mortality.csv\n",
      "2025-04-28 21:47:20,041 - INFO - Loading... -> o4_y_test_los.csv\n",
      "2025-04-28 21:47:20,045 - INFO - Loading... -> o4_y_test_mortality.csv\n",
      "2025-04-28 21:47:20,048 - INFO - Loading... -> o4_y_train_los.csv\n",
      "2025-04-28 21:47:20,061 - INFO - Loading... -> o4_y_train_mortality.csv\n",
      "2025-04-28 21:47:20,068 - INFO - Loading... -> o4_y_validate_los.csv\n",
      "2025-04-28 21:47:20,072 - INFO - Loading... -> o4_y_validate_mortality.csv\n",
      "2025-04-28 21:47:20,075 - INFO - o1_X_external loaded successfully with shape (234720, 345)\n",
      "2025-04-28 21:47:20,076 - INFO - o1_X_test loaded successfully with shape (15312, 345)\n",
      "2025-04-28 21:47:20,077 - INFO - o1_X_train loaded successfully with shape (122496, 345)\n",
      "2025-04-28 21:47:20,079 - INFO - o1_X_validate loaded successfully with shape (15312, 345)\n",
      "2025-04-28 21:47:20,080 - INFO - o1_y_external_los loaded successfully with shape (234720, 1)\n",
      "2025-04-28 21:47:20,080 - INFO - o1_y_external_mortality loaded successfully with shape (234720, 1)\n",
      "2025-04-28 21:47:20,082 - INFO - o1_y_test_los loaded successfully with shape (15312, 1)\n",
      "2025-04-28 21:47:20,083 - INFO - o1_y_test_mortality loaded successfully with shape (15312, 1)\n",
      "2025-04-28 21:47:20,084 - INFO - o1_y_train_los loaded successfully with shape (122496, 1)\n",
      "2025-04-28 21:47:20,085 - INFO - o1_y_train_mortality loaded successfully with shape (122496, 1)\n",
      "2025-04-28 21:47:20,086 - INFO - o1_y_validate_los loaded successfully with shape (15312, 1)\n",
      "2025-04-28 21:47:20,087 - INFO - o1_y_validate_mortality loaded successfully with shape (15312, 1)\n",
      "2025-04-28 21:47:20,088 - INFO - o2_X_external loaded successfully with shape (117360, 345)\n",
      "2025-04-28 21:47:20,089 - INFO - o2_X_test loaded successfully with shape (7656, 345)\n",
      "2025-04-28 21:47:20,090 - INFO - o2_X_train loaded successfully with shape (61248, 345)\n",
      "2025-04-28 21:47:20,090 - INFO - o2_X_validate loaded successfully with shape (7656, 345)\n",
      "2025-04-28 21:47:20,091 - INFO - o2_y_external_los loaded successfully with shape (117360, 1)\n",
      "2025-04-28 21:47:20,092 - INFO - o2_y_external_mortality loaded successfully with shape (117360, 1)\n",
      "2025-04-28 21:47:20,093 - INFO - o2_y_test_los loaded successfully with shape (7656, 1)\n",
      "2025-04-28 21:47:20,094 - INFO - o2_y_test_mortality loaded successfully with shape (7656, 1)\n",
      "2025-04-28 21:47:20,095 - INFO - o2_y_train_los loaded successfully with shape (61248, 1)\n",
      "2025-04-28 21:47:20,096 - INFO - o2_y_train_mortality loaded successfully with shape (61248, 1)\n",
      "2025-04-28 21:47:20,097 - INFO - o2_y_validate_los loaded successfully with shape (7656, 1)\n",
      "2025-04-28 21:47:20,097 - INFO - o2_y_validate_mortality loaded successfully with shape (7656, 1)\n",
      "2025-04-28 21:47:20,098 - INFO - o3_X_external loaded successfully with shape (78240, 345)\n",
      "2025-04-28 21:47:20,099 - INFO - o3_X_test loaded successfully with shape (5104, 345)\n",
      "2025-04-28 21:47:20,100 - INFO - o3_X_train loaded successfully with shape (40832, 345)\n",
      "2025-04-28 21:47:20,101 - INFO - o3_X_validate loaded successfully with shape (5104, 345)\n",
      "2025-04-28 21:47:20,102 - INFO - o3_y_external_los loaded successfully with shape (78240, 1)\n",
      "2025-04-28 21:47:20,103 - INFO - o3_y_external_mortality loaded successfully with shape (78240, 1)\n",
      "2025-04-28 21:47:20,104 - INFO - o3_y_test_los loaded successfully with shape (5104, 1)\n",
      "2025-04-28 21:47:20,105 - INFO - o3_y_test_mortality loaded successfully with shape (5104, 1)\n",
      "2025-04-28 21:47:20,106 - INFO - o3_y_train_los loaded successfully with shape (40832, 1)\n",
      "2025-04-28 21:47:20,106 - INFO - o3_y_train_mortality loaded successfully with shape (40832, 1)\n",
      "2025-04-28 21:47:20,107 - INFO - o3_y_validate_los loaded successfully with shape (5104, 1)\n",
      "2025-04-28 21:47:20,108 - INFO - o3_y_validate_mortality loaded successfully with shape (5104, 1)\n",
      "2025-04-28 21:47:20,109 - INFO - o4_X_external loaded successfully with shape (58680, 345)\n",
      "2025-04-28 21:47:20,110 - INFO - o4_X_test loaded successfully with shape (3828, 345)\n",
      "2025-04-28 21:47:20,110 - INFO - o4_X_train loaded successfully with shape (30624, 345)\n",
      "2025-04-28 21:47:20,111 - INFO - o4_X_validate loaded successfully with shape (3828, 345)\n",
      "2025-04-28 21:47:20,112 - INFO - o4_y_external_los loaded successfully with shape (58680, 1)\n",
      "2025-04-28 21:47:20,113 - INFO - o4_y_external_mortality loaded successfully with shape (58680, 1)\n",
      "2025-04-28 21:47:20,113 - INFO - o4_y_test_los loaded successfully with shape (3828, 1)\n",
      "2025-04-28 21:47:20,114 - INFO - o4_y_test_mortality loaded successfully with shape (3828, 1)\n",
      "2025-04-28 21:47:20,115 - INFO - o4_y_train_los loaded successfully with shape (30624, 1)\n",
      "2025-04-28 21:47:20,116 - INFO - o4_y_train_mortality loaded successfully with shape (30624, 1)\n",
      "2025-04-28 21:47:20,117 - INFO - o4_y_validate_los loaded successfully with shape (3828, 1)\n",
      "2025-04-28 21:47:20,118 - INFO - o4_y_validate_mortality loaded successfully with shape (3828, 1)\n",
      "2025-04-28 21:47:20,119 - INFO - Load Complete.\n",
      "2025-04-28 21:47:20,119 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "# CSVs Directory \n",
    "data_path = \"../04_ANN/CSV/exports/split_set/without_multiple_rows\"\n",
    "all_files = os.listdir(data_path)\n",
    "\n",
    "logging.info(\"+++++++++++++++++CIR-2+++++++++++++++++++++++++\")\n",
    "logging.info(\"Start Loading Dataframes.\")\n",
    "\n",
    "# Load CSVs into a dictionary of dataframes\n",
    "dataframes = {}\n",
    "for file in all_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        var_name = file.replace(\".csv\", \"\").replace(\"-\", \"_\")\n",
    "        logging.info(f\"Loading... -> {file}\")\n",
    "        dataframes[var_name] = pd.read_csv(os.path.join(data_path, file)).astype('float32')\n",
    "\n",
    "# Log loaded datasets\n",
    "for var_name, df in dataframes.items():\n",
    "    globals()[var_name] = df\n",
    "    logging.info(f\"{var_name} loaded successfully with shape {df.shape}\")\n",
    "logging.info(\"Load Complete.\")\n",
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53cf697-daa1-4e6e-b0a9-868bd8a5dad0",
   "metadata": {},
   "source": [
    "# CIR-23 Implement Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da711f0-95b4-451c-9cf9-a64e59f55617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tee class to duplicate stdout\n",
    "class Tee:\n",
    "    def __init__(self, *files):\n",
    "        self.files = files\n",
    "    def write(self, obj):\n",
    "        for f in self.files:\n",
    "            f.write(obj)\n",
    "            f.flush()\n",
    "    def flush(self):\n",
    "        for f in self.files:\n",
    "            f.flush()\n",
    "\n",
    "def impute_with_iterative(input_df, method, output_path, n_iter, log_verbose_file_path=None):\n",
    "    logging.info(f\"Starting Iterative Imputer with method={method} on input DataFrame of shape {input_df.shape}.\")\n",
    "\n",
    "    data_copy = input_df.copy()\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Estimator selection\n",
    "    if method == \"ExtraTrees\":\n",
    "        estimator = ExtraTreesRegressor(n_estimators=10, random_state=0, n_jobs=-1)\n",
    "    elif method == \"HistGradientBoosting\":\n",
    "        estimator = HistGradientBoostingRegressor(random_state=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}. Use 'ExtraTrees', 'BayesianRidge', or 'HistGradientBoosting'.\")\n",
    "\n",
    "    # IterativeImputer\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=estimator,\n",
    "        max_iter=n_iter,\n",
    "        random_state=0,\n",
    "        verbose=2,  # Print to screen\n",
    "        sample_posterior=False\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if log_verbose_file_path is not None:\n",
    "        log_dir = os.path.dirname(log_verbose_file_path)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        log_file = open(log_verbose_file_path, \"w\")\n",
    "\n",
    "        # Save original stdout\n",
    "        original_stdout = sys.stdout\n",
    "\n",
    "        # Duplicate printing to both console and file\n",
    "        sys.stdout = Tee(sys.__stdout__, log_file)\n",
    "\n",
    "    try:\n",
    "        # Fit and transform\n",
    "        imputed_array = imputer.fit_transform(data_copy)\n",
    "    finally:\n",
    "        if log_verbose_file_path is not None:\n",
    "            sys.stdout = original_stdout\n",
    "            log_file.close()\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    # Save imputed data\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=data_copy.columns)\n",
    "    imputed_df.to_csv(output_path, index=False)\n",
    "\n",
    "    # Logging\n",
    "    logging.info(f\"Imputation completed in {runtime:.2f} seconds.\")\n",
    "    nan_count = np.isnan(imputed_df.values).sum()\n",
    "    logging.info(f\"Number of NaNs after imputation: {nan_count}\")\n",
    "    logging.info(f\"Imputed dataset saved at {output_path}\")\n",
    "    logging.info(f\"Basic statistics after imputation:\\n{imputed_df.describe()}\")\n",
    "\n",
    "    # Save full describe output to a separate CSV file\n",
    "    describe_output_path = output_path.replace(\".csv\", \"_describe.csv\")\n",
    "    imputed_df.describe().to_csv(describe_output_path)\n",
    "    logging.info(f\"Basic statistics saved at {describe_output_path}\")\n",
    "\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862069df-0de8-46a8-928f-a5f7e9c740cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 21:47:40,246 - INFO - Switched logging to logs/CIR-23_ExtraTrees.log\n",
      "2025-04-28 21:47:40,248 - INFO - Starting Iterative Imputer with method=ExtraTrees on input DataFrame of shape (30624, 345).\n"
     ]
    }
   ],
   "source": [
    "switch_log_file('logs/CIR-23_ExtraTrees.log')\n",
    "# ExtraTrees estimator\n",
    "impute_with_iterative(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o4_X_train_Iterative_ExtraTrees.csv\",\n",
    "    method=\"ExtraTrees\",\n",
    "    n_iter=3,\n",
    "    log_verbose_file_path=\"logs/CIR-23_ExtraTrees.log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375708b2-e3d1-46fa-b09b-5aae7fbd541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_log_file('logs/CIR-23_HistGradientBoosting.log')\n",
    "logger.info(\"This is being logged to CIR-23_HistGradientBoosting.log\")\n",
    "# HistGradientBoosting estimator\n",
    "impute_with_iterative(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o4_X_train_Iterative_HistGradientBoosting.csv\",\n",
    "    method=\"HistGradientBoosting\",\n",
    "    n_iter=20,\n",
    "    log_verbose_file_path=\"logs/CIR-23_HistGradientBoosting.log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258f36a-aaea-4318-8dac-f79500b7641c",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f7d4e-16ff-4c9d-aa90-efa3ba076a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_before_after_distributions(input_df_before, input_df_after, output_folder, sample_features=None):\n",
    "    \"\"\"\n",
    "    Plot before and after distributions for selected features and save the figures.\n",
    "    \n",
    "    If sample_features is None, it plots for all features.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    features = list(input_df_before.columns)\n",
    "\n",
    "    # If sample_features is None ➔ use all features\n",
    "    if sample_features is None:\n",
    "        sampled_features = features\n",
    "    else:\n",
    "        sampled_features = random.sample(features, min(sample_features, len(features)))\n",
    "    \n",
    "    logging.info(f\"Plotting distributions for features: {sampled_features}\")\n",
    "\n",
    "    for feature in sampled_features:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        \n",
    "        plt.hist(input_df_before[feature].dropna(), bins=50, alpha=0.5, label='Before Imputation')\n",
    "        plt.hist(input_df_after[feature].dropna(), bins=50, alpha=0.5, label='After Imputation')\n",
    "        \n",
    "        plt.title(f'Distribution of {feature}')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = os.path.join(output_folder, f\"{feature}_distribution_comparison.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "    \n",
    "    logging.info(f\"Distribution plots saved in {output_folder}\")\n",
    "\n",
    "def check_extreme_values(input_df_before, input_df_after, threshold=5.0):\n",
    "    \"\"\"\n",
    "    Check for extreme outliers introduced after imputation.\n",
    "    \"\"\"\n",
    "    suspicious_features = []\n",
    "\n",
    "    for feature in input_df_before.columns:\n",
    "        before_max = input_df_before[feature].max()\n",
    "        before_min = input_df_before[feature].min()\n",
    "        after_max = input_df_after[feature].max()\n",
    "        after_min = input_df_after[feature].min()\n",
    "\n",
    "        if before_max != 0 and (after_max > threshold * before_max or after_max < before_max / threshold):\n",
    "            suspicious_features.append((feature, 'max', before_max, after_max))\n",
    "        if before_min != 0 and (after_min < before_min / threshold or after_min > threshold * before_min):\n",
    "            suspicious_features.append((feature, 'min', before_min, after_min))\n",
    "\n",
    "    suspicious_df = pd.DataFrame(\n",
    "        suspicious_features, \n",
    "        columns=[\"Feature\", \"Type\", \"Before_Value\", \"After_Value\"]\n",
    "    )\n",
    "\n",
    "    if not suspicious_df.empty:\n",
    "        logging.warning(f\"Found {len(suspicious_df)} suspicious extreme values after imputation!\")\n",
    "\n",
    "    return suspicious_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f9e35-ed9a-40b4-9f1c-cfc039c9c6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load imputed small data\n",
    "data_imputed = pd.read_csv(f\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/{dataset}_Iterative_{method_imputed}.csv\")\n",
    "\n",
    "# Plot distributions before vs after\n",
    "plot_before_after_distributions(\n",
    "    input_df_before=dataset,\n",
    "    input_df_after=data_imputed,\n",
    "    output_folder=f\"figures/CRI-02/o1_impute_baselines/01_iterative/{method_imputed}/\",\n",
    ")\n",
    "\n",
    "# Check for extreme values\n",
    "extreme_values_df = check_extreme_values(\n",
    "    input_df_before=dataset,\n",
    "    input_df_after=data_imputed,\n",
    "    threshold=5.0\n",
    ")\n",
    "\n",
    "# Save report if needed\n",
    "if not extreme_values_df.empty:\n",
    "    extreme_values_df.to_csv(f\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/{dataset}_{method_imputed}_extreme_values.csv\", index=False)\n",
    "    logging.info(\"Extreme values report saved.\")\n",
    "else:\n",
    "    logging.info(\"No suspicious extreme values detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c78939-6284-4678-b96b-e3f5b5f3e974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfecee0-7ba0-448f-b8a8-b57b4ba78611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2c875-ed40-40e5-92c2-ea98c8620883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59963f-0645-4793-884e-c599f3db5a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05344bc-a28e-49a7-88e5-4d81593f986c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ec141-e699-4175-a1d9-376d5005c45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e00c6-2cbb-4502-ac5b-53852171c254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973dbc9e-873c-46f7-9348-c34c61a7227f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be49b3-1296-4d56-8929-759a26e94a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small subset of the dataframe (for faster testing)\n",
    "small_data = o4_X_train.iloc[:450, :]  # pick first 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120c043-4159-43aa-885a-cb76ffce5d60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Play with sample\n",
    "\"\"\"\n",
    "\n",
    "# Create a small subset of the dataframe (for faster testing)\n",
    "small_data = o4_X_train.iloc[:450, :]  # pick first 50 features\n",
    "\n",
    "# Test imputation on small_data\n",
    "impute_with_iterative(\n",
    "    input_df=small_data,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/small_o3_X_train_imputed_Iterative_ExtraTrees.csv\",\n",
    "    method=\"ExtraTrees\",\n",
    "    n_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45a9ca-e14f-4584-b5b1-adfaf9abca8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7fe60-5046-41bb-ba5f-ab40927a0913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f67544-05ba-4450-817c-2878cba95ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f58b0e-7364-4b2b-b4eb-75ba0ae04485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_before_after_distributions(input_df_before, input_df_after, output_folder, sample_features=10):\n",
    "    \"\"\"\n",
    "    Plot before and after distributions for random features and save the figures.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    features = list(input_df_before.columns)\n",
    "    sampled_features = random.sample(features, min(sample_features, len(features)))\n",
    "    \n",
    "    logging.info(f\"Plotting distributions for features: {sampled_features}\")\n",
    "\n",
    "    for feature in sampled_features:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        \n",
    "        plt.hist(input_df_before[feature].dropna(), bins=50, alpha=0.5, label='Before Imputation')\n",
    "        plt.hist(input_df_after[feature].dropna(), bins=50, alpha=0.5, label='After Imputation')\n",
    "        \n",
    "        plt.title(f'Distribution of {feature}')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = os.path.join(output_folder, f\"{feature}_distribution_comparison.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "    \n",
    "    logging.info(f\"Distribution plots saved in {output_folder}\")\n",
    "\n",
    "def check_extreme_values(input_df_before, input_df_after, threshold=5.0):\n",
    "    \"\"\"\n",
    "    Check for extreme outliers introduced after imputation.\n",
    "    \"\"\"\n",
    "    suspicious_features = []\n",
    "\n",
    "    for feature in input_df_before.columns:\n",
    "        before_max = input_df_before[feature].max()\n",
    "        before_min = input_df_before[feature].min()\n",
    "        after_max = input_df_after[feature].max()\n",
    "        after_min = input_df_after[feature].min()\n",
    "\n",
    "        if before_max != 0 and (after_max > threshold * before_max or after_max < before_max / threshold):\n",
    "            suspicious_features.append((feature, 'max', before_max, after_max))\n",
    "        if before_min != 0 and (after_min < before_min / threshold or after_min > threshold * before_min):\n",
    "            suspicious_features.append((feature, 'min', before_min, after_min))\n",
    "\n",
    "    suspicious_df = pd.DataFrame(\n",
    "        suspicious_features, \n",
    "        columns=[\"Feature\", \"Type\", \"Before_Value\", \"After_Value\"]\n",
    "    )\n",
    "\n",
    "    if not suspicious_df.empty:\n",
    "        logging.warning(f\"Found {len(suspicious_df)} suspicious extreme values after imputation!\")\n",
    "\n",
    "    return suspicious_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9848f-4f3b-4503-87a1-e91d853a19ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721fad44-3e36-4d28-a738-bfa8a9beced3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eeb3f5-3431-4000-9bcf-a51446e2bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load imputed small data\n",
    "small_data_imputed = pd.read_csv(\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/small_o3_X_train_imputed_Iterative_ExtraTrees.csv\")\n",
    "\n",
    "# Plot distributions before vs after\n",
    "plot_before_after_distributions(\n",
    "    input_df_before=small_data,\n",
    "    input_df_after=small_data_imputed,\n",
    "    output_folder=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/plots_small_test/\",\n",
    "    #sample_features=10\n",
    ")\n",
    "\n",
    "# Check for extreme values\n",
    "extreme_values_df = check_extreme_values(\n",
    "    input_df_before=small_data,\n",
    "    input_df_after=small_data_imputed,\n",
    "    threshold=5.0\n",
    ")\n",
    "\n",
    "# Save report if needed\n",
    "if not extreme_values_df.empty:\n",
    "    extreme_values_df.to_csv(\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/extreme_values_report_small_test.csv\", index=False)\n",
    "    logging.info(\"Extreme values report saved.\")\n",
    "else:\n",
    "    logging.info(\"No suspicious extreme values detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301af18-b709-45ca-a8bb-c358e1382c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5a9aa-76e1-498a-97a1-5f3cda66fde5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b457f7-7d80-4009-b0d8-a9beb0718d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc79ac-e045-4fc9-9e6b-3cbed3e8d7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6656c4-c95c-4680-9231-e39a9bdbc63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8599a64-9c3d-4e7d-b303-9f0a676abcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae89814-14f5-47e3-8111-76361fe6ca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8915f50-dd35-41cd-833f-35a51822a47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2966cf1-bb7d-4655-b261-121d2d79062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExtraTrees estimator\n",
    "impute_with_iterative(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o3_X_train_imputed_Iterative_ExtraTrees.csv\",\n",
    "    method=\"ExtraTrees\",\n",
    "    n_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c1554f-a143-4ac7-a8fa-a5a6a0a3b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianRidge estimator\n",
    "impute_with_iterative(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o3_X_train_imputed_Iterative_BayesianRidge.csv\",\n",
    "    method=\"BayesianRidge\",\n",
    "    n_iter=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
