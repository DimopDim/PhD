{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdebed6-b39e-4d34-b4c3-bda11f83ab2a",
   "metadata": {},
   "source": [
    "# CIR-2 Baseline Imputation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448fce81-0731-4ad1-8f7f-f80619f29358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8893061-af8a-4097-9c7a-e254f73c15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial logger setup\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Global variable to hold the active file handler\n",
    "current_file_handler = None\n",
    "\n",
    "# Create the stream handler (to console)\n",
    "stream_handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "def switch_log_file(filename):\n",
    "    global current_file_handler\n",
    "\n",
    "    # If a file handler already exists, remove and close it\n",
    "    if current_file_handler:\n",
    "        logger.removeHandler(current_file_handler)\n",
    "        current_file_handler.close()\n",
    "\n",
    "    # Create a new file handler\n",
    "    current_file_handler = logging.FileHandler(filename)\n",
    "    current_file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(current_file_handler)\n",
    "\n",
    "    logger.info(f\"Switched logging to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bdf90b6-c578-4594-94ce-70255ad091af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 23:14:13,654 - INFO - Switched logging to logs/CIR-2.log\n",
      "2025-04-28 23:14:13,657 - INFO - This is being logged to CIR-2.log\n"
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-2.log')\n",
    "logger.info(\"This is being logged to CIR-2.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6883c12-3115-4f33-a5bc-143d140e7b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 23:14:13,670 - INFO - +++++++++++++++++CIR-2+++++++++++++++++++++++++\n",
      "2025-04-28 23:14:13,671 - INFO - Start Loading Dataframes.\n",
      "2025-04-28 23:14:13,672 - INFO - Loading... -> o1_X_external.csv\n",
      "2025-04-28 23:14:20,912 - INFO - Loading... -> o1_X_test.csv\n",
      "2025-04-28 23:14:21,418 - INFO - Loading... -> o1_X_train.csv\n",
      "2025-04-28 23:14:25,526 - INFO - Loading... -> o1_X_validate.csv\n",
      "2025-04-28 23:14:26,087 - INFO - Loading... -> o1_y_external_los.csv\n",
      "2025-04-28 23:14:26,134 - INFO - Loading... -> o1_y_external_mortality.csv\n",
      "2025-04-28 23:14:26,164 - INFO - Loading... -> o1_y_test_los.csv\n",
      "2025-04-28 23:14:26,176 - INFO - Loading... -> o1_y_test_mortality.csv\n",
      "2025-04-28 23:14:26,181 - INFO - Loading... -> o1_y_train_los.csv\n",
      "2025-04-28 23:14:26,225 - INFO - Loading... -> o1_y_train_mortality.csv\n",
      "2025-04-28 23:14:26,245 - INFO - Loading... -> o1_y_validate_los.csv\n",
      "2025-04-28 23:14:26,255 - INFO - Loading... -> o1_y_validate_mortality.csv\n",
      "2025-04-28 23:14:26,260 - INFO - Loading... -> o2_X_external.csv\n",
      "2025-04-28 23:14:29,953 - INFO - Loading... -> o2_X_test.csv\n",
      "2025-04-28 23:14:30,252 - INFO - Loading... -> o2_X_train.csv\n",
      "2025-04-28 23:14:32,390 - INFO - Loading... -> o2_X_validate.csv\n",
      "2025-04-28 23:14:32,673 - INFO - Loading... -> o2_y_external_los.csv\n",
      "2025-04-28 23:14:32,697 - INFO - Loading... -> o2_y_external_mortality.csv\n",
      "2025-04-28 23:14:32,715 - INFO - Loading... -> o2_y_test_los.csv\n",
      "2025-04-28 23:14:32,723 - INFO - Loading... -> o2_y_test_mortality.csv\n",
      "2025-04-28 23:14:32,728 - INFO - Loading... -> o2_y_train_los.csv\n",
      "2025-04-28 23:14:32,751 - INFO - Loading... -> o2_y_train_mortality.csv\n",
      "2025-04-28 23:14:32,762 - INFO - Loading... -> o2_y_validate_los.csv\n",
      "2025-04-28 23:14:32,769 - INFO - Loading... -> o2_y_validate_mortality.csv\n",
      "2025-04-28 23:14:32,773 - INFO - Loading... -> o3_X_external.csv\n",
      "2025-04-28 23:14:35,095 - INFO - Loading... -> o3_X_test.csv\n",
      "2025-04-28 23:14:35,293 - INFO - Loading... -> o3_X_train.csv\n",
      "2025-04-28 23:14:36,640 - INFO - Loading... -> o3_X_validate.csv\n",
      "2025-04-28 23:14:36,829 - INFO - Loading... -> o3_y_external_los.csv\n",
      "2025-04-28 23:14:36,846 - INFO - Loading... -> o3_y_external_mortality.csv\n",
      "2025-04-28 23:14:36,860 - INFO - Loading... -> o3_y_test_los.csv\n",
      "2025-04-28 23:14:36,864 - INFO - Loading... -> o3_y_test_mortality.csv\n",
      "2025-04-28 23:14:36,869 - INFO - Loading... -> o3_y_train_los.csv\n",
      "2025-04-28 23:14:36,886 - INFO - Loading... -> o3_y_train_mortality.csv\n",
      "2025-04-28 23:14:36,894 - INFO - Loading... -> o3_y_validate_los.csv\n",
      "2025-04-28 23:14:36,898 - INFO - Loading... -> o3_y_validate_mortality.csv\n",
      "2025-04-28 23:14:36,904 - INFO - Loading... -> o4_X_external.csv\n",
      "2025-04-28 23:14:38,691 - INFO - Loading... -> o4_X_test.csv\n",
      "2025-04-28 23:14:38,849 - INFO - Loading... -> o4_X_train.csv\n",
      "2025-04-28 23:14:39,908 - INFO - Loading... -> o4_X_validate.csv\n",
      "2025-04-28 23:14:40,066 - INFO - Loading... -> o4_y_external_los.csv\n",
      "2025-04-28 23:14:40,082 - INFO - Loading... -> o4_y_external_mortality.csv\n",
      "2025-04-28 23:14:40,094 - INFO - Loading... -> o4_y_test_los.csv\n",
      "2025-04-28 23:14:40,099 - INFO - Loading... -> o4_y_test_mortality.csv\n",
      "2025-04-28 23:14:40,104 - INFO - Loading... -> o4_y_train_los.csv\n",
      "2025-04-28 23:14:40,120 - INFO - Loading... -> o4_y_train_mortality.csv\n",
      "2025-04-28 23:14:40,126 - INFO - Loading... -> o4_y_validate_los.csv\n",
      "2025-04-28 23:14:40,130 - INFO - Loading... -> o4_y_validate_mortality.csv\n",
      "2025-04-28 23:14:40,133 - INFO - o1_X_external loaded successfully with shape (234720, 345)\n",
      "2025-04-28 23:14:40,137 - INFO - o1_X_test loaded successfully with shape (15312, 345)\n",
      "2025-04-28 23:14:40,138 - INFO - o1_X_train loaded successfully with shape (122496, 345)\n",
      "2025-04-28 23:14:40,140 - INFO - o1_X_validate loaded successfully with shape (15312, 345)\n",
      "2025-04-28 23:14:40,141 - INFO - o1_y_external_los loaded successfully with shape (234720, 1)\n",
      "2025-04-28 23:14:40,141 - INFO - o1_y_external_mortality loaded successfully with shape (234720, 1)\n",
      "2025-04-28 23:14:40,144 - INFO - o1_y_test_los loaded successfully with shape (15312, 1)\n",
      "2025-04-28 23:14:40,145 - INFO - o1_y_test_mortality loaded successfully with shape (15312, 1)\n",
      "2025-04-28 23:14:40,146 - INFO - o1_y_train_los loaded successfully with shape (122496, 1)\n",
      "2025-04-28 23:14:40,147 - INFO - o1_y_train_mortality loaded successfully with shape (122496, 1)\n",
      "2025-04-28 23:14:40,148 - INFO - o1_y_validate_los loaded successfully with shape (15312, 1)\n",
      "2025-04-28 23:14:40,149 - INFO - o1_y_validate_mortality loaded successfully with shape (15312, 1)\n",
      "2025-04-28 23:14:40,150 - INFO - o2_X_external loaded successfully with shape (117360, 345)\n",
      "2025-04-28 23:14:40,154 - INFO - o2_X_test loaded successfully with shape (7656, 345)\n",
      "2025-04-28 23:14:40,156 - INFO - o2_X_train loaded successfully with shape (61248, 345)\n",
      "2025-04-28 23:14:40,157 - INFO - o2_X_validate loaded successfully with shape (7656, 345)\n",
      "2025-04-28 23:14:40,158 - INFO - o2_y_external_los loaded successfully with shape (117360, 1)\n",
      "2025-04-28 23:14:40,159 - INFO - o2_y_external_mortality loaded successfully with shape (117360, 1)\n",
      "2025-04-28 23:14:40,160 - INFO - o2_y_test_los loaded successfully with shape (7656, 1)\n",
      "2025-04-28 23:14:40,161 - INFO - o2_y_test_mortality loaded successfully with shape (7656, 1)\n",
      "2025-04-28 23:14:40,162 - INFO - o2_y_train_los loaded successfully with shape (61248, 1)\n",
      "2025-04-28 23:14:40,163 - INFO - o2_y_train_mortality loaded successfully with shape (61248, 1)\n",
      "2025-04-28 23:14:40,164 - INFO - o2_y_validate_los loaded successfully with shape (7656, 1)\n",
      "2025-04-28 23:14:40,165 - INFO - o2_y_validate_mortality loaded successfully with shape (7656, 1)\n",
      "2025-04-28 23:14:40,166 - INFO - o3_X_external loaded successfully with shape (78240, 345)\n",
      "2025-04-28 23:14:40,167 - INFO - o3_X_test loaded successfully with shape (5104, 345)\n",
      "2025-04-28 23:14:40,169 - INFO - o3_X_train loaded successfully with shape (40832, 345)\n",
      "2025-04-28 23:14:40,170 - INFO - o3_X_validate loaded successfully with shape (5104, 345)\n",
      "2025-04-28 23:14:40,172 - INFO - o3_y_external_los loaded successfully with shape (78240, 1)\n",
      "2025-04-28 23:14:40,173 - INFO - o3_y_external_mortality loaded successfully with shape (78240, 1)\n",
      "2025-04-28 23:14:40,174 - INFO - o3_y_test_los loaded successfully with shape (5104, 1)\n",
      "2025-04-28 23:14:40,175 - INFO - o3_y_test_mortality loaded successfully with shape (5104, 1)\n",
      "2025-04-28 23:14:40,176 - INFO - o3_y_train_los loaded successfully with shape (40832, 1)\n",
      "2025-04-28 23:14:40,177 - INFO - o3_y_train_mortality loaded successfully with shape (40832, 1)\n",
      "2025-04-28 23:14:40,178 - INFO - o3_y_validate_los loaded successfully with shape (5104, 1)\n",
      "2025-04-28 23:14:40,178 - INFO - o3_y_validate_mortality loaded successfully with shape (5104, 1)\n",
      "2025-04-28 23:14:40,179 - INFO - o4_X_external loaded successfully with shape (58680, 345)\n",
      "2025-04-28 23:14:40,180 - INFO - o4_X_test loaded successfully with shape (3828, 345)\n",
      "2025-04-28 23:14:40,181 - INFO - o4_X_train loaded successfully with shape (30624, 345)\n",
      "2025-04-28 23:14:40,181 - INFO - o4_X_validate loaded successfully with shape (3828, 345)\n",
      "2025-04-28 23:14:40,182 - INFO - o4_y_external_los loaded successfully with shape (58680, 1)\n",
      "2025-04-28 23:14:40,183 - INFO - o4_y_external_mortality loaded successfully with shape (58680, 1)\n",
      "2025-04-28 23:14:40,186 - INFO - o4_y_test_los loaded successfully with shape (3828, 1)\n",
      "2025-04-28 23:14:40,187 - INFO - o4_y_test_mortality loaded successfully with shape (3828, 1)\n",
      "2025-04-28 23:14:40,188 - INFO - o4_y_train_los loaded successfully with shape (30624, 1)\n",
      "2025-04-28 23:14:40,188 - INFO - o4_y_train_mortality loaded successfully with shape (30624, 1)\n",
      "2025-04-28 23:14:40,189 - INFO - o4_y_validate_los loaded successfully with shape (3828, 1)\n",
      "2025-04-28 23:14:40,190 - INFO - o4_y_validate_mortality loaded successfully with shape (3828, 1)\n",
      "2025-04-28 23:14:40,191 - INFO - Load Complete.\n",
      "2025-04-28 23:14:40,192 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "# CSVs Directory \n",
    "data_path = \"../04_ANN/CSV/exports/split_set/without_multiple_rows\"\n",
    "all_files = os.listdir(data_path)\n",
    "\n",
    "logging.info(\"+++++++++++++++++CIR-2+++++++++++++++++++++++++\")\n",
    "logging.info(\"Start Loading Dataframes.\")\n",
    "\n",
    "# Load CSVs into a dictionary of dataframes\n",
    "dataframes = {}\n",
    "for file in all_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        var_name = file.replace(\".csv\", \"\").replace(\"-\", \"_\")\n",
    "        logging.info(f\"Loading... -> {file}\")\n",
    "        dataframes[var_name] = pd.read_csv(os.path.join(data_path, file)).astype('float32')\n",
    "\n",
    "# Log loaded datasets\n",
    "for var_name, df in dataframes.items():\n",
    "    globals()[var_name] = df\n",
    "    logging.info(f\"{var_name} loaded successfully with shape {df.shape}\")\n",
    "logging.info(\"Load Complete.\")\n",
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53cf697-daa1-4e6e-b0a9-868bd8a5dad0",
   "metadata": {},
   "source": [
    "# CIR-23 Implement Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9da711f0-95b4-451c-9cf9-a64e59f55617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tee class to duplicate stdout\n",
    "class Tee:\n",
    "    def __init__(self, *files):\n",
    "        self.files = files\n",
    "    def write(self, obj):\n",
    "        for f in self.files:\n",
    "            f.write(obj)\n",
    "            f.flush()\n",
    "    def flush(self):\n",
    "        for f in self.files:\n",
    "            f.flush()\n",
    "\n",
    "def impute_with_iterative(input_df, method, output_path, n_iter, log_verbose_file_path=None):\n",
    "    logging.info(f\"Starting Iterative Imputer with method={method} on input DataFrame of shape {input_df.shape}.\")\n",
    "\n",
    "    data_copy = input_df.copy()\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Estimator selection\n",
    "    if method == \"ExtraTrees\":\n",
    "        estimator = ExtraTreesRegressor(n_estimators=10, random_state=0, n_jobs=-1)\n",
    "    elif method == \"HistGradientBoosting\":\n",
    "        estimator = HistGradientBoostingRegressor(random_state=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}. Use 'ExtraTrees', 'BayesianRidge', or 'HistGradientBoosting'.\")\n",
    "\n",
    "    # IterativeImputer\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=estimator,\n",
    "        max_iter=n_iter,\n",
    "        random_state=0,\n",
    "        verbose=2,  # Print to screen\n",
    "        sample_posterior=False\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Handle logging duplication\n",
    "    if log_verbose_file_path is not None:\n",
    "        log_dir = os.path.dirname(log_verbose_file_path)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "        original_stdout = sys.stdout  # Save original stdout\n",
    "        with open(log_verbose_file_path, \"w\") as log_file:\n",
    "            sys.stdout = Tee(sys.__stdout__, log_file)\n",
    "\n",
    "            try:\n",
    "                # Fit and transform\n",
    "                imputed_array = imputer.fit_transform(data_copy)\n",
    "            finally:\n",
    "                sys.stdout = original_stdout  # Restore stdout\n",
    "                # No need to manually close or flush, 'with' handles it\n",
    "\n",
    "    else:\n",
    "        # If no logging redirection\n",
    "        imputed_array = imputer.fit_transform(data_copy)\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    # Save imputed data\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=data_copy.columns)\n",
    "    imputed_df.to_csv(output_path, index=False)\n",
    "\n",
    "    # Logging\n",
    "    logging.info(f\"Imputation completed in {runtime:.2f} seconds.\")\n",
    "    nan_count = np.isnan(imputed_df.values).sum()\n",
    "    logging.info(f\"Number of NaNs after imputation: {nan_count}\")\n",
    "    logging.info(f\"Imputed dataset saved at {output_path}\")\n",
    "    logging.info(f\"Basic statistics after imputation:\\n{imputed_df.describe()}\")\n",
    "\n",
    "    # Save full describe output to a separate CSV file\n",
    "    describe_output_path = output_path.replace(\".csv\", \"_describe.csv\")\n",
    "    imputed_df.describe().to_csv(describe_output_path)\n",
    "    logging.info(f\"Basic statistics saved at {describe_output_path}\")\n",
    "\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "862069df-0de8-46a8-928f-a5f7e9c740cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 23:25:34,017 - INFO - Switched logging to logs/CIR-23_ExtraTrees.log\n",
      "2025-04-28 23:25:34,018 - INFO - Starting Iterative Imputer with method=ExtraTrees on input DataFrame of shape (450, 345).\n",
      "C:\\Users\\Dimopoulos\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "2025-04-28 23:26:21,752 - INFO - Imputation completed in 47.61 seconds.\n",
      "2025-04-28 23:26:21,753 - INFO - Number of NaNs after imputation: 0\n",
      "2025-04-28 23:26:21,755 - INFO - Imputed dataset saved at CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o4_X_train_Iterative_ExtraTrees.csv\n",
      "2025-04-28 23:26:22,268 - INFO - Basic statistics after imputation:\n",
      "              age  Alanine_Aminotransferase_(ALT)_(Max)  \\\n",
      "count  450.000000                            450.000000   \n",
      "mean    69.199997                             61.234222   \n",
      "std     14.875883                             90.188217   \n",
      "min     32.000000                             11.000000   \n",
      "25%     56.000000                             40.900002   \n",
      "50%     71.000000                             50.000000   \n",
      "75%     82.000000                             66.000000   \n",
      "max     89.000000                           1784.000000   \n",
      "\n",
      "       Alanine_Aminotransferase_(ALT)_(Mean)  \\\n",
      "count                             450.000000   \n",
      "mean                               58.587002   \n",
      "std                                55.953644   \n",
      "min                                11.000000   \n",
      "25%                                42.500000   \n",
      "50%                                50.650002   \n",
      "75%                                58.849998   \n",
      "max                               921.000000   \n",
      "\n",
      "       Alanine_Aminotransferase_(ALT)_(Median)  \\\n",
      "count                               450.000000   \n",
      "mean                                 58.958553   \n",
      "std                                  56.389568   \n",
      "min                                  11.000000   \n",
      "25%                                  42.500000   \n",
      "50%                                  50.650002   \n",
      "75%                                  58.849998   \n",
      "max                                 921.000000   \n",
      "\n",
      "       Alanine_Aminotransferase_(ALT)_(Min)  Albumin_(Max)  Albumin_(Mean)  \\\n",
      "count                            450.000000     450.000000      450.000000   \n",
      "mean                              54.721554       3.286511        3.244700   \n",
      "std                               37.698448       0.486605        0.491783   \n",
      "min                               11.000000       2.200000        2.200000   \n",
      "25%                               39.599998       2.902500        2.900000   \n",
      "50%                               46.700001       3.265000        3.200000   \n",
      "75%                               58.099998       3.700000        3.647500   \n",
      "max                              211.000000       4.200000        4.200000   \n",
      "\n",
      "       Albumin_(Median)  Albumin_(Min)  Alkaline_Phosphatase_(Max)  ...  \\\n",
      "count        450.000000     450.000000                  450.000000  ...   \n",
      "mean           3.243755       3.163578                   99.861565  ...   \n",
      "std            0.491589       0.547424                   51.229717  ...   \n",
      "min            2.200000       2.200000                   41.000000  ...   \n",
      "25%            2.900000       2.810000                   80.000000  ...   \n",
      "50%            3.200000       3.200000                   87.900002  ...   \n",
      "75%            3.647500       3.610000                  108.900002  ...   \n",
      "max            4.200000       4.200000                  343.000000  ...   \n",
      "\n",
      "       race_PATIENT DECLINED TO ANSWER  race_PORTUGUESE  race_SOUTH AMERICAN  \\\n",
      "count                       450.000000            450.0                450.0   \n",
      "mean                          0.026667              0.0                  0.0   \n",
      "std                           0.161288              0.0                  0.0   \n",
      "min                           0.000000              0.0                  0.0   \n",
      "25%                           0.000000              0.0                  0.0   \n",
      "50%                           0.000000              0.0                  0.0   \n",
      "75%                           0.000000              0.0                  0.0   \n",
      "max                           1.000000              0.0                  0.0   \n",
      "\n",
      "       race_UNABLE TO OBTAIN  race_UNKNOWN  race_WHITE  \\\n",
      "count             450.000000    450.000000  450.000000   \n",
      "mean                0.026667      0.106667    0.586667   \n",
      "std                 0.161287      0.309033    0.492980   \n",
      "min                 0.000000      0.000000    0.000000   \n",
      "25%                 0.000000      0.000000    0.000000   \n",
      "50%                 0.000000      0.000000    1.000000   \n",
      "75%                 0.000000      0.000000    1.000000   \n",
      "max                 1.000000      1.000000    1.000000   \n",
      "\n",
      "       race_WHITE - BRAZILIAN  race_WHITE - EASTERN EUROPEAN  \\\n",
      "count                   450.0                          450.0   \n",
      "mean                      0.0                            0.0   \n",
      "std                       0.0                            0.0   \n",
      "min                       0.0                            0.0   \n",
      "25%                       0.0                            0.0   \n",
      "50%                       0.0                            0.0   \n",
      "75%                       0.0                            0.0   \n",
      "max                       0.0                            0.0   \n",
      "\n",
      "       race_WHITE - OTHER EUROPEAN  race_WHITE - RUSSIAN  \n",
      "count                        450.0                 450.0  \n",
      "mean                           0.0                   0.0  \n",
      "std                            0.0                   0.0  \n",
      "min                            0.0                   0.0  \n",
      "25%                            0.0                   0.0  \n",
      "50%                            0.0                   0.0  \n",
      "75%                            0.0                   0.0  \n",
      "max                            0.0                   0.0  \n",
      "\n",
      "[8 rows x 345 columns]\n",
      "2025-04-28 23:26:22,770 - INFO - Basic statistics saved at CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o4_X_train_Iterative_ExtraTrees_describe.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Alanine_Aminotransferase_(ALT)_(Max)</th>\n",
       "      <th>Alanine_Aminotransferase_(ALT)_(Mean)</th>\n",
       "      <th>Alanine_Aminotransferase_(ALT)_(Median)</th>\n",
       "      <th>Alanine_Aminotransferase_(ALT)_(Min)</th>\n",
       "      <th>Albumin_(Max)</th>\n",
       "      <th>Albumin_(Mean)</th>\n",
       "      <th>Albumin_(Median)</th>\n",
       "      <th>Albumin_(Min)</th>\n",
       "      <th>Alkaline_Phosphatase_(Max)</th>\n",
       "      <th>...</th>\n",
       "      <th>race_PATIENT DECLINED TO ANSWER</th>\n",
       "      <th>race_PORTUGUESE</th>\n",
       "      <th>race_SOUTH AMERICAN</th>\n",
       "      <th>race_UNABLE TO OBTAIN</th>\n",
       "      <th>race_UNKNOWN</th>\n",
       "      <th>race_WHITE</th>\n",
       "      <th>race_WHITE - BRAZILIAN</th>\n",
       "      <th>race_WHITE - EASTERN EUROPEAN</th>\n",
       "      <th>race_WHITE - OTHER EUROPEAN</th>\n",
       "      <th>race_WHITE - RUSSIAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>78.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>78.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>78.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>78.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>78.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 345 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  Alanine_Aminotransferase_(ALT)_(Max)  \\\n",
       "0    51.0                                  46.0   \n",
       "1    51.0                                  46.0   \n",
       "2    51.0                                  46.0   \n",
       "3    51.0                                  46.0   \n",
       "4    51.0                                  46.0   \n",
       "..    ...                                   ...   \n",
       "445  78.0                                  11.0   \n",
       "446  78.0                                  11.0   \n",
       "447  78.0                                  11.0   \n",
       "448  78.0                                  11.0   \n",
       "449  78.0                                  11.0   \n",
       "\n",
       "     Alanine_Aminotransferase_(ALT)_(Mean)  \\\n",
       "0                                     46.0   \n",
       "1                                     46.0   \n",
       "2                                     46.0   \n",
       "3                                     46.0   \n",
       "4                                     46.0   \n",
       "..                                     ...   \n",
       "445                                   11.0   \n",
       "446                                   11.0   \n",
       "447                                   11.0   \n",
       "448                                   11.0   \n",
       "449                                   11.0   \n",
       "\n",
       "     Alanine_Aminotransferase_(ALT)_(Median)  \\\n",
       "0                                       46.0   \n",
       "1                                       46.0   \n",
       "2                                       46.0   \n",
       "3                                       46.0   \n",
       "4                                       46.0   \n",
       "..                                       ...   \n",
       "445                                     11.0   \n",
       "446                                     11.0   \n",
       "447                                     11.0   \n",
       "448                                     11.0   \n",
       "449                                     11.0   \n",
       "\n",
       "     Alanine_Aminotransferase_(ALT)_(Min)  Albumin_(Max)  Albumin_(Mean)  \\\n",
       "0                                    46.0            2.9             2.9   \n",
       "1                                    46.0            2.9             2.9   \n",
       "2                                    46.0            2.9             2.9   \n",
       "3                                    46.0            2.9             2.9   \n",
       "4                                    46.0            2.9             2.9   \n",
       "..                                    ...            ...             ...   \n",
       "445                                  11.0            4.1             4.1   \n",
       "446                                  11.0            4.1             4.1   \n",
       "447                                  11.0            4.1             4.1   \n",
       "448                                  11.0            4.1             4.1   \n",
       "449                                  11.0            4.1             4.1   \n",
       "\n",
       "     Albumin_(Median)  Albumin_(Min)  Alkaline_Phosphatase_(Max)  ...  \\\n",
       "0                 2.9            2.9                       113.0  ...   \n",
       "1                 2.9            2.9                       113.0  ...   \n",
       "2                 2.9            2.9                       113.0  ...   \n",
       "3                 2.9            2.9                       113.0  ...   \n",
       "4                 2.9            2.9                       113.0  ...   \n",
       "..                ...            ...                         ...  ...   \n",
       "445               4.1            4.1                        91.0  ...   \n",
       "446               4.1            4.1                        91.0  ...   \n",
       "447               4.1            4.1                        91.0  ...   \n",
       "448               4.1            4.1                        91.0  ...   \n",
       "449               4.1            4.1                        91.0  ...   \n",
       "\n",
       "     race_PATIENT DECLINED TO ANSWER  race_PORTUGUESE  race_SOUTH AMERICAN  \\\n",
       "0                                0.0              0.0                  0.0   \n",
       "1                                0.0              0.0                  0.0   \n",
       "2                                0.0              0.0                  0.0   \n",
       "3                                0.0              0.0                  0.0   \n",
       "4                                0.0              0.0                  0.0   \n",
       "..                               ...              ...                  ...   \n",
       "445                              0.0              0.0                  0.0   \n",
       "446                              0.0              0.0                  0.0   \n",
       "447                              0.0              0.0                  0.0   \n",
       "448                              0.0              0.0                  0.0   \n",
       "449                              0.0              0.0                  0.0   \n",
       "\n",
       "     race_UNABLE TO OBTAIN  race_UNKNOWN  race_WHITE  race_WHITE - BRAZILIAN  \\\n",
       "0                      0.0           1.0         0.0                     0.0   \n",
       "1                      0.0           1.0         0.0                     0.0   \n",
       "2                      0.0           1.0         0.0                     0.0   \n",
       "3                      0.0           1.0         0.0                     0.0   \n",
       "4                      0.0           1.0         0.0                     0.0   \n",
       "..                     ...           ...         ...                     ...   \n",
       "445                    0.0           0.0         0.0                     0.0   \n",
       "446                    0.0           0.0         0.0                     0.0   \n",
       "447                    0.0           0.0         0.0                     0.0   \n",
       "448                    0.0           0.0         0.0                     0.0   \n",
       "449                    0.0           0.0         0.0                     0.0   \n",
       "\n",
       "     race_WHITE - EASTERN EUROPEAN  race_WHITE - OTHER EUROPEAN  \\\n",
       "0                              0.0                          0.0   \n",
       "1                              0.0                          0.0   \n",
       "2                              0.0                          0.0   \n",
       "3                              0.0                          0.0   \n",
       "4                              0.0                          0.0   \n",
       "..                             ...                          ...   \n",
       "445                            0.0                          0.0   \n",
       "446                            0.0                          0.0   \n",
       "447                            0.0                          0.0   \n",
       "448                            0.0                          0.0   \n",
       "449                            0.0                          0.0   \n",
       "\n",
       "     race_WHITE - RUSSIAN  \n",
       "0                     0.0  \n",
       "1                     0.0  \n",
       "2                     0.0  \n",
       "3                     0.0  \n",
       "4                     0.0  \n",
       "..                    ...  \n",
       "445                   0.0  \n",
       "446                   0.0  \n",
       "447                   0.0  \n",
       "448                   0.0  \n",
       "449                   0.0  \n",
       "\n",
       "[450 rows x 345 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_log_file('logs/CIR-23_ExtraTrees.log')\n",
    "# ExtraTrees estimator\n",
    "impute_with_iterative(\n",
    "    input_df=small_data,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o4_X_train_Iterative_ExtraTrees.csv\",\n",
    "    method=\"ExtraTrees\",\n",
    "    n_iter=3,\n",
    "    log_verbose_file_path=\"logs/CIR-23_ExtraTrees.log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375708b2-e3d1-46fa-b09b-5aae7fbd541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_log_file('logs/CIR-23_HistGradientBoosting.log')\n",
    "logger.info(\"This is being logged to CIR-23_HistGradientBoosting.log\")\n",
    "# HistGradientBoosting estimator\n",
    "impute_with_iterative(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o4_X_train_Iterative_HistGradientBoosting.csv\",\n",
    "    method=\"HistGradientBoosting\",\n",
    "    n_iter=20,\n",
    "    log_verbose_file_path=\"logs/CIR-23_HistGradientBoosting.log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d5407-9a2e-40f2-b51d-6813b8b801dd",
   "metadata": {},
   "source": [
    "# Custom Iterative Imputer\n",
    "## I should see it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0090b5f-4323-44ab-9816-5301ff1077db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import clone\n",
    "\n",
    "def custom_iterative_imputer(input_df, method, output_folder, n_iter=5, random_state=0):\n",
    "    \"\"\"\n",
    "    Custom Iterative Imputer that saves outputs after each iteration,\n",
    "    calculates Change and Scaled Tolerance.\n",
    "\n",
    "    Parameters:\n",
    "    - input_df: pd.DataFrame to impute.\n",
    "    - method: \"ExtraTrees\", \"BayesianRidge\", or \"HistGradientBoosting\".\n",
    "    - output_folder: Folder to save intermediate outputs.\n",
    "    - n_iter: Number of maximum iterations.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting Custom Iterative Imputer with method={method} on input DataFrame of shape {input_df.shape}.\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    data_copy = input_df.copy()\n",
    "\n",
    "    # Choose estimator\n",
    "    if method == \"ExtraTrees\":\n",
    "        base_estimator = ExtraTreesRegressor(n_estimators=10, random_state=random_state, n_jobs=-1)\n",
    "    elif method == \"BayesianRidge\":\n",
    "        base_estimator = BayesianRidge()\n",
    "    elif method == \"HistGradientBoosting\":\n",
    "        base_estimator = HistGradientBoostingRegressor(random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}\")\n",
    "\n",
    "    # Initial mean fill to avoid full NaN columns\n",
    "    simple_imputer = SimpleImputer(strategy=\"mean\")\n",
    "    X_filled = pd.DataFrame(simple_imputer.fit_transform(data_copy), columns=data_copy.columns).astype(np.float32)\n",
    "\n",
    "    # Save initial state for comparison\n",
    "    X_prev = X_filled.copy()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for round_idx in range(1, n_iter + 1):\n",
    "        logging.info(f\"Starting iteration {round_idx}/{n_iter}\")\n",
    "\n",
    "        for feature_idx, feature in enumerate(X_filled.columns):\n",
    "            if data_copy[feature].isnull().sum() == 0:\n",
    "                continue  # No missing values to predict for this feature\n",
    "\n",
    "            # Split data\n",
    "            known_mask = ~data_copy[feature].isnull()\n",
    "            unknown_mask = data_copy[feature].isnull()\n",
    "\n",
    "            if known_mask.sum() == 0:\n",
    "                logging.warning(f\"Skipping feature {feature} entirely missing.\")\n",
    "                continue\n",
    "\n",
    "            X_known = X_filled.loc[known_mask].drop(columns=[feature])\n",
    "            y_known = X_filled.loc[known_mask, feature]\n",
    "            X_unknown = X_filled.loc[unknown_mask].drop(columns=[feature])\n",
    "\n",
    "            if X_unknown.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            # Clone the base estimator each time to avoid contamination\n",
    "            estimator = clone(base_estimator)\n",
    "            estimator.fit(X_known, y_known)\n",
    "            y_pred = estimator.predict(X_unknown)\n",
    "\n",
    "            # Fix dtype to avoid FutureWarning\n",
    "            X_filled.loc[unknown_mask, feature] = y_pred.astype(np.float32)\n",
    "\n",
    "        # Calculate Change and Scaled Tolerance\n",
    "        change = np.abs(X_filled.values - X_prev.values).sum()\n",
    "        scaled_tolerance = np.mean(np.abs(X_prev.values)) * 1e-3\n",
    "\n",
    "        logging.info(f\"Ending iteration {round_idx}/{n_iter}\")\n",
    "        logging.info(f\"Change: {change:.6f}, Scaled Tolerance: {scaled_tolerance:.6f}\")\n",
    "\n",
    "        # Save after each round\n",
    "        round_csv = os.path.join(output_folder, f\"imputed_round_{round_idx}.csv\")\n",
    "        round_desc = os.path.join(output_folder, f\"imputed_round_{round_idx}_describe.csv\")\n",
    "\n",
    "        X_filled.to_csv(round_csv, index=False)\n",
    "        X_filled.describe().to_csv(round_desc)\n",
    "\n",
    "        logging.info(f\"Iteration {round_idx} completed and saved to {round_csv}\")\n",
    "\n",
    "        # Update X_prev for next round\n",
    "        X_prev = X_filled.copy()\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    logging.info(f\"Custom Iterative Imputer completed in {runtime:.2f} seconds.\")\n",
    "\n",
    "    return X_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b162ff12-f04b-42be-9510-aeb0f4d2c59d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 23:24:30,721 - INFO - Switched logging to logs/CIR-24_Custom_Iterative.log\n",
      "2025-04-28 23:24:30,722 - INFO - Starting Custom Iterative Imputer with method=ExtraTrees on input DataFrame of shape (450, 345).\n",
      "2025-04-28 23:24:30,737 - INFO - Starting iteration 1/3\n",
      "2025-04-28 23:24:46,313 - INFO - Ending iteration 1/3\n",
      "2025-04-28 23:24:46,314 - INFO - Change: 446884.625000, Scaled Tolerance: 0.051366\n",
      "2025-04-28 23:24:46,956 - INFO - Iteration 1 completed and saved to CSV/exports/CRI-02/o1_impute_baselines/01_custom_iterative/imputed_round_1.csv\n",
      "2025-04-28 23:24:46,958 - INFO - Starting iteration 2/3\n",
      "2025-04-28 23:25:02,331 - INFO - Ending iteration 2/3\n",
      "2025-04-28 23:25:02,332 - INFO - Change: 364468.812500, Scaled Tolerance: 0.052348\n",
      "2025-04-28 23:25:02,925 - INFO - Iteration 2 completed and saved to CSV/exports/CRI-02/o1_impute_baselines/01_custom_iterative/imputed_round_2.csv\n",
      "2025-04-28 23:25:02,926 - INFO - Starting iteration 3/3\n",
      "2025-04-28 23:25:18,414 - INFO - Ending iteration 3/3\n",
      "2025-04-28 23:25:18,415 - INFO - Change: 252314.984375, Scaled Tolerance: 0.051758\n",
      "2025-04-28 23:25:19,178 - INFO - Iteration 3 completed and saved to CSV/exports/CRI-02/o1_impute_baselines/01_custom_iterative/imputed_round_3.csv\n",
      "2025-04-28 23:25:19,179 - INFO - Custom Iterative Imputer completed in 48.44 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Switch to new log file if needed\n",
    "switch_log_file('logs/CIR-24_Custom_Iterative.log')\n",
    "\n",
    "# Call custom iterative imputer\n",
    "final_imputed_df = custom_iterative_imputer(\n",
    "    input_df=small_data,\n",
    "    method=\"ExtraTrees\",  # or \"HistGradientBoosting\", \"BayesianRidge\"\n",
    "    output_folder=\"CSV/exports/CRI-02/o1_impute_baselines/01_custom_iterative/\",\n",
    "    n_iter=3  # for example\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83768d-d6b1-4648-a300-ab0881aacebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72bc28b-44c2-4132-84fd-54f098e8193f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885d82f-d028-463d-92ce-b7922eeb3be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ef1c5-40e7-4ecb-b034-ff618e523f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5420a-ee4a-4bca-abb9-049cea796f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e0515-24c3-496c-b08e-5ee87ef45fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c4eea8-c941-4c41-9ca3-909608f46ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcdc622-7dc7-4d1a-8b83-702343af0b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a258f36a-aaea-4318-8dac-f79500b7641c",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f7d4e-16ff-4c9d-aa90-efa3ba076a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_before_after_distributions(input_df_before, input_df_after, output_folder, sample_features=None):\n",
    "    \"\"\"\n",
    "    Plot before and after distributions for selected features and save the figures.\n",
    "    \n",
    "    If sample_features is None, it plots for all features.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    features = list(input_df_before.columns)\n",
    "\n",
    "    # If sample_features is None ➔ use all features\n",
    "    if sample_features is None:\n",
    "        sampled_features = features\n",
    "    else:\n",
    "        sampled_features = random.sample(features, min(sample_features, len(features)))\n",
    "    \n",
    "    logging.info(f\"Plotting distributions for features: {sampled_features}\")\n",
    "\n",
    "    for feature in sampled_features:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        \n",
    "        plt.hist(input_df_before[feature].dropna(), bins=50, alpha=0.5, label='Before Imputation')\n",
    "        plt.hist(input_df_after[feature].dropna(), bins=50, alpha=0.5, label='After Imputation')\n",
    "        \n",
    "        plt.title(f'Distribution of {feature}')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = os.path.join(output_folder, f\"{feature}_distribution_comparison.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "    \n",
    "    logging.info(f\"Distribution plots saved in {output_folder}\")\n",
    "\n",
    "def check_extreme_values(input_df_before, input_df_after, threshold=5.0):\n",
    "    \"\"\"\n",
    "    Check for extreme outliers introduced after imputation.\n",
    "    \"\"\"\n",
    "    suspicious_features = []\n",
    "\n",
    "    for feature in input_df_before.columns:\n",
    "        before_max = input_df_before[feature].max()\n",
    "        before_min = input_df_before[feature].min()\n",
    "        after_max = input_df_after[feature].max()\n",
    "        after_min = input_df_after[feature].min()\n",
    "\n",
    "        if before_max != 0 and (after_max > threshold * before_max or after_max < before_max / threshold):\n",
    "            suspicious_features.append((feature, 'max', before_max, after_max))\n",
    "        if before_min != 0 and (after_min < before_min / threshold or after_min > threshold * before_min):\n",
    "            suspicious_features.append((feature, 'min', before_min, after_min))\n",
    "\n",
    "    suspicious_df = pd.DataFrame(\n",
    "        suspicious_features, \n",
    "        columns=[\"Feature\", \"Type\", \"Before_Value\", \"After_Value\"]\n",
    "    )\n",
    "\n",
    "    if not suspicious_df.empty:\n",
    "        logging.warning(f\"Found {len(suspicious_df)} suspicious extreme values after imputation!\")\n",
    "\n",
    "    return suspicious_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f9e35-ed9a-40b4-9f1c-cfc039c9c6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load imputed small data\n",
    "data_imputed = pd.read_csv(f\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/{dataset}_Iterative_{method_imputed}.csv\")\n",
    "\n",
    "# Plot distributions before vs after\n",
    "plot_before_after_distributions(\n",
    "    input_df_before=dataset,\n",
    "    input_df_after=data_imputed,\n",
    "    output_folder=f\"figures/CRI-02/o1_impute_baselines/01_iterative/{method_imputed}/\",\n",
    ")\n",
    "\n",
    "# Check for extreme values\n",
    "extreme_values_df = check_extreme_values(\n",
    "    input_df_before=dataset,\n",
    "    input_df_after=data_imputed,\n",
    "    threshold=5.0\n",
    ")\n",
    "\n",
    "# Save report if needed\n",
    "if not extreme_values_df.empty:\n",
    "    extreme_values_df.to_csv(f\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/{dataset}_{method_imputed}_extreme_values.csv\", index=False)\n",
    "    logging.info(\"Extreme values report saved.\")\n",
    "else:\n",
    "    logging.info(\"No suspicious extreme values detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c78939-6284-4678-b96b-e3f5b5f3e974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfecee0-7ba0-448f-b8a8-b57b4ba78611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2c875-ed40-40e5-92c2-ea98c8620883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59963f-0645-4793-884e-c599f3db5a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05344bc-a28e-49a7-88e5-4d81593f986c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ec141-e699-4175-a1d9-376d5005c45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e00c6-2cbb-4502-ac5b-53852171c254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973dbc9e-873c-46f7-9348-c34c61a7227f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4be49b3-1296-4d56-8929-759a26e94a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small subset of the dataframe (for faster testing)\n",
    "small_data = o4_X_train.iloc[:450, :]  # pick first 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120c043-4159-43aa-885a-cb76ffce5d60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Play with sample\n",
    "\"\"\"\n",
    "\n",
    "# Create a small subset of the dataframe (for faster testing)\n",
    "small_data = o4_X_train.iloc[:450, :]  # pick first 50 features\n",
    "\n",
    "# Test imputation on small_data\n",
    "impute_with_iterative(\n",
    "    input_df=small_data,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/small_o3_X_train_imputed_Iterative_ExtraTrees.csv\",\n",
    "    method=\"ExtraTrees\",\n",
    "    n_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45a9ca-e14f-4584-b5b1-adfaf9abca8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7fe60-5046-41bb-ba5f-ab40927a0913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f67544-05ba-4450-817c-2878cba95ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f58b0e-7364-4b2b-b4eb-75ba0ae04485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_before_after_distributions(input_df_before, input_df_after, output_folder, sample_features=10):\n",
    "    \"\"\"\n",
    "    Plot before and after distributions for random features and save the figures.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    features = list(input_df_before.columns)\n",
    "    sampled_features = random.sample(features, min(sample_features, len(features)))\n",
    "    \n",
    "    logging.info(f\"Plotting distributions for features: {sampled_features}\")\n",
    "\n",
    "    for feature in sampled_features:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        \n",
    "        plt.hist(input_df_before[feature].dropna(), bins=50, alpha=0.5, label='Before Imputation')\n",
    "        plt.hist(input_df_after[feature].dropna(), bins=50, alpha=0.5, label='After Imputation')\n",
    "        \n",
    "        plt.title(f'Distribution of {feature}')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = os.path.join(output_folder, f\"{feature}_distribution_comparison.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "    \n",
    "    logging.info(f\"Distribution plots saved in {output_folder}\")\n",
    "\n",
    "def check_extreme_values(input_df_before, input_df_after, threshold=5.0):\n",
    "    \"\"\"\n",
    "    Check for extreme outliers introduced after imputation.\n",
    "    \"\"\"\n",
    "    suspicious_features = []\n",
    "\n",
    "    for feature in input_df_before.columns:\n",
    "        before_max = input_df_before[feature].max()\n",
    "        before_min = input_df_before[feature].min()\n",
    "        after_max = input_df_after[feature].max()\n",
    "        after_min = input_df_after[feature].min()\n",
    "\n",
    "        if before_max != 0 and (after_max > threshold * before_max or after_max < before_max / threshold):\n",
    "            suspicious_features.append((feature, 'max', before_max, after_max))\n",
    "        if before_min != 0 and (after_min < before_min / threshold or after_min > threshold * before_min):\n",
    "            suspicious_features.append((feature, 'min', before_min, after_min))\n",
    "\n",
    "    suspicious_df = pd.DataFrame(\n",
    "        suspicious_features, \n",
    "        columns=[\"Feature\", \"Type\", \"Before_Value\", \"After_Value\"]\n",
    "    )\n",
    "\n",
    "    if not suspicious_df.empty:\n",
    "        logging.warning(f\"Found {len(suspicious_df)} suspicious extreme values after imputation!\")\n",
    "\n",
    "    return suspicious_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9848f-4f3b-4503-87a1-e91d853a19ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721fad44-3e36-4d28-a738-bfa8a9beced3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eeb3f5-3431-4000-9bcf-a51446e2bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load imputed small data\n",
    "small_data_imputed = pd.read_csv(\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/small_o3_X_train_imputed_Iterative_ExtraTrees.csv\")\n",
    "\n",
    "# Plot distributions before vs after\n",
    "plot_before_after_distributions(\n",
    "    input_df_before=small_data,\n",
    "    input_df_after=small_data_imputed,\n",
    "    output_folder=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/plots_small_test/\",\n",
    "    #sample_features=10\n",
    ")\n",
    "\n",
    "# Check for extreme values\n",
    "extreme_values_df = check_extreme_values(\n",
    "    input_df_before=small_data,\n",
    "    input_df_after=small_data_imputed,\n",
    "    threshold=5.0\n",
    ")\n",
    "\n",
    "# Save report if needed\n",
    "if not extreme_values_df.empty:\n",
    "    extreme_values_df.to_csv(\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/extreme_values_report_small_test.csv\", index=False)\n",
    "    logging.info(\"Extreme values report saved.\")\n",
    "else:\n",
    "    logging.info(\"No suspicious extreme values detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301af18-b709-45ca-a8bb-c358e1382c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5a9aa-76e1-498a-97a1-5f3cda66fde5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b457f7-7d80-4009-b0d8-a9beb0718d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc79ac-e045-4fc9-9e6b-3cbed3e8d7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6656c4-c95c-4680-9231-e39a9bdbc63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8599a64-9c3d-4e7d-b303-9f0a676abcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae89814-14f5-47e3-8111-76361fe6ca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8915f50-dd35-41cd-833f-35a51822a47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2966cf1-bb7d-4655-b261-121d2d79062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExtraTrees estimator\n",
    "impute_with_iterative(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o3_X_train_imputed_Iterative_ExtraTrees.csv\",\n",
    "    method=\"ExtraTrees\",\n",
    "    n_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c1554f-a143-4ac7-a8fa-a5a6a0a3b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianRidge estimator\n",
    "impute_with_iterative(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o3_X_train_imputed_Iterative_BayesianRidge.csv\",\n",
    "    method=\"BayesianRidge\",\n",
    "    n_iter=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
