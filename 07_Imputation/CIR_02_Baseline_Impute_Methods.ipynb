{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdebed6-b39e-4d34-b4c3-bda11f83ab2a",
   "metadata": {},
   "source": [
    "# CIR-2 Baseline Imputation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448fce81-0731-4ad1-8f7f-f80619f29358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8893061-af8a-4097-9c7a-e254f73c15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"logs/CIR-22.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6883c12-3115-4f33-a5bc-143d140e7b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 17:14:52,818 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n",
      "2025-04-27 17:14:52,820 - INFO - Start Loading Dataframes.\n",
      "2025-04-27 17:14:52,821 - INFO - Loading... -> o1_X_external.csv\n",
      "2025-04-27 17:14:59,798 - INFO - Loading... -> o1_X_test.csv\n",
      "2025-04-27 17:15:00,337 - INFO - Loading... -> o1_X_train.csv\n",
      "2025-04-27 17:15:04,801 - INFO - Loading... -> o1_X_validate.csv\n",
      "2025-04-27 17:15:05,339 - INFO - Loading... -> o1_y_external_los.csv\n",
      "2025-04-27 17:15:05,382 - INFO - Loading... -> o1_y_external_mortality.csv\n",
      "2025-04-27 17:15:05,410 - INFO - Loading... -> o1_y_test_los.csv\n",
      "2025-04-27 17:15:05,420 - INFO - Loading... -> o1_y_test_mortality.csv\n",
      "2025-04-27 17:15:05,425 - INFO - Loading... -> o1_y_train_los.csv\n",
      "2025-04-27 17:15:05,462 - INFO - Loading... -> o1_y_train_mortality.csv\n",
      "2025-04-27 17:15:05,480 - INFO - Loading... -> o1_y_validate_los.csv\n",
      "2025-04-27 17:15:05,489 - INFO - Loading... -> o1_y_validate_mortality.csv\n",
      "2025-04-27 17:15:05,495 - INFO - Loading... -> o2_X_external.csv\n",
      "2025-04-27 17:15:09,215 - INFO - Loading... -> o2_X_test.csv\n",
      "2025-04-27 17:15:09,503 - INFO - Loading... -> o2_X_train.csv\n",
      "2025-04-27 17:15:11,559 - INFO - Loading... -> o2_X_validate.csv\n",
      "2025-04-27 17:15:11,856 - INFO - Loading... -> o2_y_external_los.csv\n",
      "2025-04-27 17:15:11,879 - INFO - Loading... -> o2_y_external_mortality.csv\n",
      "2025-04-27 17:15:11,896 - INFO - Loading... -> o2_y_test_los.csv\n",
      "2025-04-27 17:15:11,903 - INFO - Loading... -> o2_y_test_mortality.csv\n",
      "2025-04-27 17:15:11,906 - INFO - Loading... -> o2_y_train_los.csv\n",
      "2025-04-27 17:15:11,929 - INFO - Loading... -> o2_y_train_mortality.csv\n",
      "2025-04-27 17:15:11,939 - INFO - Loading... -> o2_y_validate_los.csv\n",
      "2025-04-27 17:15:11,946 - INFO - Loading... -> o2_y_validate_mortality.csv\n",
      "2025-04-27 17:15:11,950 - INFO - Loading... -> o3_X_external.csv\n",
      "2025-04-27 17:15:14,356 - INFO - Loading... -> o3_X_test.csv\n",
      "2025-04-27 17:15:14,542 - INFO - Loading... -> o3_X_train.csv\n",
      "2025-04-27 17:15:15,888 - INFO - Loading... -> o3_X_validate.csv\n",
      "2025-04-27 17:15:16,080 - INFO - Loading... -> o3_y_external_los.csv\n",
      "2025-04-27 17:15:16,098 - INFO - Loading... -> o3_y_external_mortality.csv\n",
      "2025-04-27 17:15:16,111 - INFO - Loading... -> o3_y_test_los.csv\n",
      "2025-04-27 17:15:16,116 - INFO - Loading... -> o3_y_test_mortality.csv\n",
      "2025-04-27 17:15:16,119 - INFO - Loading... -> o3_y_train_los.csv\n",
      "2025-04-27 17:15:16,137 - INFO - Loading... -> o3_y_train_mortality.csv\n",
      "2025-04-27 17:15:16,147 - INFO - Loading... -> o3_y_validate_los.csv\n",
      "2025-04-27 17:15:16,151 - INFO - Loading... -> o3_y_validate_mortality.csv\n",
      "2025-04-27 17:15:16,154 - INFO - Loading... -> o4_X_external.csv\n",
      "2025-04-27 17:15:17,975 - INFO - Loading... -> o4_X_test.csv\n",
      "2025-04-27 17:15:18,133 - INFO - Loading... -> o4_X_train.csv\n",
      "2025-04-27 17:15:19,192 - INFO - Loading... -> o4_X_validate.csv\n",
      "2025-04-27 17:15:19,347 - INFO - Loading... -> o4_y_external_los.csv\n",
      "2025-04-27 17:15:19,362 - INFO - Loading... -> o4_y_external_mortality.csv\n",
      "2025-04-27 17:15:19,372 - INFO - Loading... -> o4_y_test_los.csv\n",
      "2025-04-27 17:15:19,378 - INFO - Loading... -> o4_y_test_mortality.csv\n",
      "2025-04-27 17:15:19,381 - INFO - Loading... -> o4_y_train_los.csv\n",
      "2025-04-27 17:15:19,398 - INFO - Loading... -> o4_y_train_mortality.csv\n",
      "2025-04-27 17:15:19,404 - INFO - Loading... -> o4_y_validate_los.csv\n",
      "2025-04-27 17:15:19,409 - INFO - Loading... -> o4_y_validate_mortality.csv\n",
      "2025-04-27 17:15:19,413 - INFO - o1_X_external loaded successfully with shape (234720, 345)\n",
      "2025-04-27 17:15:19,414 - INFO - o1_X_test loaded successfully with shape (15312, 345)\n",
      "2025-04-27 17:15:19,415 - INFO - o1_X_train loaded successfully with shape (122496, 345)\n",
      "2025-04-27 17:15:19,416 - INFO - o1_X_validate loaded successfully with shape (15312, 345)\n",
      "2025-04-27 17:15:19,417 - INFO - o1_y_external_los loaded successfully with shape (234720, 1)\n",
      "2025-04-27 17:15:19,419 - INFO - o1_y_external_mortality loaded successfully with shape (234720, 1)\n",
      "2025-04-27 17:15:19,419 - INFO - o1_y_test_los loaded successfully with shape (15312, 1)\n",
      "2025-04-27 17:15:19,421 - INFO - o1_y_test_mortality loaded successfully with shape (15312, 1)\n",
      "2025-04-27 17:15:19,421 - INFO - o1_y_train_los loaded successfully with shape (122496, 1)\n",
      "2025-04-27 17:15:19,422 - INFO - o1_y_train_mortality loaded successfully with shape (122496, 1)\n",
      "2025-04-27 17:15:19,423 - INFO - o1_y_validate_los loaded successfully with shape (15312, 1)\n",
      "2025-04-27 17:15:19,426 - INFO - o1_y_validate_mortality loaded successfully with shape (15312, 1)\n",
      "2025-04-27 17:15:19,427 - INFO - o2_X_external loaded successfully with shape (117360, 345)\n",
      "2025-04-27 17:15:19,428 - INFO - o2_X_test loaded successfully with shape (7656, 345)\n",
      "2025-04-27 17:15:19,429 - INFO - o2_X_train loaded successfully with shape (61248, 345)\n",
      "2025-04-27 17:15:19,430 - INFO - o2_X_validate loaded successfully with shape (7656, 345)\n",
      "2025-04-27 17:15:19,431 - INFO - o2_y_external_los loaded successfully with shape (117360, 1)\n",
      "2025-04-27 17:15:19,432 - INFO - o2_y_external_mortality loaded successfully with shape (117360, 1)\n",
      "2025-04-27 17:15:19,433 - INFO - o2_y_test_los loaded successfully with shape (7656, 1)\n",
      "2025-04-27 17:15:19,434 - INFO - o2_y_test_mortality loaded successfully with shape (7656, 1)\n",
      "2025-04-27 17:15:19,435 - INFO - o2_y_train_los loaded successfully with shape (61248, 1)\n",
      "2025-04-27 17:15:19,436 - INFO - o2_y_train_mortality loaded successfully with shape (61248, 1)\n",
      "2025-04-27 17:15:19,437 - INFO - o2_y_validate_los loaded successfully with shape (7656, 1)\n",
      "2025-04-27 17:15:19,438 - INFO - o2_y_validate_mortality loaded successfully with shape (7656, 1)\n",
      "2025-04-27 17:15:19,442 - INFO - o3_X_external loaded successfully with shape (78240, 345)\n",
      "2025-04-27 17:15:19,443 - INFO - o3_X_test loaded successfully with shape (5104, 345)\n",
      "2025-04-27 17:15:19,444 - INFO - o3_X_train loaded successfully with shape (40832, 345)\n",
      "2025-04-27 17:15:19,445 - INFO - o3_X_validate loaded successfully with shape (5104, 345)\n",
      "2025-04-27 17:15:19,446 - INFO - o3_y_external_los loaded successfully with shape (78240, 1)\n",
      "2025-04-27 17:15:19,446 - INFO - o3_y_external_mortality loaded successfully with shape (78240, 1)\n",
      "2025-04-27 17:15:19,447 - INFO - o3_y_test_los loaded successfully with shape (5104, 1)\n",
      "2025-04-27 17:15:19,448 - INFO - o3_y_test_mortality loaded successfully with shape (5104, 1)\n",
      "2025-04-27 17:15:19,449 - INFO - o3_y_train_los loaded successfully with shape (40832, 1)\n",
      "2025-04-27 17:15:19,450 - INFO - o3_y_train_mortality loaded successfully with shape (40832, 1)\n",
      "2025-04-27 17:15:19,451 - INFO - o3_y_validate_los loaded successfully with shape (5104, 1)\n",
      "2025-04-27 17:15:19,452 - INFO - o3_y_validate_mortality loaded successfully with shape (5104, 1)\n",
      "2025-04-27 17:15:19,452 - INFO - o4_X_external loaded successfully with shape (58680, 345)\n",
      "2025-04-27 17:15:19,453 - INFO - o4_X_test loaded successfully with shape (3828, 345)\n",
      "2025-04-27 17:15:19,454 - INFO - o4_X_train loaded successfully with shape (30624, 345)\n",
      "2025-04-27 17:15:19,455 - INFO - o4_X_validate loaded successfully with shape (3828, 345)\n",
      "2025-04-27 17:15:19,456 - INFO - o4_y_external_los loaded successfully with shape (58680, 1)\n",
      "2025-04-27 17:15:19,456 - INFO - o4_y_external_mortality loaded successfully with shape (58680, 1)\n",
      "2025-04-27 17:15:19,458 - INFO - o4_y_test_los loaded successfully with shape (3828, 1)\n",
      "2025-04-27 17:15:19,459 - INFO - o4_y_test_mortality loaded successfully with shape (3828, 1)\n",
      "2025-04-27 17:15:19,460 - INFO - o4_y_train_los loaded successfully with shape (30624, 1)\n",
      "2025-04-27 17:15:19,461 - INFO - o4_y_train_mortality loaded successfully with shape (30624, 1)\n",
      "2025-04-27 17:15:19,461 - INFO - o4_y_validate_los loaded successfully with shape (3828, 1)\n",
      "2025-04-27 17:15:19,462 - INFO - o4_y_validate_mortality loaded successfully with shape (3828, 1)\n",
      "2025-04-27 17:15:19,463 - INFO - Load Complete.\n",
      "2025-04-27 17:15:19,464 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "# CSVs Directory \n",
    "data_path = \"../04_ANN/CSV/exports/split_set/without_multiple_rows\"\n",
    "all_files = os.listdir(data_path)\n",
    "\n",
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")\n",
    "logging.info(\"Start Loading Dataframes.\")\n",
    "\n",
    "# Load CSVs into a dictionary of dataframes\n",
    "dataframes = {}\n",
    "for file in all_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        var_name = file.replace(\".csv\", \"\").replace(\"-\", \"_\")\n",
    "        logging.info(f\"Loading... -> {file}\")\n",
    "        dataframes[var_name] = pd.read_csv(os.path.join(data_path, file)).astype('float32')\n",
    "\n",
    "# Log loaded datasets\n",
    "for var_name, df in dataframes.items():\n",
    "    globals()[var_name] = df\n",
    "    logging.info(f\"{var_name} loaded successfully with shape {df.shape}\")\n",
    "logging.info(\"Load Complete.\")\n",
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53cf697-daa1-4e6e-b0a9-868bd8a5dad0",
   "metadata": {},
   "source": [
    "# CIR-23 Implement Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9da711f0-95b4-451c-9cf9-a64e59f55617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imputes missing values in a dataframe using\n",
    "IterativeImputer with ExtraTreesRegressor or BayesianRidge.\n",
    "    \n",
    "Parameters:\n",
    "    input_df (pd.DataFrame): Input dataframe to impute.\n",
    "    output_path (str): Path to save the imputed CSV.\n",
    "    method (str): \"ExtraTrees\" or \"BayesianRidge\".\n",
    "    n_iter (int): Number of max iterations for the imputer.\n",
    "\"\"\"\n",
    "\n",
    "def impute_with_iterative(input_df, output_path, method, n_iter):\n",
    "    # Start\n",
    "    logging.info(f\"Starting Iterative Imputer with method={method} on input DataFrame of shape {input_df.shape}.\")\n",
    "\n",
    "    # Copy input\n",
    "    data_copy = input_df.copy()\n",
    "\n",
    "    # Create output folder if needed\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Choose estimator\n",
    "    if method == \"ExtraTrees\":\n",
    "        estimator = ExtraTreesRegressor(\n",
    "            n_estimators=10,\n",
    "            random_state=0,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    elif method == \"BayesianRidge\":\n",
    "        estimator = BayesianRidge()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}. Use 'ExtraTrees' or 'BayesianRidge'.\")\n",
    "\n",
    "    # Create imputer\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=estimator,\n",
    "        max_iter=n_iter,\n",
    "        random_state=0,\n",
    "        verbose=2,  # Show progress per iteration\n",
    "        sample_posterior=False\n",
    "    )\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fit and transform\n",
    "    imputed_array = imputer.fit_transform(data_copy)\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    imputed_df = pd.DataFrame(\n",
    "        imputed_array,\n",
    "        columns=data_copy.columns\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    imputed_df.to_csv(output_path, index=False)\n",
    "\n",
    "    # Logs\n",
    "    logging.info(f\"Imputation completed in {runtime:.2f} seconds.\")\n",
    "    nan_count = np.isnan(imputed_df.values).sum()\n",
    "    logging.info(f\"Number of NaNs after imputation: {nan_count}\")\n",
    "    logging.info(f\"Imputed dataset saved at {output_path}\")\n",
    "    logging.info(f\"Basic statistics after imputation:\\n{imputed_df.describe()}\")\n",
    "\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862069df-0de8-46a8-928f-a5f7e9c740cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 17:15:28,549 - INFO - Starting Iterative Imputer with method=ExtraTrees on input DataFrame of shape (30624, 345).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (30624, 345)\n"
     ]
    }
   ],
   "source": [
    "# ExtraTrees estimator\n",
    "impute_with_iterative(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o3_X_train_imputed_Iterative_ExtraTrees.csv\",\n",
    "    method=\"ExtraTrees\",\n",
    "    n_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e667bd3d-f845-46b3-9289-264907e280d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianRidge estimator\n",
    "impute_with_iterative(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o3_X_train_imputed_Iterative_BayesianRidge.csv\",\n",
    "    method=\"BayesianRidge\",\n",
    "    n_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258f36a-aaea-4318-8dac-f79500b7641c",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f7d4e-16ff-4c9d-aa90-efa3ba076a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_before_after_distributions(input_df_before, input_df_after, output_folder, sample_features=None):\n",
    "    \"\"\"\n",
    "    Plot before and after distributions for selected features and save the figures.\n",
    "    \n",
    "    If sample_features is None, it plots for all features.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    features = list(input_df_before.columns)\n",
    "\n",
    "    # If sample_features is None ➔ use all features\n",
    "    if sample_features is None:\n",
    "        sampled_features = features\n",
    "    else:\n",
    "        sampled_features = random.sample(features, min(sample_features, len(features)))\n",
    "    \n",
    "    logging.info(f\"Plotting distributions for features: {sampled_features}\")\n",
    "\n",
    "    for feature in sampled_features:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        \n",
    "        plt.hist(input_df_before[feature].dropna(), bins=50, alpha=0.5, label='Before Imputation')\n",
    "        plt.hist(input_df_after[feature].dropna(), bins=50, alpha=0.5, label='After Imputation')\n",
    "        \n",
    "        plt.title(f'Distribution of {feature}')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = os.path.join(output_folder, f\"{feature}_distribution_comparison.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "    \n",
    "    logging.info(f\"Distribution plots saved in {output_folder}\")\n",
    "\n",
    "def check_extreme_values(input_df_before, input_df_after, threshold=5.0):\n",
    "    \"\"\"\n",
    "    Check for extreme outliers introduced after imputation.\n",
    "    \"\"\"\n",
    "    suspicious_features = []\n",
    "\n",
    "    for feature in input_df_before.columns:\n",
    "        before_max = input_df_before[feature].max()\n",
    "        before_min = input_df_before[feature].min()\n",
    "        after_max = input_df_after[feature].max()\n",
    "        after_min = input_df_after[feature].min()\n",
    "\n",
    "        if before_max != 0 and (after_max > threshold * before_max or after_max < before_max / threshold):\n",
    "            suspicious_features.append((feature, 'max', before_max, after_max))\n",
    "        if before_min != 0 and (after_min < before_min / threshold or after_min > threshold * before_min):\n",
    "            suspicious_features.append((feature, 'min', before_min, after_min))\n",
    "\n",
    "    suspicious_df = pd.DataFrame(\n",
    "        suspicious_features, \n",
    "        columns=[\"Feature\", \"Type\", \"Before_Value\", \"After_Value\"]\n",
    "    )\n",
    "\n",
    "    if not suspicious_df.empty:\n",
    "        logging.warning(f\"Found {len(suspicious_df)} suspicious extreme values after imputation!\")\n",
    "\n",
    "    return suspicious_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f9e35-ed9a-40b4-9f1c-cfc039c9c6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c78939-6284-4678-b96b-e3f5b5f3e974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfecee0-7ba0-448f-b8a8-b57b4ba78611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2c875-ed40-40e5-92c2-ea98c8620883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59963f-0645-4793-884e-c599f3db5a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05344bc-a28e-49a7-88e5-4d81593f986c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ec141-e699-4175-a1d9-376d5005c45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e00c6-2cbb-4502-ac5b-53852171c254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973dbc9e-873c-46f7-9348-c34c61a7227f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be49b3-1296-4d56-8929-759a26e94a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120c043-4159-43aa-885a-cb76ffce5d60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Play with sample\n",
    "\"\"\"\n",
    "\n",
    "# Create a small subset of the dataframe (for faster testing)\n",
    "small_data = o4_X_train.iloc[:500, :]  # pick first 50 features\n",
    "\n",
    "# Test imputation on small_data\n",
    "impute_with_iterative(\n",
    "    input_df=small_data,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/small_o3_X_train_imputed_Iterative_ExtraTrees.csv\",\n",
    "    method=\"ExtraTrees\",\n",
    "    n_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45a9ca-e14f-4584-b5b1-adfaf9abca8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7fe60-5046-41bb-ba5f-ab40927a0913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f67544-05ba-4450-817c-2878cba95ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f58b0e-7364-4b2b-b4eb-75ba0ae04485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_before_after_distributions(input_df_before, input_df_after, output_folder, sample_features=10):\n",
    "    \"\"\"\n",
    "    Plot before and after distributions for random features and save the figures.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    features = list(input_df_before.columns)\n",
    "    sampled_features = random.sample(features, min(sample_features, len(features)))\n",
    "    \n",
    "    logging.info(f\"Plotting distributions for features: {sampled_features}\")\n",
    "\n",
    "    for feature in sampled_features:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        \n",
    "        plt.hist(input_df_before[feature].dropna(), bins=50, alpha=0.5, label='Before Imputation')\n",
    "        plt.hist(input_df_after[feature].dropna(), bins=50, alpha=0.5, label='After Imputation')\n",
    "        \n",
    "        plt.title(f'Distribution of {feature}')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = os.path.join(output_folder, f\"{feature}_distribution_comparison.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "    \n",
    "    logging.info(f\"Distribution plots saved in {output_folder}\")\n",
    "\n",
    "def check_extreme_values(input_df_before, input_df_after, threshold=5.0):\n",
    "    \"\"\"\n",
    "    Check for extreme outliers introduced after imputation.\n",
    "    \"\"\"\n",
    "    suspicious_features = []\n",
    "\n",
    "    for feature in input_df_before.columns:\n",
    "        before_max = input_df_before[feature].max()\n",
    "        before_min = input_df_before[feature].min()\n",
    "        after_max = input_df_after[feature].max()\n",
    "        after_min = input_df_after[feature].min()\n",
    "\n",
    "        if before_max != 0 and (after_max > threshold * before_max or after_max < before_max / threshold):\n",
    "            suspicious_features.append((feature, 'max', before_max, after_max))\n",
    "        if before_min != 0 and (after_min < before_min / threshold or after_min > threshold * before_min):\n",
    "            suspicious_features.append((feature, 'min', before_min, after_min))\n",
    "\n",
    "    suspicious_df = pd.DataFrame(\n",
    "        suspicious_features, \n",
    "        columns=[\"Feature\", \"Type\", \"Before_Value\", \"After_Value\"]\n",
    "    )\n",
    "\n",
    "    if not suspicious_df.empty:\n",
    "        logging.warning(f\"Found {len(suspicious_df)} suspicious extreme values after imputation!\")\n",
    "\n",
    "    return suspicious_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9848f-4f3b-4503-87a1-e91d853a19ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721fad44-3e36-4d28-a738-bfa8a9beced3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eeb3f5-3431-4000-9bcf-a51446e2bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load imputed small data\n",
    "small_data_imputed = pd.read_csv(\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/small_o3_X_train_imputed_Iterative_ExtraTrees.csv\")\n",
    "\n",
    "# Plot distributions before vs after\n",
    "plot_before_after_distributions(\n",
    "    input_df_before=small_data,\n",
    "    input_df_after=small_data_imputed,\n",
    "    output_folder=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/plots_small_test/\",\n",
    "    #sample_features=10\n",
    ")\n",
    "\n",
    "# Check for extreme values\n",
    "extreme_values_df = check_extreme_values(\n",
    "    input_df_before=small_data,\n",
    "    input_df_after=small_data_imputed,\n",
    "    threshold=5.0\n",
    ")\n",
    "\n",
    "# Save report if needed\n",
    "if not extreme_values_df.empty:\n",
    "    extreme_values_df.to_csv(\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/extreme_values_report_small_test.csv\", index=False)\n",
    "    logging.info(\"Extreme values report saved.\")\n",
    "else:\n",
    "    logging.info(\"No suspicious extreme values detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301af18-b709-45ca-a8bb-c358e1382c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5a9aa-76e1-498a-97a1-5f3cda66fde5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b457f7-7d80-4009-b0d8-a9beb0718d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc79ac-e045-4fc9-9e6b-3cbed3e8d7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6656c4-c95c-4680-9231-e39a9bdbc63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8599a64-9c3d-4e7d-b303-9f0a676abcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae89814-14f5-47e3-8111-76361fe6ca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8915f50-dd35-41cd-833f-35a51822a47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2966cf1-bb7d-4655-b261-121d2d79062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExtraTrees estimator\n",
    "impute_with_iterative(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o3_X_train_imputed_Iterative_ExtraTrees.csv\",\n",
    "    method=\"ExtraTrees\",\n",
    "    n_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c1554f-a143-4ac7-a8fa-a5a6a0a3b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianRidge estimator\n",
    "impute_with_iterative(\n",
    "    input_df=o4_X_train,\n",
    "    output_path=\"CSV/exports/CRI-02/o1_impute_baselines/01_iterative/o3_X_train_imputed_Iterative_BayesianRidge.csv\",\n",
    "    method=\"BayesianRidge\",\n",
    "    n_iter=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
