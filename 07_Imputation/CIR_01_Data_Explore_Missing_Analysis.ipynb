{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed96430-8fa8-472d-8b11-77f38f7bbde1",
   "metadata": {},
   "source": [
    "# Sprint 1: Foundational Setup\n",
    "\n",
    "## CIR-8: Load and Validate all Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "175f0647-a0e3-45de-8b58-b76fd775d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from math import ceil\n",
    "from matplotlib.patches import Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7c2be1f-1abc-468d-86a6-708f3725f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial logger setup\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Global variable to hold the active file handler\n",
    "current_file_handler = None\n",
    "\n",
    "# Create the stream handler (to console)\n",
    "stream_handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "def switch_log_file(filename):\n",
    "    global current_file_handler\n",
    "\n",
    "    # If a file handler already exists, remove and close it\n",
    "    if current_file_handler:\n",
    "        logger.removeHandler(current_file_handler)\n",
    "        current_file_handler.close()\n",
    "\n",
    "    # Create a new file handler\n",
    "    current_file_handler = logging.FileHandler(filename)\n",
    "    current_file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(current_file_handler)\n",
    "\n",
    "    logger.info(f\"Switched logging to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b2521b-30f2-402c-b430-95ed5ba917a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:17:10,377 - INFO - Switched logging to logs/CIR-8.log\n",
      "2025-04-30 12:17:10,380 - INFO - This is being logged to CIR-8.log\n"
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-8.log')\n",
    "logger.info(\"This is being logged to CIR-8.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "493510c5-0c04-42a5-96aa-e19fa8dcffa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:17:10,393 - INFO - ++++++++++++++++++CIR-1++++++++++++++++++++++++\n",
      "2025-04-30 12:17:10,394 - INFO - Start Loading Dataframes.\n",
      "2025-04-30 12:17:10,395 - INFO - Loading... -> o1_X_external.csv\n",
      "2025-04-30 12:17:17,449 - INFO - Loading... -> o1_X_test.csv\n",
      "2025-04-30 12:17:17,952 - INFO - Loading... -> o1_X_train.csv\n",
      "2025-04-30 12:17:22,051 - INFO - Loading... -> o1_X_validate.csv\n",
      "2025-04-30 12:17:22,585 - INFO - Loading... -> o1_y_external_los.csv\n",
      "2025-04-30 12:17:22,627 - INFO - Loading... -> o1_y_external_mortality.csv\n",
      "2025-04-30 12:17:22,656 - INFO - Loading... -> o1_y_test_los.csv\n",
      "2025-04-30 12:17:22,666 - INFO - Loading... -> o1_y_test_mortality.csv\n",
      "2025-04-30 12:17:22,671 - INFO - Loading... -> o1_y_train_los.csv\n",
      "2025-04-30 12:17:22,707 - INFO - Loading... -> o1_y_train_mortality.csv\n",
      "2025-04-30 12:17:22,725 - INFO - Loading... -> o1_y_validate_los.csv\n",
      "2025-04-30 12:17:22,736 - INFO - Loading... -> o1_y_validate_mortality.csv\n",
      "2025-04-30 12:17:22,741 - INFO - Loading... -> o2_X_external.csv\n",
      "2025-04-30 12:17:26,362 - INFO - Loading... -> o2_X_test.csv\n",
      "2025-04-30 12:17:26,623 - INFO - Loading... -> o2_X_train.csv\n",
      "2025-04-30 12:17:28,899 - INFO - Loading... -> o2_X_validate.csv\n",
      "2025-04-30 12:17:29,199 - INFO - Loading... -> o2_y_external_los.csv\n",
      "2025-04-30 12:17:29,236 - INFO - Loading... -> o2_y_external_mortality.csv\n",
      "2025-04-30 12:17:29,254 - INFO - Loading... -> o2_y_test_los.csv\n",
      "2025-04-30 12:17:29,261 - INFO - Loading... -> o2_y_test_mortality.csv\n",
      "2025-04-30 12:17:29,267 - INFO - Loading... -> o2_y_train_los.csv\n",
      "2025-04-30 12:17:29,299 - INFO - Loading... -> o2_y_train_mortality.csv\n",
      "2025-04-30 12:17:29,311 - INFO - Loading... -> o2_y_validate_los.csv\n",
      "2025-04-30 12:17:29,319 - INFO - Loading... -> o2_y_validate_mortality.csv\n",
      "2025-04-30 12:17:29,324 - INFO - Loading... -> o3_X_external.csv\n",
      "2025-04-30 12:17:31,763 - INFO - Loading... -> o3_X_test.csv\n",
      "2025-04-30 12:17:31,948 - INFO - Loading... -> o3_X_train.csv\n",
      "2025-04-30 12:17:33,267 - INFO - Loading... -> o3_X_validate.csv\n",
      "2025-04-30 12:17:33,473 - INFO - Loading... -> o3_y_external_los.csv\n",
      "2025-04-30 12:17:33,492 - INFO - Loading... -> o3_y_external_mortality.csv\n",
      "2025-04-30 12:17:33,505 - INFO - Loading... -> o3_y_test_los.csv\n",
      "2025-04-30 12:17:33,510 - INFO - Loading... -> o3_y_test_mortality.csv\n",
      "2025-04-30 12:17:33,514 - INFO - Loading... -> o3_y_train_los.csv\n",
      "2025-04-30 12:17:33,531 - INFO - Loading... -> o3_y_train_mortality.csv\n",
      "2025-04-30 12:17:33,539 - INFO - Loading... -> o3_y_validate_los.csv\n",
      "2025-04-30 12:17:33,544 - INFO - Loading... -> o3_y_validate_mortality.csv\n",
      "2025-04-30 12:17:33,548 - INFO - Loading... -> o4_X_external.csv\n",
      "2025-04-30 12:17:35,307 - INFO - Loading... -> o4_X_test.csv\n",
      "2025-04-30 12:17:35,456 - INFO - Loading... -> o4_X_train.csv\n",
      "2025-04-30 12:17:36,453 - INFO - Loading... -> o4_X_validate.csv\n",
      "2025-04-30 12:17:36,601 - INFO - Loading... -> o4_y_external_los.csv\n",
      "2025-04-30 12:17:36,616 - INFO - Loading... -> o4_y_external_mortality.csv\n",
      "2025-04-30 12:17:36,627 - INFO - Loading... -> o4_y_test_los.csv\n",
      "2025-04-30 12:17:36,631 - INFO - Loading... -> o4_y_test_mortality.csv\n",
      "2025-04-30 12:17:36,636 - INFO - Loading... -> o4_y_train_los.csv\n",
      "2025-04-30 12:17:36,652 - INFO - Loading... -> o4_y_train_mortality.csv\n",
      "2025-04-30 12:17:36,661 - INFO - Loading... -> o4_y_validate_los.csv\n",
      "2025-04-30 12:17:36,667 - INFO - Loading... -> o4_y_validate_mortality.csv\n",
      "2025-04-30 12:17:36,672 - INFO - o1_X_external loaded successfully with shape (234720, 345)\n",
      "2025-04-30 12:17:36,673 - INFO - o1_X_test loaded successfully with shape (15312, 345)\n",
      "2025-04-30 12:17:36,674 - INFO - o1_X_train loaded successfully with shape (122496, 345)\n",
      "2025-04-30 12:17:36,674 - INFO - o1_X_validate loaded successfully with shape (15312, 345)\n",
      "2025-04-30 12:17:36,675 - INFO - o1_y_external_los loaded successfully with shape (234720, 1)\n",
      "2025-04-30 12:17:36,676 - INFO - o1_y_external_mortality loaded successfully with shape (234720, 1)\n",
      "2025-04-30 12:17:36,677 - INFO - o1_y_test_los loaded successfully with shape (15312, 1)\n",
      "2025-04-30 12:17:36,678 - INFO - o1_y_test_mortality loaded successfully with shape (15312, 1)\n",
      "2025-04-30 12:17:36,679 - INFO - o1_y_train_los loaded successfully with shape (122496, 1)\n",
      "2025-04-30 12:17:36,680 - INFO - o1_y_train_mortality loaded successfully with shape (122496, 1)\n",
      "2025-04-30 12:17:36,680 - INFO - o1_y_validate_los loaded successfully with shape (15312, 1)\n",
      "2025-04-30 12:17:36,682 - INFO - o1_y_validate_mortality loaded successfully with shape (15312, 1)\n",
      "2025-04-30 12:17:36,683 - INFO - o2_X_external loaded successfully with shape (117360, 345)\n",
      "2025-04-30 12:17:36,684 - INFO - o2_X_test loaded successfully with shape (7656, 345)\n",
      "2025-04-30 12:17:36,684 - INFO - o2_X_train loaded successfully with shape (61248, 345)\n",
      "2025-04-30 12:17:36,685 - INFO - o2_X_validate loaded successfully with shape (7656, 345)\n",
      "2025-04-30 12:17:36,686 - INFO - o2_y_external_los loaded successfully with shape (117360, 1)\n",
      "2025-04-30 12:17:36,686 - INFO - o2_y_external_mortality loaded successfully with shape (117360, 1)\n",
      "2025-04-30 12:17:36,687 - INFO - o2_y_test_los loaded successfully with shape (7656, 1)\n",
      "2025-04-30 12:17:36,688 - INFO - o2_y_test_mortality loaded successfully with shape (7656, 1)\n",
      "2025-04-30 12:17:36,689 - INFO - o2_y_train_los loaded successfully with shape (61248, 1)\n",
      "2025-04-30 12:17:36,691 - INFO - o2_y_train_mortality loaded successfully with shape (61248, 1)\n",
      "2025-04-30 12:17:36,692 - INFO - o2_y_validate_los loaded successfully with shape (7656, 1)\n",
      "2025-04-30 12:17:36,692 - INFO - o2_y_validate_mortality loaded successfully with shape (7656, 1)\n",
      "2025-04-30 12:17:36,694 - INFO - o3_X_external loaded successfully with shape (78240, 345)\n",
      "2025-04-30 12:17:36,694 - INFO - o3_X_test loaded successfully with shape (5104, 345)\n",
      "2025-04-30 12:17:36,695 - INFO - o3_X_train loaded successfully with shape (40832, 345)\n",
      "2025-04-30 12:17:36,696 - INFO - o3_X_validate loaded successfully with shape (5104, 345)\n",
      "2025-04-30 12:17:36,698 - INFO - o3_y_external_los loaded successfully with shape (78240, 1)\n",
      "2025-04-30 12:17:36,699 - INFO - o3_y_external_mortality loaded successfully with shape (78240, 1)\n",
      "2025-04-30 12:17:36,700 - INFO - o3_y_test_los loaded successfully with shape (5104, 1)\n",
      "2025-04-30 12:17:36,701 - INFO - o3_y_test_mortality loaded successfully with shape (5104, 1)\n",
      "2025-04-30 12:17:36,701 - INFO - o3_y_train_los loaded successfully with shape (40832, 1)\n",
      "2025-04-30 12:17:36,702 - INFO - o3_y_train_mortality loaded successfully with shape (40832, 1)\n",
      "2025-04-30 12:17:36,703 - INFO - o3_y_validate_los loaded successfully with shape (5104, 1)\n",
      "2025-04-30 12:17:36,704 - INFO - o3_y_validate_mortality loaded successfully with shape (5104, 1)\n",
      "2025-04-30 12:17:36,705 - INFO - o4_X_external loaded successfully with shape (58680, 345)\n",
      "2025-04-30 12:17:36,705 - INFO - o4_X_test loaded successfully with shape (3828, 345)\n",
      "2025-04-30 12:17:36,707 - INFO - o4_X_train loaded successfully with shape (30624, 345)\n",
      "2025-04-30 12:17:36,708 - INFO - o4_X_validate loaded successfully with shape (3828, 345)\n",
      "2025-04-30 12:17:36,708 - INFO - o4_y_external_los loaded successfully with shape (58680, 1)\n",
      "2025-04-30 12:17:36,709 - INFO - o4_y_external_mortality loaded successfully with shape (58680, 1)\n",
      "2025-04-30 12:17:36,710 - INFO - o4_y_test_los loaded successfully with shape (3828, 1)\n",
      "2025-04-30 12:17:36,711 - INFO - o4_y_test_mortality loaded successfully with shape (3828, 1)\n",
      "2025-04-30 12:17:36,711 - INFO - o4_y_train_los loaded successfully with shape (30624, 1)\n",
      "2025-04-30 12:17:36,712 - INFO - o4_y_train_mortality loaded successfully with shape (30624, 1)\n",
      "2025-04-30 12:17:36,713 - INFO - o4_y_validate_los loaded successfully with shape (3828, 1)\n",
      "2025-04-30 12:17:36,713 - INFO - o4_y_validate_mortality loaded successfully with shape (3828, 1)\n",
      "2025-04-30 12:17:36,715 - INFO - Load Complete.\n",
      "2025-04-30 12:17:36,715 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "# CSVs Directory \n",
    "data_path = \"../04_ANN/CSV/exports/split_set/without_multiple_rows\"\n",
    "all_files = os.listdir(data_path)\n",
    "\n",
    "logging.info(\"++++++++++++++++++CIR-1++++++++++++++++++++++++\")\n",
    "logging.info(\"Start Loading Dataframes.\")\n",
    "\n",
    "# Load CSVs into a dictionary of dataframes\n",
    "dataframes = {}\n",
    "for file in all_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        var_name = file.replace(\".csv\", \"\").replace(\"-\", \"_\")\n",
    "        logging.info(f\"Loading... -> {file}\")\n",
    "        dataframes[var_name] = pd.read_csv(os.path.join(data_path, file)).astype('float32')\n",
    "\n",
    "# Log loaded datasets\n",
    "for var_name, df in dataframes.items():\n",
    "    globals()[var_name] = df\n",
    "    logging.info(f\"{var_name} loaded successfully with shape {df.shape}\")\n",
    "logging.info(\"Load Complete.\")\n",
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cceb6a9-3c44-4c6a-b426-a426744ff4a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:17:36,723 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n",
      "2025-04-30 12:17:36,724 - INFO - Check Datatypes...\n",
      "2025-04-30 12:17:36,727 - INFO - o1_X_external types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,730 - INFO - o1_X_test types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,732 - INFO - o1_X_train types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,735 - INFO - o1_X_validate types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,737 - INFO - o1_y_external_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,742 - INFO - o1_y_external_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,746 - INFO - o1_y_test_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,750 - INFO - o1_y_test_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,751 - INFO - o1_y_train_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,753 - INFO - o1_y_train_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,755 - INFO - o1_y_validate_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,757 - INFO - o1_y_validate_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,759 - INFO - o2_X_external types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,760 - INFO - o2_X_test types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,762 - INFO - o2_X_train types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,766 - INFO - o2_X_validate types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,768 - INFO - o2_y_external_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,770 - INFO - o2_y_external_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,771 - INFO - o2_y_test_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,773 - INFO - o2_y_test_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,775 - INFO - o2_y_train_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,777 - INFO - o2_y_train_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,779 - INFO - o2_y_validate_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,781 - INFO - o2_y_validate_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,783 - INFO - o3_X_external types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,785 - INFO - o3_X_test types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,787 - INFO - o3_X_train types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,789 - INFO - o3_X_validate types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,791 - INFO - o3_y_external_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,793 - INFO - o3_y_external_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,795 - INFO - o3_y_test_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,797 - INFO - o3_y_test_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,799 - INFO - o3_y_train_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,801 - INFO - o3_y_train_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,803 - INFO - o3_y_validate_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,805 - INFO - o3_y_validate_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,807 - INFO - o4_X_external types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,808 - INFO - o4_X_test types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,811 - INFO - o4_X_train types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,812 - INFO - o4_X_validate types:\n",
      "float32    345\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,814 - INFO - o4_y_external_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,816 - INFO - o4_y_external_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,820 - INFO - o4_y_test_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,823 - INFO - o4_y_test_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,825 - INFO - o4_y_train_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,827 - INFO - o4_y_train_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,829 - INFO - o4_y_validate_los types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,830 - INFO - o4_y_validate_mortality types:\n",
      "float32    1\n",
      "Name: count, dtype: int64\n",
      "2025-04-30 12:17:36,835 - INFO - Check Complete.\n",
      "2025-04-30 12:17:36,836 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")\n",
    "logging.info(\"Check Datatypes...\")\n",
    "for name, df in dataframes.items():\n",
    "    logging.info(f\"{name} types:\\n{df.dtypes.value_counts()}\")\n",
    "logging.info(\"Check Complete.\")\n",
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4f3d906-97b9-454f-9d49-32d2870f43b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:17:36,847 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n",
      "2025-04-30 12:17:36,864 - INFO - Calculate missing values...\n",
      "2025-04-30 12:17:37,062 - INFO - o1_X_external: 39527616 missing values across 308 columns\n",
      "2025-04-30 12:17:37,077 - INFO - o1_X_test: 1985664 missing values across 292 columns\n",
      "2025-04-30 12:17:37,189 - INFO - o1_X_train: 15876288 missing values across 304 columns\n",
      "2025-04-30 12:17:37,206 - INFO - o1_X_validate: 2007696 missing values across 296 columns\n",
      "2025-04-30 12:17:37,208 - INFO - o1_y_external_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,211 - INFO - o1_y_external_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,213 - INFO - o1_y_test_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,215 - INFO - o1_y_test_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,219 - INFO - o1_y_train_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,221 - INFO - o1_y_train_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,224 - INFO - o1_y_validate_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,226 - INFO - o1_y_validate_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,346 - INFO - o2_X_external: 19763572 missing values across 308 columns\n",
      "2025-04-30 12:17:37,355 - INFO - o2_X_test: 992832 missing values across 292 columns\n",
      "2025-04-30 12:17:37,415 - INFO - o2_X_train: 7938144 missing values across 304 columns\n",
      "2025-04-30 12:17:37,424 - INFO - o2_X_validate: 1003848 missing values across 296 columns\n",
      "2025-04-30 12:17:37,427 - INFO - o2_y_external_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,429 - INFO - o2_y_external_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,430 - INFO - o2_y_test_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,433 - INFO - o2_y_test_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,435 - INFO - o2_y_train_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,438 - INFO - o2_y_train_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,440 - INFO - o2_y_validate_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,442 - INFO - o2_y_validate_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,508 - INFO - o3_X_external: 13175808 missing values across 308 columns\n",
      "2025-04-30 12:17:37,515 - INFO - o3_X_test: 661888 missing values across 292 columns\n",
      "2025-04-30 12:17:37,551 - INFO - o3_X_train: 5292096 missing values across 304 columns\n",
      "2025-04-30 12:17:37,558 - INFO - o3_X_validate: 669232 missing values across 296 columns\n",
      "2025-04-30 12:17:37,560 - INFO - o3_y_external_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,564 - INFO - o3_y_external_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,568 - INFO - o3_y_test_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,570 - INFO - o3_y_test_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,572 - INFO - o3_y_train_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,575 - INFO - o3_y_train_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,576 - INFO - o3_y_validate_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,578 - INFO - o3_y_validate_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,633 - INFO - o4_X_external: 9881900 missing values across 308 columns\n",
      "2025-04-30 12:17:37,639 - INFO - o4_X_test: 496416 missing values across 292 columns\n",
      "2025-04-30 12:17:37,667 - INFO - o4_X_train: 3969072 missing values across 304 columns\n",
      "2025-04-30 12:17:37,673 - INFO - o4_X_validate: 501924 missing values across 296 columns\n",
      "2025-04-30 12:17:37,676 - INFO - o4_y_external_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,678 - INFO - o4_y_external_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,682 - INFO - o4_y_test_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,685 - INFO - o4_y_test_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,687 - INFO - o4_y_train_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,690 - INFO - o4_y_train_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,692 - INFO - o4_y_validate_los: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,694 - INFO - o4_y_validate_mortality: 0 missing values across 0 columns\n",
      "2025-04-30 12:17:37,695 - INFO - Missing values calculation complete.\n",
      "2025-04-30 12:17:37,696 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")\n",
    "logging.info(\"Calculate missing values...\")\n",
    "for name, df in dataframes.items():\n",
    "    missing_total = df.isnull().sum().sum()\n",
    "    missing_cols = df.columns[df.isnull().any()].tolist()\n",
    "    logging.info(f\"{name}: {missing_total} missing values across {len(missing_cols)} columns\")\n",
    "logging.info(\"Missing values calculation complete.\")\n",
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ecba4ed-379f-4f2b-90f7-909708c44cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:17:37,709 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n",
      "2025-04-30 12:17:37,711 - INFO - Calculate unique values.\n",
      "2025-04-30 12:17:37,720 - INFO - o1_y_external_los unique target values: {'los': 830}\n",
      "2025-04-30 12:17:37,724 - INFO - o1_y_external_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,726 - INFO - o1_y_test_los unique target values: {'los': 319}\n",
      "2025-04-30 12:17:37,728 - INFO - o1_y_test_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,732 - INFO - o1_y_train_los unique target values: {'los': 2548}\n",
      "2025-04-30 12:17:37,736 - INFO - o1_y_train_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,739 - INFO - o1_y_validate_los unique target values: {'los': 319}\n",
      "2025-04-30 12:17:37,743 - INFO - o1_y_validate_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,746 - INFO - o2_y_external_los unique target values: {'los': 830}\n",
      "2025-04-30 12:17:37,750 - INFO - o2_y_external_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,752 - INFO - o2_y_test_los unique target values: {'los': 319}\n",
      "2025-04-30 12:17:37,753 - INFO - o2_y_test_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,756 - INFO - o2_y_train_los unique target values: {'los': 2548}\n",
      "2025-04-30 12:17:37,758 - INFO - o2_y_train_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,759 - INFO - o2_y_validate_los unique target values: {'los': 319}\n",
      "2025-04-30 12:17:37,761 - INFO - o2_y_validate_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,763 - INFO - o3_y_external_los unique target values: {'los': 830}\n",
      "2025-04-30 12:17:37,766 - INFO - o3_y_external_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,768 - INFO - o3_y_test_los unique target values: {'los': 319}\n",
      "2025-04-30 12:17:37,770 - INFO - o3_y_test_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,773 - INFO - o3_y_train_los unique target values: {'los': 2548}\n",
      "2025-04-30 12:17:37,775 - INFO - o3_y_train_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,777 - INFO - o3_y_validate_los unique target values: {'los': 319}\n",
      "2025-04-30 12:17:37,778 - INFO - o3_y_validate_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,782 - INFO - o4_y_external_los unique target values: {'los': 830}\n",
      "2025-04-30 12:17:37,784 - INFO - o4_y_external_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,786 - INFO - o4_y_test_los unique target values: {'los': 319}\n",
      "2025-04-30 12:17:37,787 - INFO - o4_y_test_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,790 - INFO - o4_y_train_los unique target values: {'los': 2548}\n",
      "2025-04-30 12:17:37,792 - INFO - o4_y_train_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,793 - INFO - o4_y_validate_los unique target values: {'los': 319}\n",
      "2025-04-30 12:17:37,794 - INFO - o4_y_validate_mortality unique target values: {'hospital_expire_flag': 2}\n",
      "2025-04-30 12:17:37,795 - INFO - Unique values calculation complete.\n",
      "2025-04-30 12:17:37,796 - INFO - ++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")\n",
    "logging.info(\"Calculate unique values.\")\n",
    "for name, df in dataframes.items():\n",
    "    if 'y_' in name:\n",
    "        unique_vals = df.nunique()\n",
    "        logging.info(f\"{name} unique target values: {unique_vals.to_dict()}\")\n",
    "\n",
    "logging.info(\"Unique values calculation complete.\")\n",
    "logging.info(\"++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6c37687-5e03-43e7-adfa-cfd53a0f6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "for name, df in dataframes.items():\n",
    "    missing_values = df.isnull().sum().sum()\n",
    "    missing_cols = df.isnull().any().sum()\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    summary.append({\n",
    "        'Dataset': name,\n",
    "        'Shape': df.shape,\n",
    "        'Total Missing Values': missing_values,\n",
    "        'Accross Missing Columns': missing_cols,\n",
    "        'Missing %': 100 * missing_values / total_cells\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "# Save to file\n",
    "summary_df.to_csv(\"CSV/exports/01_dataset_summary_CIR-8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc958b38-0239-4a55-87dc-25e87da0c8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: o1_X_train\n",
      "Total Rows: 122496, Total Columns: 345\n",
      "Number of rows with missing values up to 7%: 48\n",
      "The percentage between total and missing values sets is 0.04%\n",
      "\n",
      "Dataset: o2_X_train\n",
      "Total Rows: 61248, Total Columns: 345\n",
      "Number of rows with missing values up to 7%: 24\n",
      "The percentage between total and missing values sets is 0.04%\n",
      "\n",
      "Dataset: o3_X_train\n",
      "Total Rows: 40832, Total Columns: 345\n",
      "Number of rows with missing values up to 7%: 16\n",
      "The percentage between total and missing values sets is 0.04%\n",
      "\n",
      "Dataset: o4_X_train\n",
      "Total Rows: 30624, Total Columns: 345\n",
      "Number of rows with missing values up to 7%: 12\n",
      "The percentage between total and missing values sets is 0.04%\n"
     ]
    }
   ],
   "source": [
    "percent = 7\n",
    "\n",
    "# Dictionary of datasets for iteration\n",
    "datasets = {\n",
    "    \"o1_X_train\": o1_X_train,\n",
    "    \"o2_X_train\": o2_X_train,\n",
    "    \"o3_X_train\": o3_X_train,\n",
    "    \"o4_X_train\": o4_X_train\n",
    "}\n",
    "\n",
    "# Loop through each dataset\n",
    "for name, df in datasets.items():\n",
    "    missing_percentage_per_row = df.isnull().mean(axis=1) * 100\n",
    "    missing_rows = (missing_percentage_per_row <= percent).sum()\n",
    "    total_rows, total_columns = df.shape\n",
    "    percent_between = (missing_rows * 100) / total_rows\n",
    "\n",
    "    print(f\"\\nDataset: {name}\")\n",
    "    print(f\"Total Rows: {total_rows}, Total Columns: {total_columns}\")\n",
    "    print(f\"Number of rows with missing values up to {percent}%: {missing_rows}\")\n",
    "    print(f\"The percentage between total and missing values sets is {percent_between:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827415dc-b41c-476b-89f1-3b52753d8542",
   "metadata": {},
   "source": [
    "## CIR-9: Analyze Missingness (per row/column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d518af74-ac21-4c9b-ad8b-839b101c1ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:17:38,762 - INFO - Switched logging to logs/CIR-9.log\n",
      "2025-04-30 12:17:38,763 - INFO - This is being logged to CIR-9.log\n"
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-9.log')\n",
    "logger.info(\"This is being logged to CIR-9.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbd81108-8d6e-4e89-8b79-1259af686fb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:17:40,876 - INFO -           Dataset          Shape  Total Missing Cells  Total Missing %  \\\n",
      "0   o1_X_external  (234720, 345)             39527616            48.81   \n",
      "12  o2_X_external  (117360, 345)             19763572            48.81   \n",
      "36  o4_X_external   (58680, 345)              9881900            48.81   \n",
      "24  o3_X_external   (78240, 345)             13175808            48.81   \n",
      "27  o3_X_validate    (5104, 345)               669232            38.01   \n",
      "3   o1_X_validate   (15312, 345)              2007696            38.01   \n",
      "15  o2_X_validate    (7656, 345)              1003848            38.01   \n",
      "39  o4_X_validate    (3828, 345)               501924            38.01   \n",
      "25      o3_X_test    (5104, 345)               661888            37.59   \n",
      "1       o1_X_test   (15312, 345)              1985664            37.59   \n",
      "13      o2_X_test    (7656, 345)               992832            37.59   \n",
      "37      o4_X_test    (3828, 345)               496416            37.59   \n",
      "26     o3_X_train   (40832, 345)              5292096            37.57   \n",
      "2      o1_X_train  (122496, 345)             15876288            37.57   \n",
      "14     o2_X_train   (61248, 345)              7938144            37.57   \n",
      "38     o4_X_train   (30624, 345)              3969072            37.57   \n",
      "\n",
      "    Columns with Missing (%)  Max % Missing in Row  Mean % Missing in Row  \\\n",
      "0                        308                 89.28                  48.81   \n",
      "12                       308                 89.28                  48.81   \n",
      "36                       308                 89.28                  48.81   \n",
      "24                       308                 89.28                  48.81   \n",
      "27                       296                 78.84                  38.01   \n",
      "3                        296                 78.84                  38.01   \n",
      "15                       296                 78.84                  38.01   \n",
      "39                       296                 78.84                  38.01   \n",
      "25                       292                 80.00                  37.59   \n",
      "1                        292                 80.00                  37.59   \n",
      "13                       292                 80.00                  37.59   \n",
      "37                       292                 80.00                  37.59   \n",
      "26                       304                 81.16                  37.57   \n",
      "2                        304                 81.16                  37.57   \n",
      "14                       304                 81.16                  37.57   \n",
      "38                       304                 81.16                  37.57   \n",
      "\n",
      "    Min % Missing in Row  \n",
      "0                  13.91  \n",
      "12                 13.91  \n",
      "36                 13.91  \n",
      "24                 13.91  \n",
      "27                  9.28  \n",
      "3                   9.28  \n",
      "15                  9.28  \n",
      "39                  9.28  \n",
      "25                 10.43  \n",
      "1                  10.43  \n",
      "13                 10.43  \n",
      "37                 10.43  \n",
      "26                  6.96  \n",
      "2                   6.96  \n",
      "14                  6.96  \n",
      "38                  6.96  \n"
     ]
    }
   ],
   "source": [
    "missingness_summary = []\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        continue\n",
    "    \n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    \n",
    "    col_missing = df.isnull().mean() * 100\n",
    "    row_missing = df.isnull().mean(axis=1) * 100\n",
    "    \n",
    "    summary = {\n",
    "        'Dataset': name,\n",
    "        'Shape': df.shape,\n",
    "        'Total Missing Cells': total_missing,\n",
    "        'Total Missing %': round(100 * total_missing / total_cells, 2),\n",
    "        'Columns with Missing (%)': (col_missing > 0).sum(),\n",
    "        'Max % Missing in Row': round(row_missing.max(), 2),\n",
    "        'Mean % Missing in Row': round(row_missing.mean(), 2),\n",
    "        'Min % Missing in Row': round(row_missing.min(), 2)\n",
    "    }\n",
    "    missingness_summary.append(summary)\n",
    "\n",
    "# Create DataFrame for summary\n",
    "missingness_df = pd.DataFrame(missingness_summary)\n",
    "\n",
    "# Sort by most missing\n",
    "missingness_df.sort_values(\"Total Missing %\", ascending=False, inplace=True)\n",
    "\n",
    "# Save to file\n",
    "missingness_df.to_csv(\"CSV/exports/02_missingness_summary-CIR-9.csv\", index=False)\n",
    "\n",
    "# Show top rows, I have leave out the y_ files which are labels\n",
    "logging.info(missingness_df.head(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4558adc-5cb9-4ac5-ad59-61f951801734",
   "metadata": {},
   "source": [
    "## CIR-10: Visualize Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3daa1e3-6555-40f1-9ce1-aa1b302e5a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:17:40,911 - INFO - Switched logging to logs/CIR-10.log\n",
      "2025-04-30 12:17:40,912 - INFO - This is being logged to CIR-10.log\n"
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-10.log')\n",
    "logger.info(\"This is being logged to CIR-10.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "196c2e23-1a91-4e17-8934-7eb06a927d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:17:40,947 - INFO - Generating plots for: o1_X_external\n",
      "2025-04-30 12:17:45,380 - INFO - Generating plots for: o2_X_external\n",
      "2025-04-30 12:17:49,274 - INFO - Generating plots for: o4_X_external\n",
      "2025-04-30 12:17:53,336 - INFO - Generating plots for: o3_X_external\n",
      "2025-04-30 12:17:57,306 - INFO - Generating plots for: o3_X_validate\n",
      "2025-04-30 12:18:01,239 - INFO - Generating plots for: o1_X_validate\n",
      "2025-04-30 12:18:05,165 - INFO - Generating plots for: o2_X_validate\n",
      "2025-04-30 12:18:09,389 - INFO - Generating plots for: o4_X_validate\n",
      "2025-04-30 12:18:13,208 - INFO - Generating plots for: o3_X_test\n",
      "2025-04-30 12:18:17,029 - INFO - Generating plots for: o1_X_test\n",
      "2025-04-30 12:18:20,844 - INFO - Generating plots for: o2_X_test\n",
      "2025-04-30 12:18:25,298 - INFO - Generating plots for: o4_X_test\n",
      "2025-04-30 12:18:29,771 - INFO - Generating plots for: o3_X_train\n",
      "2025-04-30 12:18:33,653 - INFO - Generating plots for: o1_X_train\n",
      "2025-04-30 12:18:37,544 - INFO - Generating plots for: o2_X_train\n",
      "2025-04-30 12:18:41,511 - INFO - Generating plots for: o4_X_train\n"
     ]
    }
   ],
   "source": [
    "# Create output folder\n",
    "output_dir = \"figures/CIR-10_missingness\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Functions for plotting\n",
    "def plot_missing_heatmap(\n",
    "    df, title, max_rows=200, save_path=None, suffix_filter='(Min)', cmap='Blues'\n",
    "):\n",
    "    filtered_cols = [col for col in df.columns if col.endswith(suffix_filter)]\n",
    "    df_filtered = df[filtered_cols].head(max_rows)\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.heatmap(df_filtered.isnull(), cbar=False, cmap=cmap, yticklabels=False,\n",
    "                linecolor='lightgrey', linewidths=0.001)\n",
    "\n",
    "    plt.title(f\"Missing Data Heatmap – First {max_rows} Rows\\n({suffix_filter}) – {title}\",\n",
    "              fontsize=14, weight='bold')\n",
    "    plt.xlabel(\"Features\", fontsize=12)\n",
    "    plt.ylabel(\"Rows\", fontsize=12)\n",
    "    plt.xticks(fontsize=8, rotation=90)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_column_missing_bar(\n",
    "    df, title, save_path=None, top_n=40, suffix_filter='(Min)', color='#3E8ED0'\n",
    "):\n",
    "    filtered_cols = [col for col in df.columns if col.endswith(suffix_filter)]\n",
    "    missing_perc = df[filtered_cols].isnull().mean() * 100\n",
    "    missing_perc = missing_perc[missing_perc > 0].sort_values(ascending=False).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(10, 0.4 * len(missing_perc)))\n",
    "    ax = missing_perc.plot(kind='barh', color=color)\n",
    "\n",
    "    plt.title(f\"Top {len(missing_perc)} Features with Missing Values\\n({suffix_filter}) – {title}\",\n",
    "              fontsize=14, weight='bold')\n",
    "    plt.xlabel(\"Percentage Missing\", fontsize=12)\n",
    "    plt.ylabel(\"Feature\", fontsize=12)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Filter only X datasets from the summary\n",
    "top_datasets = missingness_df[\n",
    "    (~missingness_df['Dataset'].str.startswith('y_')) &\n",
    "    (missingness_df['Dataset'].isin(dataframes.keys()))\n",
    "].head(16)['Dataset'].tolist()\n",
    "\n",
    "# Generate plots\n",
    "for dataset_name in top_datasets:\n",
    "    df = dataframes.get(dataset_name)\n",
    "    if df is not None:\n",
    "        logging.info(f\"Generating plots for: {dataset_name}\")\n",
    "        heatmap_path = os.path.join(output_dir, f\"{dataset_name}_heatmap.png\")\n",
    "        barplot_path = os.path.join(output_dir, f\"{dataset_name}_barplot.png\")\n",
    "        \n",
    "        plot_missing_heatmap(df, dataset_name, save_path=heatmap_path)\n",
    "        plot_column_missing_bar(df, dataset_name, save_path=barplot_path)\n",
    "    else:\n",
    "        logging.info(f\"Dataset not found in dataframes: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be29c922-56a7-4f24-b83c-804336898a40",
   "metadata": {},
   "source": [
    "## CIR-11: Define Clinical Value Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e042fd7-2b59-4be0-be2c-2f812288c7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:18:45,488 - INFO - Switched logging to logs/CIR-11.log\n",
      "2025-04-30 12:18:45,489 - INFO - This is being logged to CIR-11.log\n"
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-11.log')\n",
    "logger.info(\"This is being logged to CIR-11.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab5a9679-c114-4b8f-807d-6814aa9f3b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:18:45,504 - INFO - Processing suffix: (Mean)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observed Min</th>\n",
       "      <th>Observed Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alanine_Aminotransferase_(ALT)_(Mean)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7784.666504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albumin_(Mean)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alkaline_Phosphatase_(Mean)</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1448.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ammonia_(Mean)</th>\n",
       "      <td>2.2</td>\n",
       "      <td>130.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amylase_(Mean)</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4867.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uric_Acid_(Mean)</th>\n",
       "      <td>1.3</td>\n",
       "      <td>16.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White_Blood_Cells_(Mean)</th>\n",
       "      <td>0.2</td>\n",
       "      <td>361.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pCO2_(Mean)</th>\n",
       "      <td>9.0</td>\n",
       "      <td>101.333336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH_(Mean)</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pO2_(Mean)</th>\n",
       "      <td>18.0</td>\n",
       "      <td>609.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Observed Min  Observed Max\n",
       "Alanine_Aminotransferase_(ALT)_(Mean)           1.0   7784.666504\n",
       "Albumin_(Mean)                                  1.0      5.100000\n",
       "Alkaline_Phosphatase_(Mean)                     9.0   1448.000000\n",
       "Ammonia_(Mean)                                  2.2    130.000000\n",
       "Amylase_(Mean)                                  8.0   4867.000000\n",
       "...                                             ...           ...\n",
       "Uric_Acid_(Mean)                                1.3     16.500000\n",
       "White_Blood_Cells_(Mean)                        0.2    361.000000\n",
       "pCO2_(Mean)                                     9.0    101.333336\n",
       "pH_(Mean)                                       3.0      8.500000\n",
       "pO2_(Mean)                                     18.0    609.000000\n",
       "\n",
       "[77 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:18:45,899 - INFO - Processing suffix: (Median)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observed Min</th>\n",
       "      <th>Observed Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alanine_Aminotransferase_(ALT)_(Median)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albumin_(Median)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alkaline_Phosphatase_(Median)</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ammonia_(Median)</th>\n",
       "      <td>2.2</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amylase_(Median)</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uric_Acid_(Median)</th>\n",
       "      <td>1.3</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White_Blood_Cells_(Median)</th>\n",
       "      <td>0.2</td>\n",
       "      <td>361.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pCO2_(Median)</th>\n",
       "      <td>9.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH_(Median)</th>\n",
       "      <td>5.0</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pO2_(Median)</th>\n",
       "      <td>18.0</td>\n",
       "      <td>609.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Observed Min  Observed Max\n",
       "Alanine_Aminotransferase_(ALT)_(Median)           1.0        7510.0\n",
       "Albumin_(Median)                                  1.0           5.1\n",
       "Alkaline_Phosphatase_(Median)                     9.0        1448.0\n",
       "Ammonia_(Median)                                  2.2         130.0\n",
       "Amylase_(Median)                                  8.0        4867.0\n",
       "...                                               ...           ...\n",
       "Uric_Acid_(Median)                                1.3          16.5\n",
       "White_Blood_Cells_(Median)                        0.2         361.0\n",
       "pCO2_(Median)                                     9.0          98.0\n",
       "pH_(Median)                                       5.0           8.5\n",
       "pO2_(Median)                                     18.0         609.0\n",
       "\n",
       "[77 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:18:46,285 - INFO - Processing suffix: (Min)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observed Min</th>\n",
       "      <th>Observed Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alanine_Aminotransferase_(ALT)_(Min)</th>\n",
       "      <td>1.00</td>\n",
       "      <td>7510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albumin_(Min)</th>\n",
       "      <td>1.00</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alkaline_Phosphatase_(Min)</th>\n",
       "      <td>8.00</td>\n",
       "      <td>1448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ammonia_(Min)</th>\n",
       "      <td>2.20</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amylase_(Min)</th>\n",
       "      <td>7.00</td>\n",
       "      <td>4867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uric_Acid_(Min)</th>\n",
       "      <td>0.30</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White_Blood_Cells_(Min)</th>\n",
       "      <td>0.17</td>\n",
       "      <td>361.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pCO2_(Min)</th>\n",
       "      <td>9.00</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH_(Min)</th>\n",
       "      <td>5.00</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pO2_(Min)</th>\n",
       "      <td>18.00</td>\n",
       "      <td>609.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Observed Min  Observed Max\n",
       "Alanine_Aminotransferase_(ALT)_(Min)          1.00        7510.0\n",
       "Albumin_(Min)                                 1.00           5.1\n",
       "Alkaline_Phosphatase_(Min)                    8.00        1448.0\n",
       "Ammonia_(Min)                                 2.20         130.0\n",
       "Amylase_(Min)                                 7.00        4867.0\n",
       "...                                            ...           ...\n",
       "Uric_Acid_(Min)                               0.30          16.5\n",
       "White_Blood_Cells_(Min)                       0.17         361.0\n",
       "pCO2_(Min)                                    9.00          98.0\n",
       "pH_(Min)                                      5.00           8.5\n",
       "pO2_(Min)                                    18.00         609.0\n",
       "\n",
       "[77 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:18:46,669 - INFO - Processing suffix: (Max)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observed Min</th>\n",
       "      <th>Observed Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alanine_Aminotransferase_(ALT)_(Max)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8776.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albumin_(Max)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alkaline_Phosphatase_(Max)</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1448.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ammonia_(Max)</th>\n",
       "      <td>2.2</td>\n",
       "      <td>157.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amylase_(Max)</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4867.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uric_Acid_(Max)</th>\n",
       "      <td>1.3</td>\n",
       "      <td>16.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White_Blood_Cells_(Max)</th>\n",
       "      <td>0.2</td>\n",
       "      <td>361.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pCO2_(Max)</th>\n",
       "      <td>9.0</td>\n",
       "      <td>163.399994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH_(Max)</th>\n",
       "      <td>5.0</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pO2_(Max)</th>\n",
       "      <td>18.0</td>\n",
       "      <td>635.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Observed Min  Observed Max\n",
       "Alanine_Aminotransferase_(ALT)_(Max)           1.0   8776.000000\n",
       "Albumin_(Max)                                  1.0      5.100000\n",
       "Alkaline_Phosphatase_(Max)                     9.0   1448.000000\n",
       "Ammonia_(Max)                                  2.2    157.000000\n",
       "Amylase_(Max)                                  8.0   4867.000000\n",
       "...                                            ...           ...\n",
       "Uric_Acid_(Max)                                1.3     16.500000\n",
       "White_Blood_Cells_(Max)                        0.2    361.000000\n",
       "pCO2_(Max)                                     9.0    163.399994\n",
       "pH_(Max)                                       5.0      8.500000\n",
       "pO2_(Max)                                     18.0    635.000000\n",
       "\n",
       "[77 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of suffixes\n",
    "suffixes = ['(Mean)', '(Median)', '(Min)', '(Max)']\n",
    "\n",
    "for SUFFIX in suffixes:\n",
    "    logging.info(f\"Processing suffix: {SUFFIX}\")\n",
    "    \n",
    "    # Step 2: Collect global min/max per feature\n",
    "    global_min_max = {}\n",
    "\n",
    "    for name, df in dataframes.items():\n",
    "        if name.startswith('y_'):  # Skip target datasets\n",
    "            continue\n",
    "        filtered_cols = [col for col in df.columns if col.endswith(SUFFIX)]\n",
    "        filtered_df = df[filtered_cols]\n",
    "\n",
    "        # For each feature, collect min and max\n",
    "        for col in filtered_cols:\n",
    "            col_min = filtered_df[col].min(skipna=True)\n",
    "            col_max = filtered_df[col].max(skipna=True)\n",
    "\n",
    "            if col not in global_min_max:\n",
    "                global_min_max[col] = {'min': col_min, 'max': col_max}\n",
    "            else:\n",
    "                global_min_max[col]['min'] = min(global_min_max[col]['min'], col_min)\n",
    "                global_min_max[col]['max'] = max(global_min_max[col]['max'], col_max)\n",
    "\n",
    "    # Step 3: Convert to DataFrame\n",
    "    clinical_ranges_df = pd.DataFrame.from_dict(global_min_max, orient='index')\n",
    "    clinical_ranges_df = clinical_ranges_df.rename(columns={'min': 'Observed Min', 'max': 'Observed Max'})\n",
    "    clinical_ranges_df = clinical_ranges_df.sort_index()\n",
    "\n",
    "    # Save it to a CSV file\n",
    "    suffix_clean = SUFFIX.strip('()')  # Remove parentheses for a cleaner filename\n",
    "    output_file_path = f\"CSV/exports/CRI-11/observed_clinical_ranges_{suffix_clean}.csv\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    clinical_ranges_df.to_csv(output_file_path, index=True)\n",
    "\n",
    "    # Optional: Preview\n",
    "    display(clinical_ranges_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe91af-c979-4931-b1ef-4019a9d60729",
   "metadata": {},
   "source": [
    "## CRI-50: Identify and Quantify Out-of-Range Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e2abc4e-c8b5-4e90-bcda-d537927e765c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:18:47,059 - INFO - Switched logging to logs/CIR-50.log\n",
      "2025-04-30 12:18:47,061 - INFO - This is being logged to CIR-50.log\n"
     ]
    }
   ],
   "source": [
    "# Build log file\n",
    "switch_log_file('logs/CIR-50.log')\n",
    "logger.info(\"This is being logged to CIR-50.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "497cb493-46b9-48bf-87a7-bd3ef6350000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:20:37,942 - INFO - Processing group: survive\n",
      "[survive] Observed Ranges: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 154/154 [00:00<00:00, 327.67it/s]\n",
      "2025-04-30 12:20:38,430 - INFO - Saved observed ranges for survive\n",
      "2025-04-30 12:20:47,815 - INFO - Saved extreme values for survive                                                                                                \n",
      "2025-04-30 12:20:47,819 - INFO - Processing group: non_survive\n",
      "[non_survive] Observed Ranges: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 154/154 [00:00<00:00, 1750.06it/s]\n",
      "2025-04-30 12:20:47,914 - INFO - Saved observed ranges for non_survive\n",
      "2025-04-30 12:20:54,733 - INFO - Saved extreme values for non_survive                                                                                            \n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "datasets = [\n",
    "    \"o1_X_train\", \"o1_X_validate\", \"o1_X_test\", \"o1_X_external\",\n",
    "    \"o2_X_train\", \"o2_X_validate\", \"o2_X_test\", \"o2_X_external\",\n",
    "    \"o3_X_train\", \"o3_X_validate\", \"o3_X_test\", \"o3_X_external\",\n",
    "    \"o4_X_train\", \"o4_X_validate\", \"o4_X_test\", \"o4_X_external\"\n",
    "]\n",
    "\n",
    "# Identify clinical-laboratory features\n",
    "clinical_suffixes = ('(Min)', '(Max)')\n",
    "clinical_features = [col for col in dataframes['o1_X_train'].columns if col.endswith(clinical_suffixes)]\n",
    "\n",
    "# Concatenate all dataframes with mortality label\n",
    "all_rows = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = dataframes[dataset].copy()\n",
    "    label_name = dataset.replace(\"X\", \"y\") + \"_mortality\"\n",
    "    df[\"mortality\"] = dataframes[label_name]\n",
    "    df[\"dataset\"] = dataset  # keep track of original dataset\n",
    "    all_rows.append(df)\n",
    "\n",
    "df_all = pd.concat(all_rows)\n",
    "\n",
    "# Separate groups\n",
    "groups = {\n",
    "    \"survive\": df_all[df_all[\"mortality\"] == 0.0],\n",
    "    \"non_survive\": df_all[df_all[\"mortality\"] == 1.0]\n",
    "}\n",
    "\n",
    "# Process each group separately\n",
    "for group_name, group_df in groups.items():\n",
    "    logging.info(f\"Processing group: {group_name}\")\n",
    "\n",
    "    # --- Part A: Observed ranges ---\n",
    "    observed_ranges = []\n",
    "\n",
    "    for feature in tqdm(clinical_features, desc=f\"[{group_name}] Observed Ranges\"):\n",
    "        if feature in group_df.columns and not group_df[feature].isnull().all():\n",
    "            observed_ranges.append({\n",
    "                'Feature': feature,\n",
    "                'Observed Min': group_df[feature].min(),\n",
    "                'Observed Max': group_df[feature].max()\n",
    "            })\n",
    "\n",
    "    observed_ranges_df = pd.DataFrame(observed_ranges)\n",
    "    observed_ranges_df.to_csv(f\"CSV/exports/02_task4_observed_ranges_{group_name}.csv\", index=False)\n",
    "    logging.info(f\"Saved observed ranges for {group_name}\")\n",
    "\n",
    "    # --- Part B: Percentile detection ---\n",
    "    percentile_summary = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        df_dataset = df_all[df_all[\"dataset\"] == dataset]\n",
    "        group_df = df_dataset[df_dataset[\"mortality\"] == group_df[\"mortality\"].iloc[0]]\n",
    "\n",
    "        for feature in tqdm(clinical_features, desc=f\"[{group_name}] {dataset}\", leave=False):\n",
    "            if feature not in group_df.columns or group_df[feature].isnull().all():\n",
    "                continue\n",
    "\n",
    "            lower_pct = group_df[feature].quantile(0.005)\n",
    "            upper_pct = group_df[feature].quantile(0.995)\n",
    "\n",
    "            extreme_low = group_df[group_df[feature] < lower_pct].shape[0]\n",
    "            extreme_high = group_df[group_df[feature] > upper_pct].shape[0]\n",
    "\n",
    "            percentile_summary.append({\n",
    "                'Source Dataset': dataset,\n",
    "                'Feature': feature,\n",
    "                '0.5th Percentile Threshold': lower_pct,\n",
    "                '99.5th Percentile Threshold': upper_pct,\n",
    "                'Count Below 0.5th': extreme_low,\n",
    "                'Count Above 99.5th': extreme_high,\n",
    "                'Min Value Found': group_df[feature].min(),\n",
    "                'Max Value Found': group_df[feature].max()\n",
    "            })\n",
    "    \n",
    "    # Define the full output file path\n",
    "    output_file_path = f\"CSV/exports/CRI-50/percentile_extreme_values_{group_name}.csv\"\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    output_dir = os.path.dirname(output_file_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the DataFrame\n",
    "    percentile_df = pd.DataFrame(percentile_summary)\n",
    "    percentile_df.to_csv(output_file_path, index=False)\n",
    "    logging.info(f\"Saved extreme values for {group_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f8b33-1349-46c8-a431-d35f1166059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "survive and non survive together\n",
    "\"\"\"\n",
    "# Settings\n",
    "output_root = \"figures/CIR-50/grouped_adaptive_plots\"\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "\n",
    "suffixes = ['(Min)', '(Max)']\n",
    "LOWER_COL = '0.5th Percentile Threshold'\n",
    "UPPER_COL = '99.5th Percentile Threshold'\n",
    "CHUNK_SIZE = 10  # Features per plot\n",
    "\n",
    "# Loop through datasets and suffixes\n",
    "for dataset in datasets:\n",
    "    df = dataframes[dataset]\n",
    "    dataset_dir = os.path.join(output_root, dataset)\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    for suffix in suffixes:\n",
    "        df_suffix = percentile_df[\n",
    "            (percentile_df['Source Dataset'] == dataset) &\n",
    "            (percentile_df['Feature'].str.endswith(suffix))\n",
    "        ].copy()\n",
    "\n",
    "        if df_suffix.empty:\n",
    "            continue\n",
    "\n",
    "        df_suffix['Total_Extremes'] = df_suffix['Count Below 0.5th'] + df_suffix['Count Above 99.5th']\n",
    "        df_sorted = df_suffix.sort_values('Total_Extremes', ascending=False)\n",
    "        features_to_plot = df_sorted['Feature'].tolist()\n",
    "\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        total_chunks = ceil(len(features_to_plot) / CHUNK_SIZE)\n",
    "\n",
    "        for chunk_idx in range(total_chunks):\n",
    "            chunk_features = features_to_plot[chunk_idx * CHUNK_SIZE : (chunk_idx + 1) * CHUNK_SIZE]\n",
    "            fig, axes = plt.subplots(nrows=len(chunk_features)*2, ncols=1, figsize=(13, len(chunk_features) * 3.0))\n",
    "            if len(chunk_features) == 1:\n",
    "                axes = [axes] * 2\n",
    "\n",
    "            for i, feature in enumerate(chunk_features):\n",
    "                plot_ax = axes[i*2]\n",
    "                table_ax = axes[i*2 + 1]\n",
    "\n",
    "                data_clean = df[feature].dropna().values\n",
    "                if data_clean.size == 0:\n",
    "                    continue\n",
    "\n",
    "                row = df_sorted[df_sorted['Feature'] == feature].iloc[0]\n",
    "\n",
    "                lower_pct = row[LOWER_COL]\n",
    "                upper_pct = row[UPPER_COL]\n",
    "                x_min, x_max = np.percentile(data_clean, [1, 99])\n",
    "                x_min = min(x_min, lower_pct) - 0.05 * abs(min(x_min, lower_pct))\n",
    "                x_max = max(x_max, upper_pct) + 0.05 * abs(max(x_max, upper_pct))\n",
    "\n",
    "                sns.boxplot(x=data_clean, ax=plot_ax, orient='h', fliersize=2, color='skyblue',\n",
    "                            width=0.5, linewidth=1, boxprops=dict(alpha=0.6))\n",
    "                plot_ax.axvline(lower_pct, color='red', linestyle='--', linewidth=3, zorder=10)\n",
    "                plot_ax.axvline(upper_pct, color='purple', linestyle='--', linewidth=3, zorder=10)\n",
    "                plot_ax.set_xlim(x_min, x_max)\n",
    "                plot_ax.set_title(feature, fontsize=13, weight='bold')\n",
    "                plot_ax.set_xlabel(\"Value\", fontsize=11)\n",
    "\n",
    "                # Table\n",
    "                table_ax.axis('off')\n",
    "                table_data = [[\n",
    "                    f\"{row[LOWER_COL]:.2f}\", f\"{row[UPPER_COL]:.2f}\",\n",
    "                    f\"{row['Count Below 0.5th']}\", f\"{row['Count Above 99.5th']}\",\n",
    "                    f\"{row['Min Value Found']:.2f}\", f\"{row['Max Value Found']:.2f}\"\n",
    "                ]]\n",
    "                col_labels = [\n",
    "                    '0.5th Percentile', '99.5th Percentile',\n",
    "                    'Count < 0.5th', 'Count > 99.5th',\n",
    "                    'Min', 'Max'\n",
    "                ]\n",
    "                table = table_ax.table(\n",
    "                    cellText=table_data,\n",
    "                    colLabels=col_labels,\n",
    "                    loc='center',\n",
    "                    cellLoc='center',\n",
    "                    colWidths=[0.15] * len(col_labels)\n",
    "                )\n",
    "                table.scale(1, 2.2)\n",
    "                for (row_i, col_i), cell in table.get_celld().items():\n",
    "                    if row_i == 0:\n",
    "                        cell.set_text_props(weight='bold', fontsize=14)\n",
    "                    else:\n",
    "                        cell.set_text_props(fontsize=13)\n",
    "\n",
    "            # File naming\n",
    "            suffix_safe = suffix.replace('(', '').replace(')', '')\n",
    "            file_index = str(chunk_idx + 1).zfill(2)\n",
    "            plot_filename = f\"{dataset}_{suffix_safe}_adaptive_per_feature_table_{file_index}.png\"\n",
    "\n",
    "            # Title and single legend (positioned slightly above the title)\n",
    "            plt.suptitle(f'Top Features {suffix} – {dataset} (Set {file_index})',\n",
    "                         fontsize=15, weight='bold', y=1.035)\n",
    "            \n",
    "            legend_elements = [\n",
    "                Patch(facecolor='red', edgecolor='red', linestyle='--', label='0.5th Percentile'),\n",
    "                Patch(facecolor='purple', edgecolor='purple', linestyle='--', label='99.5th Percentile')\n",
    "            ]\n",
    "            fig.legend(\n",
    "                handles=legend_elements,\n",
    "                loc='upper center',\n",
    "                fontsize=12,\n",
    "                ncol=2,\n",
    "                frameon=False,\n",
    "                bbox_to_anchor=(0.5, 1.06)\n",
    "            )\n",
    "\n",
    "            plt.subplots_adjust(top=0.95)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(dataset_dir, plot_filename), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "print(\"All adaptive plots saved with a single top legend and clean spacing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdeca4-582b-4d6a-8033-e9e026ace014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "output_root = \"figures/CIR-50/grouped_adaptive_plots\"\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "# Settings\n",
    "suffixes = ['(Min)', '(Max)']\n",
    "LOWER_COL = '0.5th Percentile Threshold'\n",
    "UPPER_COL = '99.5th Percentile Threshold'\n",
    "CHUNK_SIZE = 10\n",
    "\n",
    "# Group info\n",
    "groups = {\n",
    "    'survive': 'percentile_extreme_values_survive.csv',\n",
    "    'non_survive': 'percentile_extreme_values_non_survive.csv'\n",
    "}\n",
    "\n",
    "for group, csv_file in groups.items():\n",
    "    print(f\"📊 Processing group: {group}\")\n",
    "    \n",
    "    # Load percentile summary\n",
    "    percentile_df = pd.read_csv(f\"CSV/exports/CRI-50/{csv_file}\")\n",
    "\n",
    "    # Output path\n",
    "    output_root = f\"figures/CIR-50/grouped_adaptive_plots_{group}\"\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "    datasets = percentile_df['Source Dataset'].unique()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        df = dataframes[dataset]\n",
    "        dataset_dir = os.path.join(output_root, dataset)\n",
    "        os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "        for suffix in suffixes:\n",
    "            df_suffix = percentile_df[\n",
    "                (percentile_df['Source Dataset'] == dataset) &\n",
    "                (percentile_df['Feature'].str.endswith(suffix))\n",
    "            ].copy()\n",
    "\n",
    "            if df_suffix.empty:\n",
    "                continue\n",
    "\n",
    "            df_suffix['Total_Extremes'] = df_suffix['Count Below 0.5th'] + df_suffix['Count Above 99.5th']\n",
    "            df_sorted = df_suffix.sort_values('Total_Extremes', ascending=False)\n",
    "            features_to_plot = df_sorted['Feature'].tolist()\n",
    "\n",
    "            sns.set(style=\"whitegrid\")\n",
    "            total_chunks = ceil(len(features_to_plot) / CHUNK_SIZE)\n",
    "\n",
    "            for chunk_idx in range(total_chunks):\n",
    "                chunk_features = features_to_plot[chunk_idx * CHUNK_SIZE : (chunk_idx + 1) * CHUNK_SIZE]\n",
    "                fig, axes = plt.subplots(nrows=len(chunk_features)*2, ncols=1, figsize=(13, len(chunk_features) * 3.0))\n",
    "                if len(chunk_features) == 1:\n",
    "                    axes = [axes] * 2\n",
    "\n",
    "                for i, feature in enumerate(chunk_features):\n",
    "                    plot_ax = axes[i*2]\n",
    "                    table_ax = axes[i*2 + 1]\n",
    "\n",
    "                    data_clean = df[feature].dropna().values\n",
    "                    if data_clean.size == 0:\n",
    "                        continue\n",
    "\n",
    "                    row = df_sorted[df_sorted['Feature'] == feature].iloc[0]\n",
    "\n",
    "                    lower_pct = row[LOWER_COL]\n",
    "                    upper_pct = row[UPPER_COL]\n",
    "                    x_min, x_max = np.percentile(data_clean, [1, 99])\n",
    "                    x_min = min(x_min, lower_pct) - 0.05 * abs(min(x_min, lower_pct))\n",
    "                    x_max = max(x_max, upper_pct) + 0.05 * abs(max(x_max, upper_pct))\n",
    "\n",
    "                    sns.boxplot(x=data_clean, ax=plot_ax, orient='h', fliersize=2, color='skyblue',\n",
    "                                width=0.5, linewidth=1, boxprops=dict(alpha=0.6))\n",
    "                    plot_ax.axvline(lower_pct, color='red', linestyle='--', linewidth=3, zorder=10)\n",
    "                    plot_ax.axvline(upper_pct, color='purple', linestyle='--', linewidth=3, zorder=10)\n",
    "                    plot_ax.set_xlim(x_min, x_max)\n",
    "                    plot_ax.set_title(feature, fontsize=13, weight='bold')\n",
    "                    plot_ax.set_xlabel(\"Value\", fontsize=11)\n",
    "\n",
    "                    table_ax.axis('off')\n",
    "\n",
    "\n",
    "                    table_data = [[\n",
    "                        f\"{row[LOWER_COL]:.2f}\", f\"{row[UPPER_COL]:.2f}\",\n",
    "                        f\"{row['Count Below 0.5th']}\", f\"{row['Count Above 99.5th']}\",\n",
    "                        f\"{row['Min Value Found']:.2f}\", f\"{row['Max Value Found']:.2f}\"\n",
    "                    ]]\n",
    "                    col_labels = [\n",
    "                        '0.5th Percentile', '99.5th Percentile',\n",
    "                        'Count < 0.5th', 'Count > 99.5th',\n",
    "                        'Min', 'Max'\n",
    "                    ]\n",
    "                    table = table_ax.table(\n",
    "                        cellText=table_data,\n",
    "                        colLabels=col_labels,\n",
    "                        loc='center',\n",
    "                        cellLoc='center',\n",
    "                        colWidths=[0.15] * len(col_labels)\n",
    "                    )\n",
    "                    table.scale(1, 2.2)\n",
    "                    for (row_i, col_i), cell in table.get_celld().items():\n",
    "                        if row_i == 0:\n",
    "                            cell.set_text_props(weight='bold', fontsize=14)\n",
    "                        else:\n",
    "                            cell.set_text_props(fontsize=13)\n",
    "\n",
    "                suffix_safe = suffix.replace('(', '').replace(')', '')\n",
    "                file_index = str(chunk_idx + 1).zfill(2)\n",
    "                plot_filename = f\"{dataset}_{suffix_safe}_adaptive_per_feature_table_{file_index}.png\"\n",
    "\n",
    "                plt.suptitle(f'Top Features {suffix} – {dataset} ({group.capitalize()} – Set {file_index})',\n",
    "                             fontsize=15, weight='bold', y=1.035)\n",
    "\n",
    "                legend_elements = [\n",
    "                    Patch(facecolor='red', edgecolor='red', linestyle='--', label='0.5th Percentile'),\n",
    "                    Patch(facecolor='purple', edgecolor='purple', linestyle='--', label='99.5th Percentile')\n",
    "                ]\n",
    "                fig.legend(\n",
    "                    handles=legend_elements,\n",
    "                    loc='upper center',\n",
    "                    fontsize=12,\n",
    "                    ncol=2,\n",
    "                    frameon=False,\n",
    "                    bbox_to_anchor=(0.5, 1.06)\n",
    "                )\n",
    "\n",
    "                plt.subplots_adjust(top=0.95)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(dataset_dir, plot_filename), dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "print(\"Plots for both survive and non-survive groups created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c50af87-120d-4ab5-9420-c80f8e1b0ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f192db-1cc3-41e6-80bb-7a6be0183136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc348d-db50-4bd5-b82c-2cd766489a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5dbb2-ad5f-495f-82bb-a310ab1a14de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3cd1f3-ad40-436a-8d1e-9448893ab7db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86414e92-1e73-4d8f-b5cd-a4e425cfe9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d458a-dd25-4f7a-9880-214f2b48987e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dba4a5-9c5d-4a56-8aa9-f0895c5b56b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3827aaba-61ae-4e24-8f7d-0a8aa9692a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb8e12-b434-41c9-9cf6-a4930d7315c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7d785-2850-4f5d-b291-8b51ad4d3560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b86d3d3a-1533-4301-9c66-bac0b345b6fd",
   "metadata": {},
   "source": [
    "# Test Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88526a24-0dec-40eb-a60d-2a90ba43f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labevents_df = pd.read_csv(r\"..\\00_Datasets\\mimic-iv-3_1\\icu\\d_items.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd48f79-bbf5-4d8b-9deb-a277c1024efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(labevents_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e3ef19-0132-4e25-88ef-ae7c13b4a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "abp_df = labevents_df[labevents_df['label'].str.startswith('Arterial Blood Pressure mean', na=False)]\n",
    "\n",
    "# Display the result\n",
    "display(abp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f540b-1847-4ca1-9960-24673a280b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(r\"../01_MimicIV/CSV/Exports/o04_icu_chartevent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e981f24-ccfc-43dc-9045-92aad1313119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f990977b-2799-4792-a14f-8bb7b0121853",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = temp_df[['itemid', 'valuenum', 'valueuom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b12f8-e4f0-4e9c-9b77-8bb77b30797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = temp_df[temp_df['itemid'] == 220052][['itemid', 'valuenum', 'valueuom']]\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d8c883-235d-4389-8044-9aa2ea1dce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.sort_values('valuenum', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d61603-51c2-42c9-a7d3-ba509bfbb2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0843f9f-70f6-46dd-8123-c3f997b9df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "file_path = \"../00_Datasets/mimic-iv-3_1/icu/chartevents.csv.gz\"\n",
    "\n",
    "# Chunk size\n",
    "chunksize = 1000000  # Adjust based on your memory capacity\n",
    "\n",
    "# Filtered data will be appended here\n",
    "filtered_chunks = []\n",
    "\n",
    "# Loop through the file in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize, compression='gzip', usecols=['subject_id', 'itemid', 'valuenum', 'valueuom']):\n",
    "    # Filter only rows with itemid == 220052\n",
    "    filtered_chunk = chunk[chunk['itemid'] == 220052]\n",
    "    filtered_chunks.append(filtered_chunk)\n",
    "\n",
    "# Combine all filtered chunks into one DataFrame\n",
    "result_df = pd.concat(filtered_chunks, ignore_index=True)\n",
    "\n",
    "# Show a preview\n",
    "print(result_df.head())\n",
    "\n",
    "# Optional: Save to CSV\n",
    "# result_df.to_csv(\"filtered_chartevents_220052.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e206ef56-1ce3-4e78-abab-e7b9ab0b074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.sort_values('valuenum', ascending=False)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaff693-44ec-4ab2-93cc-2ded3743403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ab714-8eb5-499b-b001-9189f0bd79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "file_path = \"../00_Datasets/mimic-iv-3_1/icu/chartevents.csv.gz\"\n",
    "\n",
    "# Chunk size\n",
    "chunksize = 1000000  # Adjust based on your system's RAM\n",
    "\n",
    "# Container for filtered data\n",
    "negative_valuenum_chunks = []\n",
    "\n",
    "# Process the file in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize, compression='gzip', usecols=['subject_id', 'itemid', 'value', 'valuenum', 'valueuom']):\n",
    "    # Drop rows where valuenum is NaN to avoid errors\n",
    "    chunk = chunk.dropna(subset=['valuenum'])\n",
    "\n",
    "    # Filter for negative valuenum values\n",
    "    negative_chunk = chunk[chunk['valuenum'] < 0]\n",
    "    \n",
    "    # Append to list\n",
    "    negative_valuenum_chunks.append(negative_chunk)\n",
    "\n",
    "# Combine all the negative valuenum rows\n",
    "result_df = pd.concat(negative_valuenum_chunks, ignore_index=True)\n",
    "\n",
    "# Display a few rows\n",
    "print(result_df.head())\n",
    "\n",
    "# Optional: Save to CSV\n",
    "# result_df.to_csv(\"negative_valuenum_chartevents.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768161e-5527-4187-99dd-99a6c9a344fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.sort_values('valuenum', ascending=True)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9564bab9-c9e1-447f-8490-551005530fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
