{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f641fd82-e2b0-4d3e-8cc4-3cf78a4146ad",
   "metadata": {},
   "source": [
    "# TensorFlow Testing Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f62fda-1c77-4627-9dd4-c9174fb55964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "imports several Python libraries\n",
    "and modules commonly used in machine\n",
    "learning tasks\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be3370d-286f-41af-8c29-d49aab499cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"CSV\\\\export\\\\imputed_mimic_time_zone_1_inter_10.csv\")\n",
    "\n",
    "external_data = pd.read_csv(\"mimic_mean_final.csv\") # Not included yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108efe1-14c6-437a-8b97-90b97dfd9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72509654-c26f-4334-9384-e657d0cef385",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Select columns based on their data types.\n",
    "By specifying include=['number'], we select\n",
    "only columns with numeric data types.\n",
    "The .columns attribute then retrieves the\n",
    "names of these selected columns, storing them\n",
    "in the numeric_columns variable.\n",
    "\"\"\"\n",
    "numeric_columns = data.select_dtypes(include=['number']).columns\n",
    "\n",
    "\"\"\"\n",
    "Fill missing values with the mean\n",
    "value of each respective column. \n",
    "\"\"\"\n",
    "data.fillna(data[numeric_columns].mean(), inplace=True)\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Convert categorical variables to numerical\n",
    "\"\"\"\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\"\"\"\n",
    "Convert 'gender' column from\n",
    "categorical to numeric. \n",
    "Male become 1 and Female 0.\n",
    "The specific numeric values assigned\n",
    "to each category are determined based\n",
    "on the order of appearance of the unique\n",
    "categories in the data.\n",
    "\"\"\"\n",
    "data['gender'] = label_encoder.fit_transform(data['gender'])\n",
    "\n",
    "#reverse the gender from numerical to categorical.\n",
    "#data['gender'] = label_encoder.inverse_transform(data['gender'])\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "After this line of code is executed,\n",
    "the \"race\" column will be replaced\n",
    "with one or more columns, each\n",
    "representing a category of race, with\n",
    "binary values indicating the presence or\n",
    "absence of that category for each row.\n",
    "\"\"\"\n",
    "\n",
    "data = pd.get_dummies(data, columns=['race'], drop_first=True)\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "# Attention I split the training - test set at specific row\n",
    "\n",
    "\"\"\"\n",
    "I have calculate the split point.\n",
    "Every patient has 16 rows of observations,\n",
    "we don't want to have the same patient\n",
    "to be both in training and test set\n",
    "\"\"\"\n",
    "\n",
    "# Split the dataset at row 39040 for Mimic and 60384 for eICU\n",
    "split_index = 2441\n",
    "data_train = data.iloc[:split_index]\n",
    "data_test = data.iloc[split_index:]\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test used for training and testing\n",
    "the model. We remove columns that are not usefulls\n",
    "in training and testing ('los', 'subject_id', \n",
    "'hadm_id',  'Time_Zone', 'row_count') and we leave\n",
    "the rests which represent the wanted features.\n",
    "\n",
    "y_train and y_test represent the label\n",
    "\"\"\"\n",
    "# Split the dataset into features and label variable\n",
    "X_train = data_train.drop(['los', 'subject_id', 'hadm_id', 'Time_Zone', 'row_count'], axis=1)  # Features\n",
    "y_train = data_train['los']  # label variable\n",
    "\n",
    "X_test = data_test.drop(['los', 'subject_id', 'hadm_id', 'Time_Zone', 'row_count'], axis=1)  # Features\n",
    "y_test = data_test['los']  # label variable\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Computes the mean and standard deviation of each feature\n",
    "\"\"\"\n",
    "\n",
    "# Feature scaling (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e99a5cc-f3ae-45c1-b935-2362d32b5d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 4.0964\n",
      "Epoch 2/5\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1759\n",
      "Epoch 3/5\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9016\n",
      "Epoch 4/5\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7254\n",
      "Epoch 5/5\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5667\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ANN, it's a common way to build ANN in Keras\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Neural network with multiple hidden layers and one output.\n",
    "The ReLU activation function is used in the hidden layers\n",
    "to introduce non-linearity, while the output layer uses a\n",
    "linear activation function to produce continuous predictions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Best Hyperparameters: {'hidden_units': (64, 128, 64, 32), \n",
    "                       'optimizer': 'sgd', \n",
    "                       'loss': 'mean_absolute_error', \n",
    "                       'batch_size': 32,\n",
    "                       'epochs': 15}\n",
    "Best Validation Loss: 0.48111745715141296\n",
    "\"\"\"\n",
    "first_unit = 64\n",
    "second_unit = 128\n",
    "third_unit = 64\n",
    "fourth_unit = 32\n",
    "\n",
    "# Define input shape\n",
    "input_shape = X_train_scaled.shape[1]\n",
    "\n",
    "# Add input layer with input shape\n",
    "model.add(Input(shape=(input_shape,)))\n",
    "\n",
    "# Add first hidden layer\n",
    "model.add(Dense(units=first_unit, activation='relu'))\n",
    "\n",
    "# Add second hidden layer\n",
    "model.add(Dense(units=second_unit, activation='relu'))\n",
    "\n",
    "# Add third hidden layer\n",
    "model.add(Dense(units=third_unit, activation='relu'))\n",
    "\n",
    "# Add fourth hidden layer\n",
    "model.add(Dense(units=fourth_unit, activation='relu'))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "\"\"\"\n",
    "Compile the ANN model with optimizer and loss function\n",
    "\n",
    "Optimizers: adam, sgd, rmsprop, adagrad, adadelta, adamax, nadam\n",
    "\n",
    "Loss: mean_squared_error, mean_absolute_error, huber_loss,\n",
    "      mean_squared_logarithmic_error, binary_crossentropy,\n",
    "      categorical_crossentropy, sparse_categorical_crossentropy,\n",
    "      kullback_leibler_divergence\n",
    "\"\"\"\n",
    "# Compile the ANN\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "\"\"\"\n",
    "The batch size is a hyperparameter that defines the number of\n",
    "samples to work through before updating the internal model parameters.\n",
    "\"\"\"\n",
    "# Train the ANN on the training set\n",
    "model.fit(X_train_scaled, y_train, batch_size=64, epochs=5, verbose=1)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88bc8b0f-847f-4ab3-a29d-e3300365d970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error (MSE): 25.76924563017193\n",
      "Mean Absolute Error (MAE): 2.6280303475733695\n",
      "Root Mean Squared Error (RMSE): 5.076341756636557\n",
      "Mean Squared Logarithmic Error (MSLE): 0.2997686350490015\n",
      "R-squared (R2): 0.3050956139075053\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "print(\"Mean Square Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error (RMSE):\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "# For MSLE calculation must not have negative values in y_test and y_pred\n",
    "if (y_test >= 0).all() and (y_pred >= 0).all():\n",
    "    print(\"Mean Squared Logarithmic Error (MSLE):\", mean_squared_log_error(y_test, y_pred))\n",
    "else:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")\n",
    "print(\"R-squared (R2):\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2f1dd-486c-4e6b-97a4-c97c4d04bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Print some predictions\n",
    "print(predictions[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea63763-9cdf-4d83-8551-803df8caf788",
   "metadata": {},
   "source": [
    "# Hyperparameter (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0a1f4-5914-4287-96e0-1340fc85111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function creates an ANN taken two arguments:\n",
    "input_shape: The shape of the input features.\n",
    "hidden_units: A tuple specifying the number of units in each hidden layer.\n",
    "\n",
    "\"\"\"\n",
    "# Function to create the ANN model\n",
    "def create_model(input_shape, hidden_units):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(hidden_units[0], activation='relu'),\n",
    "        layers.Dense(hidden_units[1], activation='relu'),\n",
    "        layers.Dense(hidden_units[2], activation='relu'),\n",
    "        layers.Dense(hidden_units[3], activation='relu'),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate a model with given hyperparameters\n",
    "def train_and_evaluate_model(X_train, y_train, X_val, y_val, hyperparams):\n",
    "    model = create_model(X_train.shape[1:], hyperparams['hidden_units'])\n",
    "    model.compile(optimizer=hyperparams['optimizer'], loss=hyperparams['loss'])\n",
    "    model.fit(X_train, y_train, batch_size=hyperparams['batch_size'], epochs=hyperparams['epochs'], verbose=0)\n",
    "    loss = model.evaluate(X_val, y_val)\n",
    "    return loss\n",
    "\n",
    "# Define hyperparameters ranges\n",
    "hidden_units_range = [(64, 128, 64, 32), (32, 64, 32, 16)]  # Possible combinations of hidden units\n",
    "optimizer_choices = ['adam', 'sgd', 'rmsprop']  # Possible optimizers\n",
    "loss_choices = ['mean_squared_error', 'mean_absolute_error']  # Possible loss functions\n",
    "batch_size_range = [16, 32, 64]  # Possible batch sizes\n",
    "epochs_range = [5, 10, 15]  # Possible number of epochs\n",
    "\n",
    "# Define population size\n",
    "population_size = 5\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize population with random hyperparameters\n",
    "population = []\n",
    "for _ in range(population_size):\n",
    "    hyperparams = {\n",
    "        'hidden_units': hidden_units_range[np.random.choice(len(hidden_units_range))],\n",
    "        'optimizer': np.random.choice(optimizer_choices),\n",
    "        'loss': np.random.choice(loss_choices),\n",
    "        'batch_size': np.random.choice(batch_size_range),\n",
    "        'epochs': np.random.choice(epochs_range)\n",
    "    }\n",
    "    population.append(hyperparams)\n",
    "\n",
    "best_hyperparams = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Perform Population-Based Training (PBT)\n",
    "for generation in range(5):  # Number of generations\n",
    "    print(f\"Generation {generation+1}:\")\n",
    "    for i, hyperparams in enumerate(population):\n",
    "        print(f\"Model {i+1}: Hyperparameters - {hyperparams}\")\n",
    "        loss = train_and_evaluate_model(X_train, y_train, X_val, y_val, hyperparams)\n",
    "        print(f\"        Validation Loss: {loss}\")\n",
    "        \n",
    "        # Update best hyperparameters if the current model performs better\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_hyperparams = hyperparams\n",
    "    \n",
    "    # Update hyperparameters based on performance\n",
    "    for i, hyperparams in enumerate(population):\n",
    "        for j, other_hyperparams in enumerate(population):\n",
    "            if i != j and np.random.rand() < 0.5:  # Randomly choose whether to exchange hyperparameters\n",
    "                other_loss = train_and_evaluate_model(X_train, y_train, X_val, y_val, other_hyperparams)\n",
    "                if loss < other_loss * 0.9:  # Threshold for replacing hyperparameters\n",
    "                    other_hyperparams['hidden_units'] = hyperparams['hidden_units']\n",
    "                    other_hyperparams['optimizer'] = hyperparams['optimizer']\n",
    "                    other_hyperparams['loss'] = hyperparams['loss']\n",
    "                    other_hyperparams['batch_size'] = hyperparams['batch_size']\n",
    "                    other_hyperparams['epochs'] = hyperparams['epochs']\n",
    "                    print(f\"        Updated hyperparameters for model {j+1}: {other_hyperparams}\")\n",
    "\n",
    "    # Perturb hyperparameters\n",
    "    for hyperparams in population:\n",
    "        if np.random.rand() < 0.5:\n",
    "            hyperparams['hidden_units'] = hidden_units_range[np.random.choice(len(hidden_units_range))]\n",
    "        if np.random.rand() < 0.5:\n",
    "            hyperparams['optimizer'] = np.random.choice(optimizer_choices)\n",
    "        if np.random.rand() < 0.5:\n",
    "            hyperparams['loss'] = np.random.choice(loss_choices)\n",
    "        if np.random.rand() < 0.5:\n",
    "            hyperparams['batch_size'] = np.random.choice(batch_size_range)\n",
    "        if np.random.rand() < 0.5:\n",
    "            hyperparams['epochs'] = np.random.choice(epochs_range)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "print(\"Best Validation Loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed81e0c-bff9-4954-a7e5-2cf1413eb3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
