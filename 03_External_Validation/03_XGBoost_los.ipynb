{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6243b02-165d-496a-a198-f00704a71eb6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671463d-f9c0-4542-b28c-eec0dfb36e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import textwrap\n",
    "import logging\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterGrid, train_test_split, ParameterSampler\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error, mean_squared_log_error, r2_score\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor # Regression Impute\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8efbb-083c-4230-b048-59045707bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"logging.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d8750f-9855-4776-9dc5-683e5edd6c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name for the files\n",
    "file_name = \"100\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25beddeb-cbdd-4fc6-bcd2-dc740997c4da",
   "metadata": {},
   "source": [
    "# Reads | Filter Patients (Phase 01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a182eba9-001e-43e2-b928-7bdd96cf667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MIMICs CSV file\n",
    "mimic_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\mimic_mean_median_min_max_final.csv\")\n",
    "\n",
    "# Read eICUs CSV file\n",
    "eicu_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\eicu_mean_median_min_max_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae581b-694b-43c7-9805-90856381db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 10\n",
    "\n",
    "# Filter icu stay less than 10 days\n",
    "mimic_df = mimic_df[mimic_df['los'] < day]\n",
    "\n",
    "# Filter icu stay less than 10 days\n",
    "eicu_df = eicu_df[eicu_df['los'] < day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc6cae-2821-43c1-a6b1-04c79b38273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Time Zone\n",
    "\n",
    "#time_zone = 16\n",
    "#mimic_df = mimic_df[mimic_df['Time_Zone'] == time_zone]\n",
    "#eicu_df = eicu_df[eicu_df['Time_Zone'] == time_zone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894f6245-e142-45de-971e-fb52f9a1ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I'm gonna concat and split the mimic and icu\n",
    "at this point. I must create the same columns\n",
    "from the tranformation of categorical data.\n",
    "\"\"\"\n",
    "row_count = mimic_df.shape[0]\n",
    "print(f\"Row count: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9462d7a8-d964-43b5-9e61-6064e02f0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dataframes\n",
    "df_combined = pd.concat([mimic_df, eicu_df], ignore_index=True)\n",
    "\n",
    "# Find all categorical columns in mimic\n",
    "categorical_columns = df_combined.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Apply one-hot encoding to all categorical columns\n",
    "df_encoded = pd.get_dummies(df_combined, columns=categorical_columns)\n",
    "\n",
    "# Split the concatenate dataframe\n",
    "mimic_df = df_encoded.iloc[:row_count, :]\n",
    "eicu_df = df_encoded.iloc[row_count:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1786ba0-60a9-4593-87be-689ee23fffe2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Split Training - Validation - Test Set Balanced (Phase 02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308615a-2615-468c-a99f-a0b2bf265697",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_val_perc = 0.2\n",
    "split_between_test_val_perc = 0.5\n",
    "\n",
    "# Group data by subject_id and hadm_id\n",
    "grouped_df = mimic_df.groupby(['subject_id', 'hadm_id'])\n",
    "\n",
    "# Get a new dataframe with one row per patient (subject_id, hadm_id) pair\n",
    "patient_df = grouped_df['hospital_expire_flag'].first().reset_index()\n",
    "\n",
    "# Split the patient_df into training (80%), validation (10%), and test (10%) while keeping the ratio of hospital_expired_flag\n",
    "train, temp = train_test_split(patient_df, test_size=total_test_val_perc, stratify=patient_df['hospital_expire_flag'], random_state=42)\n",
    "val, test = train_test_split(temp, test_size=split_between_test_val_perc, stratify=temp['hospital_expire_flag'], random_state=42)\n",
    "\n",
    "# Step 4: Merge back with the original df to get the rows for each patient in the splits\n",
    "train_df = mimic_df.merge(train[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "val_df = mimic_df.merge(val[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "test_df = mimic_df.merge(test[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "\n",
    "# Check the sizes of the splits\n",
    "print(f'Training set size: {train_df.shape[0]}')\n",
    "print(f'Validation set size: {val_df.shape[0]}')\n",
    "print(f'Test set size: {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b1d8d9-e1f5-402f-8107-c3de39ed735c",
   "metadata": {},
   "source": [
    "# Split Training - Validation - Test Set Unbalanced (Phase 02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2df0f8-294c-4947-b408-50296e8f26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_val_perc = 0.2\n",
    "split_between_test_val_perc = 0.5\n",
    "\n",
    "# Group data by subject_id and hadm_id\n",
    "grouped_df = mimic_df.groupby(['subject_id', 'hadm_id'])\n",
    "\n",
    "# One row per patient (subject_id, hadm_id)\n",
    "patient_df = grouped_df['hospital_expire_flag'].first().reset_index()\n",
    "\n",
    "# Step 1: Hold out 20% of the data for val + test (unbalanced)\n",
    "train_candidates, temp = train_test_split(\n",
    "    patient_df,\n",
    "    test_size=total_test_val_perc,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Step 2: Create a 4:1 survive:die ratio in the training set\n",
    "pos = train_candidates[train_candidates['hospital_expire_flag'] == 1]\n",
    "neg = train_candidates[train_candidates['hospital_expire_flag'] == 0]\n",
    "\n",
    "# Compute desired number of negatives\n",
    "desired_neg_count = min(len(neg), 4 * len(pos))\n",
    "\n",
    "# Downsample negatives\n",
    "neg_downsampled = neg.sample(n=desired_neg_count, random_state=42)\n",
    "\n",
    "# Combine and shuffle\n",
    "train = pd.concat([pos, neg_downsampled]).sample(frac=1, random_state=42)\n",
    "\n",
    "# Step 3: Split temp into val and test (unbalanced)\n",
    "val, test = train_test_split(\n",
    "    temp,\n",
    "    test_size=split_between_test_val_perc,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Step 4: Merge back to get full rows\n",
    "train_df = mimic_df.merge(train[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "val_df = mimic_df.merge(val[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "test_df = mimic_df.merge(test[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "\n",
    "# Print split sizes\n",
    "logging.info(f'Training set size: {train_df.shape[0]}')\n",
    "logging.info(f'Validation set size: {val_df.shape[0]}')\n",
    "logging.info(f'Test set size: {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47462f19-6295-4cb1-a8f7-64b8bd104609",
   "metadata": {},
   "source": [
    "# Check ratio and unique patients between sets (Phase 03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ed3bc-b7ae-48fe-892f-66b6f814c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count on Training set survive and non-survive\n",
    "survival_counts = train_df['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/16\n",
    "temp_non_survive = survival_counts.get(1, 0)/16\n",
    "\n",
    "# Display the results\n",
    "logging.info(f'Train Set')\n",
    "logging.info(f'Survive: {temp_survive}')\n",
    "logging.info(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "logging.info(f'Ratio Set: {ratio:.2f}:1')\n",
    "\n",
    "\"\"\"----------------------------\"\"\"\n",
    "\n",
    "# Count on validation set survive and non-survive\n",
    "survival_counts = val_df['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/16\n",
    "temp_non_survive = survival_counts.get(1, 0)/16\n",
    "\n",
    "# Display the results\n",
    "logging.info(f'\\nValidation Set')\n",
    "logging.info(f'Survive: {temp_survive}')\n",
    "logging.info(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "logging.info(f'Ratio Set: {ratio:.2f}:1')\n",
    "\n",
    "\"\"\"----------------------------\"\"\"\n",
    "\n",
    "# Count on validation set survive and non-survive\n",
    "survival_counts = test_df['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/16\n",
    "temp_non_survive = survival_counts.get(1, 0)/16\n",
    "\n",
    "# Display the results\n",
    "logging.info(f'\\nTest Set')\n",
    "logging.info(f'Survive: {temp_survive}')\n",
    "logging.info(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "logging.info(f'Ratio Set: {ratio:.2f}:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73205561-9bcc-42fe-acc4-8bcd1616d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mine unique subject_id from sets\n",
    "train_subjects = set(train_df['subject_id'].unique())\n",
    "val_subjects = set(val_df['subject_id'].unique())\n",
    "test_subjects = set(test_df['subject_id'].unique())\n",
    "\n",
    "# Check if there are overlaping subject_id\n",
    "train_val_overlap = train_subjects.intersection(val_subjects)\n",
    "train_test_overlap = train_subjects.intersection(test_subjects)\n",
    "val_test_overlap = val_subjects.intersection(test_subjects)\n",
    "\n",
    "# Display the results\n",
    "logging.info(f'Overlap between training and validation sets: {len(train_val_overlap)}')\n",
    "logging.info(f'Overlap between training and test sets: {len(train_test_overlap)}')\n",
    "logging.info(f'Overlap between validation and test sets: {len(val_test_overlap)}')\n",
    "\n",
    "# print overlaping\n",
    "if train_val_overlap:\n",
    "    logging.info(f'Subjects in both training and validation: {train_val_overlap}')\n",
    "if train_test_overlap:\n",
    "    logging.info(f'Subjects in both training and test: {train_test_overlap}')\n",
    "if val_test_overlap:\n",
    "    logging.info(f'Subjects in both validation and test: {val_test_overlap}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d473a75a-86eb-4dac-be9a-b73cf80b111b",
   "metadata": {},
   "source": [
    "# Split label from Train - Validation - Test Sets (Phase 04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62e244-e475-4222-802b-57d374b63c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External validation from eICU\n",
    "X_external = eicu_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_external = eicu_df['los']\n",
    "\n",
    "# Separate features and target for the training, validation, and test sets\n",
    "X_train = train_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_train = train_df['los']\n",
    "\n",
    "X_validate = val_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_validate = val_df['los']\n",
    "\n",
    "X_test = test_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_test = test_df['los']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b069035-3469-4e8c-8849-9225e0fad52e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load Train - Validation - Test & External Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0de75c-f5da-475a-86ec-fad496d7c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the saved files\n",
    "load_path = 'CSV/exports/impute/o03_Interpolation/'\n",
    "\n",
    "# Load external validation set from eICU\n",
    "#X_external = pd.read_csv(load_path + 'X_external.csv')\n",
    "#y_external = pd.read_csv(load_path + 'y_external.csv')\n",
    "\n",
    "# Load training, validation, and test sets\n",
    "X_train = pd.read_csv(load_path + 'X_train.csv')\n",
    "#y_train = pd.read_csv(load_path + 'y_train.csv')\n",
    "\n",
    "#X_validate = pd.read_csv(load_path + 'X_validate.csv')\n",
    "#y_validate = pd.read_csv(load_path + 'y_validate.csv')\n",
    "\n",
    "#X_test = pd.read_csv(load_path + 'X_test.csv')\n",
    "#y_test = pd.read_csv(load_path + 'y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75bc50d-e5e2-44a3-9eb9-57a3adcdb72b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fill Missing Values by Mean (Phase 05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa326f0e-f29f-44c5-8cc3-13cc5551b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imputer object with mean strategy\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit the imputer on X_train and transform the training set\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train_imputed, columns=X_train.columns)\n",
    "\n",
    "# Transform the validation, test, and external sets using the same imputer\n",
    "X_validate_imputed = imputer.transform(X_validate)\n",
    "X_validate = pd.DataFrame(X_validate_imputed, columns=X_validate.columns)\n",
    "\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test_imputed, columns=X_test.columns)\n",
    "\n",
    "X_external_imputed = imputer.transform(X_external)\n",
    "X_external = pd.DataFrame(X_external_imputed, columns=X_external.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59484a45-6411-421b-99db-fe39c8fc5ff0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fill Missing Values by KNN (Phase 05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac37e45-0585-49a5-91b7-0336cb61c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chunked transformation function with a progress bar\n",
    "def chunked_transform(imputer, X, chunk_size):\n",
    "    X_imputed = []\n",
    "    for i in tqdm(range(0, X.shape[0], chunk_size), desc=\"Imputing\"):\n",
    "        chunk = X.iloc[i:i+chunk_size]\n",
    "        imputed_chunk = imputer.transform(chunk)\n",
    "        X_imputed.append(imputed_chunk)\n",
    "    return pd.DataFrame(np.vstack(X_imputed), columns=X.columns)\n",
    "\n",
    "# Step 1: Create and fit the KNN imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "knn_imputer.fit(X_train)\n",
    "\n",
    "# Step 2: Apply imputation with specific chunk sizes for each dataset\n",
    "X_train = chunked_transform(knn_imputer, X_train, chunk_size=1000)\n",
    "X_validate = chunked_transform(knn_imputer, X_validate, chunk_size=512)\n",
    "X_test = chunked_transform(knn_imputer, X_test, chunk_size=512)\n",
    "X_external = chunked_transform(knn_imputer, X_external, chunk_size=2000)\n",
    "\n",
    "# Step 3: Save to CSV\n",
    "X_train.to_csv(\"CSV/exports/impute/o01_KNN/X_train.csv\", index=False)\n",
    "X_validate.to_csv(\"CSV/exports/impute/o01_KNN/X_validate.csv\", index=False)\n",
    "X_test.to_csv(\"CSV/exports/impute/o01_KNN/X_test.csv\", index=False)\n",
    "X_external.to_csv(\"CSV/exports/impute/o01_KNN/X_external.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa0f63-07cd-433d-a8cf-443c4c9fc982",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fill Missing Values by Regression Imputation (Phase 05)\n",
    "## Save the datasets and retrieve them for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c647bb-2c66-4ab3-9127-0b78c1a1427e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I should change the estimator and I'm gonna execute\n",
    "scale and normalization first.\n",
    "\n",
    "In the defult settings there is a big chagnes between\n",
    "round 1 and round 3. The number of imputation makes\n",
    "significant imcreasement. We expect from round to round\n",
    "the change to be smaller. This behavior of causing the\n",
    "model to make massive adjustments to certain predictions\n",
    "shows that maybe one or more features have high variance\n",
    "or outliers, thats why I'm gonna execute before imputation\n",
    "scale and normalization.\n",
    "\"\"\"\n",
    "\n",
    "# Define the imputer\n",
    "iter_imputer = IterativeImputer(estimator=ExtraTreesRegressor(n_estimators=10, random_state=0),\n",
    "                                 max_iter=20, random_state=0, verbose=0)\n",
    "\n",
    "# --- Impute Training Set ---\n",
    "print(\"Imputing missing values on training set...\")\n",
    "with tqdm(total=1, desc=\"X_train\") as pbar:\n",
    "    X_train_imputed = iter_imputer.fit_transform(X_train)\n",
    "    X_train = pd.DataFrame(X_train_imputed, columns=X_train.columns)\n",
    "    pbar.update(1)\n",
    "\n",
    "# --- Impute Validation Set ---\n",
    "print(\"Imputing missing values on validation set...\")\n",
    "with tqdm(total=1, desc=\"X_validate\") as pbar:\n",
    "    X_validate_imputed = iter_imputer.transform(X_validate)\n",
    "    X_validate = pd.DataFrame(X_validate_imputed, columns=X_validate.columns)\n",
    "    pbar.update(1)\n",
    "\n",
    "# --- Impute Test Set ---\n",
    "print(\"Imputing missing values on test set...\")\n",
    "with tqdm(total=1, desc=\"X_test\") as pbar:\n",
    "    X_test_imputed = iter_imputer.transform(X_test)\n",
    "    X_test = pd.DataFrame(X_test_imputed, columns=X_test.columns)\n",
    "    pbar.update(1)\n",
    "\n",
    "# --- Impute External Validation Set ---\n",
    "print(\"Imputing missing values on external validation set...\")\n",
    "with tqdm(total=1, desc=\"X_external\") as pbar:\n",
    "    X_external_imputed = iter_imputer.transform(X_external)\n",
    "    X_external = pd.DataFrame(X_external_imputed, columns=X_external.columns)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab459737-2116-45fd-817b-7062b5618b9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fill Missing Values Interpolation Imputation (Phase 05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2135a-13a7-4f2b-acad-a2d7a29cdcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation method (you can use 'linear', 'polynomial', 'spline', etc.)\n",
    "interpolation_method = 'linear'\n",
    "\n",
    "# Interpolate missing values in X_train\n",
    "X_train = X_train.interpolate(method=interpolation_method, axis=0, limit_direction='both')\n",
    "\n",
    "# Interpolate missing values in X_validate\n",
    "X_validate = X_validate.interpolate(method=interpolation_method, axis=0, limit_direction='both')\n",
    "\n",
    "# Interpolate missing values in X_test\n",
    "X_test = X_test.interpolate(method=interpolatbion_method, axis=0, limit_direction='both')\n",
    "\n",
    "# Interpolate missing values in X_external\n",
    "X_external = X_external.interpolate(method=interpolation_method, axis=0, limit_direction='both')\n",
    "\n",
    "# Resetting indexes after interpolation to avoid misalignments\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_validate = X_validate.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "X_external = X_external.reset_index(drop=True)\n",
    "\n",
    "# Optionally, you can check if there are still any missing values\n",
    "print(f\"Missing values in X_train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in X_validate: {X_validate.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in X_test: {X_test.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in X_external: {X_external.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56848f0f-b36c-4b59-8e8c-8c9f4807a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation method\n",
    "interpolation_method = 'linear'\n",
    "\n",
    "# Define your datasets\n",
    "datasets = {\n",
    "    \"X_train\": X_train,\n",
    "    \"X_validate\": X_validate,\n",
    "    \"X_test\": X_test,\n",
    "    \"X_external\": X_external\n",
    "}\n",
    "\n",
    "# Create a progress bar using tqdm\n",
    "for name in tqdm(datasets, desc=\"Interpolating datasets\"):\n",
    "    df = datasets[name]\n",
    "    df_interpolated = df.interpolate(method=interpolation_method, axis=0, limit_direction='both')\n",
    "    df_interpolated = df_interpolated.reset_index(drop=True)\n",
    "    datasets[name] = df_interpolated\n",
    "    print(f\"Missing values in {name}: {df_interpolated.isnull().sum().sum()}\")\n",
    "\n",
    "# Unpack the updated datasets\n",
    "X_train = datasets[\"X_train\"]\n",
    "X_validate = datasets[\"X_validate\"]\n",
    "X_test = datasets[\"X_test\"]\n",
    "X_external = datasets[\"X_external\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea1845-e2ed-4c86-bdd8-bc7cf8ffe8e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fill Missing Values by ANN (Phase 05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3216eb-3d59-4548-a39c-482e782e2778",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Feedforward MLP (Multilayer Perceptron)\n",
    "\"\"\"\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to impute missing values for any dataset independently\n",
    "def impute_missing_values_ann(X, dataset_name):\n",
    "    print(f\"Imputing missing values for {dataset_name}\")\n",
    "    \n",
    "    # Step 1: Identify columns with missing values\n",
    "    missing_columns = X.columns[X.isnull().any()].tolist()\n",
    "    print(f\"Columns with missing values: {missing_columns}\")\n",
    "\n",
    "    # Step 2: Loop through each column with missing values and build an ANN to predict missing values\n",
    "    for col in missing_columns:\n",
    "        print(f\"Filling missing values in column: {col}\")\n",
    "\n",
    "        # Separate rows with and without missing values in the current column\n",
    "        missing_rows = X[X[col].isnull()]\n",
    "        non_missing_rows = X[~X[col].isnull()]\n",
    "\n",
    "        # Skip the column if no data is available for training\n",
    "        if len(missing_rows) == 0 or len(non_missing_rows) == 0:\n",
    "            print(f\"Skipping {col}, insufficient data\")\n",
    "            continue\n",
    "\n",
    "        # Separate features and target for non-missing rows\n",
    "        X_non_missing = non_missing_rows.drop(columns=missing_columns).copy()  # Exclude other missing columns from features\n",
    "        y_non_missing = non_missing_rows[col].copy()  # Target is the column we're filling\n",
    "\n",
    "        # Features for the rows with missing values (we'll predict the column for these rows)\n",
    "        X_missing = missing_rows.drop(columns=missing_columns).copy()\n",
    "\n",
    "        # Step 3: Ensure all data is numeric and convert to float if necessary\n",
    "        X_non_missing = X_non_missing.apply(pd.to_numeric, errors='coerce').astype(float)\n",
    "        y_non_missing = y_non_missing.astype(float)\n",
    "        X_missing = X_missing.apply(pd.to_numeric, errors='coerce').astype(float)\n",
    "\n",
    "        # Step 4: Build the ANN model for filling missing values\n",
    "        model_missing = Sequential()\n",
    "        model_missing.add(Input(shape=(X_non_missing.shape[1],)))  # Input layer\n",
    "        model_missing.add(Dense(units=128, activation='relu'))\n",
    "        model_missing.add(Dropout(0.4))\n",
    "        model_missing.add(Dense(units=64, activation='relu'))\n",
    "        model_missing.add(Dropout(0.4))\n",
    "        model_missing.add(Dense(units=32, activation='relu'))\n",
    "        model_missing.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "\n",
    "        # Compile the model\n",
    "        model_missing.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "        # Early stopping to avoid overfitting\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "        # Step 5: Train the model\n",
    "        model_missing.fit(X_non_missing, y_non_missing, \n",
    "                          epochs=50, batch_size=32, \n",
    "                          validation_split=0.1, \n",
    "                          callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "        # Step 6: Predict the missing values\n",
    "        predicted_values = model_missing.predict(X_missing)\n",
    "\n",
    "        # Step 7: Fill the missing values in the dataset\n",
    "        X.loc[X[col].isnull(), col] = predicted_values\n",
    "\n",
    "        print(f\"Filled missing values in column: {col}\")\n",
    "\n",
    "    # Verify if there are any remaining missing values in the dataset\n",
    "    print(X.isnull().sum())\n",
    "    return X\n",
    "\n",
    "# Impute missing values for each dataset independently to avoid data leakage\n",
    "\n",
    "# Impute missing values for X_train\n",
    "X_train_imputed = impute_missing_values_ann(X_train, \"X_train\")\n",
    "\n",
    "# Impute missing values for X_test\n",
    "X_test_imputed = impute_missing_values_ann(X_test, \"X_test\")\n",
    "\n",
    "# Impute missing values for X_validate\n",
    "X_validate_imputed = impute_missing_values_ann(X_validate, \"X_validate\")\n",
    "\n",
    "# Impute missing values for X_external\n",
    "\n",
    "X_external_imputed = impute_missing_values_ann(X_external, \"X_external\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5b212-81cd-4a10-98c2-937b68168a73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RNN\n",
    "\"\"\"\n",
    "# Define a function to impute missing values for any dataset independently using an RNN\n",
    "def impute_missing_values_rnn(X, dataset_name):\n",
    "    print(f\"Imputing missing values for {dataset_name}\")\n",
    "    \n",
    "    # Step 1: Identify columns with missing values\n",
    "    missing_columns = X.columns[X.isnull().any()].tolist()\n",
    "    print(f\"Columns with missing values: {missing_columns}\")\n",
    "\n",
    "    # Step 2: Loop through each column with missing values and build an RNN to predict missing values\n",
    "    for col in missing_columns:\n",
    "        print(f\"Filling missing values in column: {col}\")\n",
    "\n",
    "        # Separate rows with and without missing values in the current column\n",
    "        missing_rows = X[X[col].isnull()]\n",
    "        non_missing_rows = X[~X[col].isnull()]\n",
    "\n",
    "        # Skip the column if no data is available for training\n",
    "        if len(missing_rows) == 0 or len(non_missing_rows) == 0:\n",
    "            print(f\"Skipping {col}, insufficient data\")\n",
    "            continue\n",
    "\n",
    "        # Separate features and target for non-missing rows\n",
    "        X_non_missing = non_missing_rows.drop(columns=missing_columns).copy()  # Exclude other missing columns from features\n",
    "        y_non_missing = non_missing_rows[col].copy()  # Target is the column we're filling\n",
    "\n",
    "        # Features for the rows with missing values (we'll predict the column for these rows)\n",
    "        X_missing = missing_rows.drop(columns=missing_columns).copy()\n",
    "\n",
    "        # Step 3: Ensure all data is numeric and convert to float if necessary\n",
    "        X_non_missing = X_non_missing.apply(pd.to_numeric, errors='coerce').astype(float)\n",
    "        y_non_missing = y_non_missing.astype(float)\n",
    "        X_missing = X_missing.apply(pd.to_numeric, errors='coerce').astype(float)\n",
    "\n",
    "        # Step 4: Reshape the data to 3D for RNN input (samples, timesteps=1, features)\n",
    "        X_non_missing_rnn = np.expand_dims(X_non_missing, axis=1)  # Add timestep dimension\n",
    "        X_missing_rnn = np.expand_dims(X_missing, axis=1)\n",
    "\n",
    "        # Step 5: Build the RNN model for filling missing values with Input layer\n",
    "        model_missing = Sequential()\n",
    "        model_missing.add(Input(shape=(1, X_non_missing.shape[1])))  # Input layer with timestep=1 and number of features\n",
    "        model_missing.add(LSTM(units=64, return_sequences=False))  # LSTM layer\n",
    "        model_missing.add(Dropout(0.3))\n",
    "        model_missing.add(Dense(units=32, activation='relu'))\n",
    "        model_missing.add(Dense(units=1, activation='linear'))  # Output layer for regression (predicting the missing value)\n",
    "\n",
    "        # Compile the model\n",
    "        model_missing.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "        # Early stopping to avoid overfitting\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "        # Step 6: Train the model\n",
    "        model_missing.fit(X_non_missing_rnn, y_non_missing, \n",
    "                          epochs=50, batch_size=32, \n",
    "                          validation_split=0.1, \n",
    "                          callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "        # Step 7: Predict the missing values\n",
    "        predicted_values = model_missing.predict(X_missing_rnn)\n",
    "\n",
    "        # Step 8: Fill the missing values in the dataset\n",
    "        X.loc[X[col].isnull(), col] = predicted_values.flatten()\n",
    "\n",
    "        print(f\"Filled missing values in column: {col}\")\n",
    "\n",
    "    # Verify if there are any remaining missing values in the dataset\n",
    "    print(X.isnull().sum())\n",
    "    return X\n",
    "\n",
    "# Impute missing values for each dataset independently to avoid data leakage\n",
    "\n",
    "# Impute missing values for X_train\n",
    "X_train_imputed = impute_missing_values_rnn(X_train, \"X_train\")\n",
    "\n",
    "# Impute missing values for X_test\n",
    "X_test_imputed = impute_missing_values_rnn(X_test, \"X_test\")\n",
    "\n",
    "# Impute missing values for X_validate\n",
    "X_validate_imputed = impute_missing_values_rnn(X_validate, \"X_validate\")\n",
    "\n",
    "# Impute missing values for X_external\n",
    "X_external_imputed = impute_missing_values_rnn(X_external, \"X_external\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab85f8-775d-4aaa-8338-ecb29e51acdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Scale and Normalize (Phase 06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08718dd-5868-431f-b55d-f6bbd9468ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Just Scale\n",
    "\"\"\"\n",
    "\n",
    "# Identify binary categorical features\n",
    "def identify_binary_categorical_features(df):\n",
    "    binary_categorical_features = []\n",
    "    for col in df.columns:\n",
    "        unique_values = df[col].unique()\n",
    "        if set(unique_values) == {0.0, 1.0}:\n",
    "            binary_categorical_features.append(col)\n",
    "    return binary_categorical_features\n",
    "\n",
    "\n",
    "# Identify binary categorical features\n",
    "binary_categorical_features = identify_binary_categorical_features(X_train)\n",
    "\n",
    "# Exclude binary categorical features from scaling\n",
    "numerical_features = [col for col in X_train.columns if col not in binary_categorical_features]\n",
    "\n",
    "# Create scalers\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "# Fit and transform only numerical features\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[numerical_features] = standard_scaler.fit_transform(X_train[numerical_features])\n",
    "X_train = X_train_scaled\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "\n",
    "# Fit and transform only numerical features\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[numerical_features] = standard_scaler.fit_transform(X_test[numerical_features])\n",
    "X_test = X_test_scaled\n",
    "\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "\n",
    "# Fit and transform only numerical features\n",
    "X_validate_scaled = X_validate.copy()\n",
    "X_validate_scaled[numerical_features] = standard_scaler.fit_transform(X_validate[numerical_features])\n",
    "X_validate = X_validate_scaled\n",
    "\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "\n",
    "# Fit and transform only numerical features\n",
    "X_external_scaled = X_external.copy()\n",
    "X_external_scaled[numerical_features] = standard_scaler.fit_transform(X_external[numerical_features])\n",
    "X_external = X_external_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fe676-faa5-4fa8-a568-37b64f3e63ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Only Normalization\n",
    "\"\"\"\n",
    "\n",
    "# Identify binary categorical features\n",
    "def identify_binary_categorical_features(df):\n",
    "    binary_categorical_features = []\n",
    "    for col in df.columns:\n",
    "        unique_values = df[col].unique()\n",
    "        if set(unique_values) == {0.0, 1.0}:\n",
    "            binary_categorical_features.append(col)\n",
    "    return binary_categorical_features\n",
    "\n",
    "\n",
    "# Identify binary categorical features\n",
    "binary_categorical_features = identify_binary_categorical_features(X_train)\n",
    "\n",
    "# Exclude binary categorical features from normalization\n",
    "numerical_features = [col for col in X_train.columns if col not in binary_categorical_features]\n",
    "\n",
    "# Create the Min-Max scaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "# Fit and transform only numerical features (normalization)\n",
    "X_train_normalized = X_train.copy()\n",
    "X_train_normalized[numerical_features] = minmax_scaler.fit_transform(X_train[numerical_features])\n",
    "X_train = X_train_normalized\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "\n",
    "# Fit and transform only numerical features (normalization)\n",
    "X_test_normalized = X_test.copy()\n",
    "X_test_normalized[numerical_features] = minmax_scaler.fit_transform(X_test[numerical_features])\n",
    "X_test = X_test_normalized\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "\n",
    "# Fit and transform only numerical features (normalization)\n",
    "X_validate_normalized = X_validate.copy()\n",
    "X_validate_normalized[numerical_features] = minmax_scaler.fit_transform(X_validate[numerical_features])\n",
    "X_validate = X_validate_normalized\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "\n",
    "# Fit and transform only numerical features (normalization)\n",
    "X_external_normalized = X_external.copy()\n",
    "X_external_normalized[numerical_features] = minmax_scaler.fit_transform(X_external[numerical_features])\n",
    "X_external = X_external_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7c33af-292e-4e18-9f4c-04aa7ae1d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scale and Normalize\n",
    "\"\"\"\n",
    "\n",
    "# Identify binary categorical features\n",
    "def identify_binary_categorical_features(df):\n",
    "    binary_categorical_features = []\n",
    "    for col in df.columns:\n",
    "        unique_values = df[col].unique()\n",
    "        if set(unique_values) == {0.0, 1.0}:\n",
    "            binary_categorical_features.append(col)\n",
    "    return binary_categorical_features\n",
    "\n",
    "\n",
    "# Identify binary categorical features\n",
    "binary_categorical_features = identify_binary_categorical_features(X_train)\n",
    "\n",
    "# Exclude binary categorical features from scaling\n",
    "numerical_features = [col for col in X_train.columns if col not in binary_categorical_features]\n",
    "\n",
    "# Create scalers\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "# Fit and transform only numerical features\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[numerical_features] = standard_scaler.fit_transform(X_train[numerical_features])\n",
    "X_train = X_train_scaled\n",
    "\n",
    "# Optionally, apply Min-Max normalization\n",
    "X_train_normalized = X_train_scaled.copy()\n",
    "X_train_normalized[numerical_features] = minmax_scaler.fit_transform(X_train_scaled[numerical_features])\n",
    "X_train = X_train_normalized\n",
    "\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "\n",
    "# Fit and transform only numerical features\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[numerical_features] = standard_scaler.fit_transform(X_test[numerical_features])\n",
    "X_test = X_test_scaled\n",
    "\n",
    "# Optionally, apply Min-Max normalization\n",
    "X_test_normalized = X_test_scaled.copy()\n",
    "X_test_normalized[numerical_features] = minmax_scaler.fit_transform(X_test_scaled[numerical_features])\n",
    "X_test = X_test_normalized\n",
    "\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "\n",
    "# Fit and transform only numerical features\n",
    "X_validate_scaled = X_validate.copy()\n",
    "X_validate_scaled[numerical_features] = standard_scaler.fit_transform(X_validate[numerical_features])\n",
    "X_validate = X_validate_scaled\n",
    "\n",
    "# Optionally, apply Min-Max normalization\n",
    "X_validate_normalized = X_validate_scaled.copy()\n",
    "X_validate_normalized[numerical_features] = minmax_scaler.fit_transform(X_validate_scaled[numerical_features])\n",
    "X_validate = X_validate_normalized\n",
    "\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "\n",
    "# Fit and transform only numerical features\n",
    "X_external_scaled = X_external.copy()\n",
    "X_external_scaled[numerical_features] = standard_scaler.fit_transform(X_external[numerical_features])\n",
    "X_external = X_external_scaled\n",
    "\n",
    "# Optionally, apply Min-Max normalization\n",
    "X_external_normalized = X_external_scaled.copy()\n",
    "X_external_normalized[numerical_features] = minmax_scaler.fit_transform(X_external_scaled[numerical_features])\n",
    "X_external = X_external_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f727d0-6369-4e0e-9d2f-ae447500734d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HP GridSearchCV\n",
    "## To slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e5d12-f1bd-4242-9d64-35a14ba91996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A smaller learning rate makes the boosting\n",
    "process more robust and can lead to better\n",
    "generalization but requires more trees\n",
    "(higher n_estimators) to achieve the same result.\n",
    "A larger learning rate speeds up training bu\n",
    "may risk overfitting.\n",
    "\"\"\"\n",
    "\n",
    "# Define the parameter grid\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300], # controls the total number of trees in the ensemble\n",
    "    'learning_rate': np.arange(0.01, 1.02, 0.2),\n",
    "    'max_depth': np.arange(1, 10, 1),\n",
    "    'reg_lambda': np.arange(0.1, 15.1, 1),\n",
    "    'reg_alpha': np.arange(0.1, 15.1, 1)\n",
    "}\n",
    "\n",
    "# Create an XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', \n",
    "                           cv=3,  # Number of folds for cross-validation\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score (negative MSE):\", grid_search.best_score_)\n",
    "\n",
    "# Predict on the validation set with the best model\n",
    "y_pred_validate = grid_search.predict(X_validate)\n",
    "\n",
    "# Optionally: Evaluate the model on the validation set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_validate, y_pred_validate)\n",
    "print(\"Validation MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae09b09f-e87b-436b-bf5d-a35bb597659b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HP RandomizedSearchCV & Train Model\n",
    "Choose randomly samples a subset of hyperparameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3316a365-114c-4f10-9553-469d66342b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': np.arange(0.01, 1.02, 0.2),\n",
    "    'max_depth': np.arange(1, 10, 1),\n",
    "    'reg_lambda': np.arange(0.1, 15.1, 1),\n",
    "    'reg_alpha': np.arange(0.1, 15.1, 1)\n",
    "}\n",
    "\n",
    "# Number of random samples\n",
    "n_iter = 50\n",
    "\n",
    "# Generate random combinations\n",
    "param_list = list(ParameterSampler(param_distributions, n_iter=n_iter, random_state=42))\n",
    "\n",
    "# Tracking best model\n",
    "best_score = float('inf')\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Progress bar\n",
    "for params in tqdm(param_list, desc=\"Hyperparameter tuning\"):\n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror', **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred_val = model.predict(X_validate)\n",
    "    \n",
    "    # Evaluate with MSE\n",
    "    mse = mean_squared_error(y_validate, y_pred_val)\n",
    "    \n",
    "    if mse < best_score:\n",
    "        best_score = mse\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "\n",
    "# Evaluate best model on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Evaluate on external validation set\n",
    "y_pred_external = best_model.predict(X_external)\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "\n",
    "# Results\n",
    "logging.info(f\"Best parameters: {best_params}\")\n",
    "logging.info(f\"Best validation MSE: {best_score}\")\n",
    "logging.info(f\"Test Set - MSE: {mse_test}, MAE: {mae_test}\")\n",
    "logging.info(f\"External Validation Set - MSE: {mse_external}, MAE: {mae_external}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110b1d7-7f38-42df-9eed-ed0829249eee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HP Bayesian Optimization & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09340dad-8d72-4555-9b84-07c6afa84bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=50, desc=\"Bayesian Optimization Progress\")\n",
    "\n",
    "# Callback to update tqdm\n",
    "def on_step(optim_result):\n",
    "    pbar.update(1)\n",
    "\n",
    "# Define the parameter search space\n",
    "param_space = {\n",
    "    'n_estimators': (100, 300),\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'max_depth': (1, 10),\n",
    "    'reg_lambda': (0.1, 15.0),\n",
    "    'reg_alpha': (0.1, 15.0)\n",
    "}\n",
    "\n",
    "# Create the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Create BayesSearchCV for Bayesian Optimization\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    search_spaces=param_space,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    verbose=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit BayesSearchCV with tqdm callback\n",
    "bayes_search.fit(X_train, y_train, callback=on_step)\n",
    "pbar.close()\n",
    "\n",
    "# Log best parameters and score\n",
    "logging.info(\"Best parameters: %s\", bayes_search.best_params_)\n",
    "logging.info(\"Best score (negative MSE): %.4f\", bayes_search.best_score_)\n",
    "\n",
    "# Predict on the validation set with the best model\n",
    "y_pred_validate = bayes_search.predict(X_validate)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "mse_validate = mean_squared_error(y_validate, y_pred_validate)\n",
    "mae_validate = mean_absolute_error(y_validate, y_pred_validate)\n",
    "logging.info(\"Validation MSE: %.4f\", mse_validate)\n",
    "logging.info(\"Validation MAE: %.4f\", mae_validate)\n",
    "\n",
    "# Extract the best hyperparameters from BayesSearchCV\n",
    "best_params = bayes_search.best_params_\n",
    "\n",
    "# Initialize the XGBoost model with the best hyperparameters\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    reg_lambda=best_params['reg_lambda'],\n",
    "    reg_alpha=best_params['reg_alpha']\n",
    ")\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set (eICU data)\n",
    "y_pred_external = model.predict(X_external)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on the external validation set\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "\n",
    "# Log final evaluation metrics\n",
    "logging.info(\"Test Set - MSE: %.4f, MAE: %.4f\", mse_test, mae_test)\n",
    "logging.info(\"External Validation Set (eICU) - MSE: %.4f, MAE: %.4f\", mse_external, mae_external)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63d430b-79c1-4340-835e-119a468f1483",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# HP HyperOpt & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b2115-569e-4929-90c0-da230a015f50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the number of evaluations\n",
    "MAX_EVALS = 50\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=MAX_EVALS, desc=\"HyperOpt Progress\")\n",
    "\n",
    "# Define the wrapped objective function\n",
    "def objective(params):\n",
    "    model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=int(params['n_estimators']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=int(params['max_depth']),\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        reg_alpha=params['reg_alpha']\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred_validate = model.predict(X_validate)\n",
    "    \n",
    "    # Compute the MSE\n",
    "    mse = mean_squared_error(y_validate, y_pred_validate)\n",
    "\n",
    "    # Log the result\n",
    "    logging.info(\"Params: %s | Validation MSE: %.4f\", params, mse)\n",
    "    \n",
    "    # Update progress bar\n",
    "    pbar.update(1)\n",
    "\n",
    "    return {'loss': mse, 'status': 'ok'}\n",
    "\n",
    "# Define the parameter search space\n",
    "param_space = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 100, 300, 50)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1.0)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.1, 15.0),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.1, 15.0)\n",
    "}\n",
    "\n",
    "# Create a Trials object to keep track of the search\n",
    "trials = Trials()\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=param_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=MAX_EVALS,\n",
    "    trials=trials,\n",
    "    show_progressbar=False  # Disable internal bar to avoid overlap with tqdm\n",
    ")\n",
    "\n",
    "# Close progress bar\n",
    "pbar.close()\n",
    "\n",
    "# Log the best parameters\n",
    "logging.info(\"Best parameters: %s\", best)\n",
    "\n",
    "# Initialize the XGBoost model with the best hyperparameters\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=int(best['n_estimators']),\n",
    "    learning_rate=best['learning_rate'],\n",
    "    max_depth=int(best['max_depth']),\n",
    "    reg_lambda=best['reg_lambda'],\n",
    "    reg_alpha=best['reg_alpha']\n",
    ")\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set (eICU data)\n",
    "y_pred_external = model.predict(X_external)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on the external validation set\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "\n",
    "# Log final evaluation results\n",
    "logging.info(\"Test Set - MSE: %.4f, MAE: %.4f\", mse_test, mae_test)\n",
    "logging.info(\"External Validation Set (eICU) - MSE: %.4f, MAE: %.4f\", mse_external, mae_external)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493f917-45cb-478c-af02-6116cb50953d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train Model without HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec099179-7c37-4f77-9229-a2da9069d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default XGBoost Model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set (eICU data)\n",
    "y_pred_external = model.predict(X_external)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddece62-f4c2-4e5c-897b-f0fa49d5a394",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a810c9c-58b5-41f7-b700-e62558004bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "save_path = 'CSV/exports/impute/o03_Interpolation/'\n",
    "\n",
    "# Check if the directory exists, and if not, create it\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Save external validation set from eICU\n",
    "X_external.to_csv(save_path + 'X_external.csv', index=False)\n",
    "y_external.to_csv(save_path + 'y_external.csv', index=False)\n",
    "\n",
    "# Save training, validation, and test sets\n",
    "X_train.to_csv(save_path + 'X_train.csv', index=False)\n",
    "y_train.to_csv(save_path + 'y_train.csv', index=False)\n",
    "\n",
    "X_validate.to_csv(save_path + 'X_validate.csv', index=False)\n",
    "y_validate.to_csv(save_path + 'y_validate.csv', index=False)\n",
    "\n",
    "X_test.to_csv(save_path + 'X_test.csv', index=False)\n",
    "y_test.to_csv(save_path + 'y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1530e609-4239-4d84-88f6-48ed91bf5a28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e03eb8-70ee-4578-bd93-2b7032252210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and file path\n",
    "\n",
    "name = f\"{file_name}_model.json\"\n",
    "directory = 'models/'\n",
    "\n",
    "file_path = os.path.join(directory, name)\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the model as a JSON file\n",
    "model.save_model(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b303f38-a8c0-4488-8e9c-05a83768a275",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105cdba2-86f9-41a6-ba38-c11d492f9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model file path\n",
    "file_name = \"06\"  # replace with the actual name you used before saving\n",
    "directory = 'models/'\n",
    "file_path = os.path.join(directory, f\"{file_name}_model.json\")\n",
    "\n",
    "# Load the model\n",
    "model = xgb.XGBRegressor()\n",
    "model.load_model(file_path)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set\n",
    "y_pred_external = model.predict(X_external)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c859dc-94ca-4033-a6cd-b91249f23693",
   "metadata": {},
   "source": [
    "# Test Set Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bfe86-fa9d-4eae-b671-86c3c19ba10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics calculation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse:.4f}\")\n",
    "print(f\"Test Set MAE: {mae:.4f}\")\n",
    "print(f\"Test Set RMSE: {rmse:.4f}\")\n",
    "print(f\"Test Set R2: {r2:.4f}\")\n",
    "\n",
    "# Initialize error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [mse, mae, rmse]\n",
    "\n",
    "# Try to calculate MSLE\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_pred)\n",
    "    print(f\"Test Set MSLE: {msle:.4f}\")\n",
    "    \n",
    "    # Add MSLE to the list of metrics if applicable\n",
    "    error_metrics.append('MSLE')\n",
    "    values.append(msle)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")\n",
    "\n",
    "# Plot error metrics (with or without MSLE)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics')\n",
    "plt.savefig(f\"plots/03_Error_Metric_Plots/{file_name}_internal_error_metrics.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the test set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if r2 >= 0:\n",
    "    plt.pie([r2, 100 - r2], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Test Set Explained Variance by R-squared (R2)')\n",
    "plt.savefig(f\"plots/03_Error_Metric_Plots/{file_name}_internal_R2.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353110ac-0992-4807-9c0f-a88ec34b2c5a",
   "metadata": {},
   "source": [
    "# External Validation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d35f0-0758-45ed-a510-b70a5e0af7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the external validation set (eICU data)\n",
    "y_pred_external = model.predict(X_external)\n",
    "\n",
    "# Metrics for external validation set\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "rmse_external = np.sqrt(mse_external)\n",
    "r2_external = r2_score(y_external, y_pred_external) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external:.4f}\")\n",
    "print(f\"External Validation Set MAE: {mae_external:.4f}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external:.4f}\")\n",
    "print(f\"External Validation Set R2: {r2_external:.4f}\")\n",
    "\n",
    "# Initialize error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [mse_external, mae_external, rmse_external]\n",
    "\n",
    "# Try to calculate MSLE\n",
    "try:\n",
    "    msle_external = mean_squared_log_error(y_external, y_pred_external)\n",
    "    print(f\"External Validation Set MSLE: {msle_external:.4f}\")\n",
    "    \n",
    "    # Add MSLE to the list of metrics if applicable\n",
    "    error_metrics.append('MSLE')\n",
    "    values.append(msle_external)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")\n",
    "\n",
    "# Plot error metrics (with or without MSLE)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics')\n",
    "plt.savefig(f\"plots/03_Error_Metric_Plots/{file_name}_external_error_metrics.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the external validation set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if r2_external >= 0:\n",
    "    plt.pie([r2_external, 100 - r2_external], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Validation Set Explained Variance by R-squared (R2)')\n",
    "plt.savefig(f\"plots/03_Error_Metric_Plots/{file_name}_external_R2.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba566d-6b67-480f-a4bb-e394d8c01fce",
   "metadata": {},
   "source": [
    "# Calibration Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeba03a-8721-4a77-a1d2-557ac782c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to save plots if not exist\n",
    "os.makedirs(\"plots/04_Calibration_Plots\", exist_ok=True)\n",
    "\n",
    "# Function to create calibration plot\n",
    "def plot_calibration(y_true, y_pred, title, filename):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.regplot(x=y_true, y=y_pred, lowess=True, line_kws={'color': 'red'}, scatter_kws={'alpha': 0.4})\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'k--', lw=2)\n",
    "    plt.xlabel('Actual LOS')\n",
    "    plt.ylabel('Predicted LOS')\n",
    "    plt.title(f'Calibration Plot: {title}')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"plots/04_Calibration_Plots/{filename}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot for test set\n",
    "plot_calibration(y_test, y_pred, \"Internal Test Set\", f\"{file_name}_calibration_internal\")\n",
    "\n",
    "# Plot for external set\n",
    "plot_calibration(y_external, y_pred_external, \"External Validation Set\", f\"{file_name}_calibration_external\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5ee89-6e54-49fa-8e9b-40c5629f7ee6",
   "metadata": {},
   "source": [
    "# Most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f15d96-8eab-4129-9b21-4932c1701469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "most_important_df = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances along with their corresponding names\n",
    "most_important_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': most_important_df})\n",
    "\n",
    "# Sort the DataFrame by feature importance in descending order\n",
    "most_important_df = most_important_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Scale the importance\n",
    "most_important_df['Importance'] *= 100000\n",
    "\n",
    "# Print the top N most important features\n",
    "top_n = 20  # set features number\n",
    "print(f\"Top {top_n} most important features:\")\n",
    "print(most_important_df.head(top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf81e5-689e-4426-80ac-52c1c4ce470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_features = most_important_df.head(20).copy()  # Create a copy explicitly\n",
    "top_20_features['Feature'] = top_20_features['Feature'].apply(lambda x: '\\n'.join(textwrap.wrap(x, width=20)))\n",
    "\n",
    "# Set seaborn style and remove gridlines\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Top 20 most important features (create a copy)\n",
    "top_20_features = most_important_df.head(20).copy()\n",
    "\n",
    "# Wrap long feature names\n",
    "top_20_features['Feature'] = top_20_features['Feature'].apply(lambda x: '\\n'.join(textwrap.wrap(x, width=20)))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(18, 11))  # Increase figure size for better visibility\n",
    "plot = sns.barplot(x='Importance', y='Feature', data=top_20_features, hue='Feature', palette=\"Blues\", legend=False)\n",
    "\n",
    "# Set font size for labels and title\n",
    "plt.xlabel('Importance', fontsize=16)\n",
    "plt.ylabel('Feature', fontsize=16)\n",
    "plt.title('Top 20 Features with Highest Importance', fontsize=20)\n",
    "\n",
    "# Save the plot in high resolution\n",
    "plt.savefig(f\"plots/01_Most_Important_SHAP/{file_name}_most_important_features.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f11674-d136-4426-926e-b759f5035a26",
   "metadata": {},
   "source": [
    "# Shap plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e76068-a999-43e9-a02f-7b565c2b6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the model's predictions using SHAP values\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Mean absolute SHAP values to rank feature importance\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Mean Absolute SHAP Value': mean_abs_shap\n",
    "}).sort_values(by='Mean Absolute SHAP Value', ascending=False)\n",
    "\n",
    "# Display the top N most important features\n",
    "top_n = 20\n",
    "print(f\"Top {top_n} features by SHAP importance:\")\n",
    "print(feature_importance_df.head(top_n))\n",
    "\n",
    "# Create a Matplotlib figure\n",
    "plt.figure()\n",
    "\n",
    "# SHAP Summary Plot with Bee Swarm (distribution of feature impacts)\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"dot\", show=False)\n",
    "\n",
    "# Add grid to the plot\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "plt.savefig(f\"plots/01_Most_Important_SHAP/{file_name}_shap_plot.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a20b9d-5a4b-45e5-b4e3-a828f0bbee44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Create Shap by saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d72135-8af6-497a-8eed-594ef923aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the XGBoost model from the JSON file\n",
    "model = xgb.Booster()  # Initialize the Booster object\n",
    "model.load_model(\"models\\\\02_Mean_Impute\\\\o11_Mean_Scale_Norm_Bayes_.json\")\n",
    "\n",
    "# I must load training - test set\n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Generate SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"dot\", show=False)\n",
    "\n",
    "# Add grid to the plot\n",
    "plt.grid(True)\n",
    "\n",
    "# If i want to save the plot I must change in line 14 the show=True to False\n",
    "# plt.savefig(\"shap_summary_plot.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616729d-6efe-452c-b36b-28b1d20bc257",
   "metadata": {},
   "source": [
    "# Plotting True vs. Predicted LOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d0bd3b-6125-44e6-a46d-a0f6c2acd1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', label='Prediction')\n",
    "\n",
    "# Line for Perfect Prediction\n",
    "perfect_line = np.linspace(y_test.min(), y_test.max(), 100)\n",
    "plt.plot(perfect_line, perfect_line, color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Labels, legend, and grid\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Predicted LOS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Predicted vs. True LOS (Test Set)')\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "plt.savefig(f\"plots/02_Prediction_Plot/02_true_vs_pred/{file_name}_true_vs_pred_test_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# External Validation Set Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_external, y_pred_external, color='blue', label='Prediction')\n",
    "\n",
    "# Line for Perfect Prediction (y = x)\n",
    "perfect_line_ext = np.linspace(y_external.min(), y_external.max(), 100)\n",
    "plt.plot(perfect_line_ext, perfect_line_ext, color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Labels, legend, and grid\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Predicted LOS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Predicted vs. True LOS (External Validation Set)')\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "plt.savefig(f\"plots/02_Prediction_Plot/02_true_vs_pred/{file_name}_true_vs_pred_external_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f9ec6-43a2-41b0-9a37-317c4db7bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_test to a 1D numpy array\n",
    "y_test = y_test.values.flatten()\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, residuals, color='blue', alpha=0.5, label=\"Residuals\")\n",
    "plt.axhline(y=0, color='red', linestyle='--', label=\"Zero Line\")\n",
    "plt.axhline(y=mae, color='green', linestyle='--', label=f\"MAE = {mae:.2f}\")\n",
    "plt.axhline(y=-mae, color='green', linestyle='--')\n",
    "plt.xlabel('True LOS')\n",
    "plt.ylabel('Residuals (True - Predicted)')\n",
    "plt.title('Residuals Plot with MAE Bounds')\n",
    "plt.grid(True)\n",
    "\n",
    "# Place the legend outside of the plot\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Save the plot as a PNG image\n",
    "plt.savefig(f\"plots/02_Prediction_Plot/01_residuals/{file_name}_residuals_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45ed80-6c84-47ba-b7ed-34631763e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"y_pred shape:\", y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade636b8-35d8-4726-8587-7776029df178",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba9b93-f41d-40cb-9e02-d7caaf2e647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_test to a 1D numpy array\n",
    "y_test = y_test.values.flatten()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
