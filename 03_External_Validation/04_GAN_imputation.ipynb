{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27150fe1-2273-4084-bfdb-af667c2c54c2",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba9ff51-183c-48dd-ab85-986b02ba8647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterGrid, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c94c6d-1c45-4a23-8d0c-ced1519d7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MIMICs CSV file\n",
    "mimic_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\mimic_mean_median_min_max_final.csv\")\n",
    "\n",
    "# Read eICUs CSV file\n",
    "eicu_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\eicu_mean_median_min_max_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e73ecd05-dc5d-4103-8959-4948a825288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 10\n",
    "\n",
    "# Filter icu stay less than 10 days\n",
    "mimic_df = mimic_df[mimic_df['los'] < day]\n",
    "\n",
    "# Filter icu stay less than 10 days\n",
    "eicu_df = eicu_df[eicu_df['los'] < day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64f2142c-43bf-44fe-9838-b7be5302785d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 51040\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I'm gonna concat and split the mimic and icu\n",
    "at this point. I must create the same columns\n",
    "from the tranformation of categorical data.\n",
    "\"\"\"\n",
    "row_count = mimic_df.shape[0]\n",
    "print(f\"Row count: {row_count}\")\n",
    "\n",
    "# Concat dataframes\n",
    "df_combined = pd.concat([mimic_df, eicu_df], ignore_index=True)\n",
    "\n",
    "# Find all categorical columns in mimic\n",
    "categorical_columns = df_combined.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Apply one-hot encoding to all categorical columns\n",
    "df_encoded = pd.get_dummies(df_combined, columns=categorical_columns)\n",
    "\n",
    "# Split the concatenate dataframe\n",
    "mimic_df = df_encoded.iloc[:row_count, :]\n",
    "eicu_df = df_encoded.iloc[row_count:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01eb144-0185-4e6c-94f4-43104907fd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 40832\n",
      "Validation set size: 5104\n",
      "Test set size: 5104\n"
     ]
    }
   ],
   "source": [
    "total_test_val_perc = 0.2\n",
    "split_between_test_val_perc = 0.5\n",
    "\n",
    "# Group data by subject_id and hadm_id\n",
    "grouped_df = mimic_df.groupby(['subject_id', 'hadm_id'])\n",
    "\n",
    "# Get a new dataframe with one row per patient (subject_id, hadm_id) pair\n",
    "patient_df = grouped_df['hospital_expire_flag'].first().reset_index()\n",
    "\n",
    "# Split the patient_df into training (80%), validation (10%), and test (10%) while keeping the ratio of hospital_expired_flag\n",
    "train, temp = train_test_split(patient_df, test_size=total_test_val_perc, stratify=patient_df['hospital_expire_flag'], random_state=42)\n",
    "val, test = train_test_split(temp, test_size=split_between_test_val_perc, stratify=temp['hospital_expire_flag'], random_state=42)\n",
    "\n",
    "# Step 4: Merge back with the original df to get the rows for each patient in the splits\n",
    "train_df = mimic_df.merge(train[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "val_df = mimic_df.merge(val[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "test_df = mimic_df.merge(test[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "\n",
    "# Check the sizes of the splits\n",
    "print(f'Training set size: {train_df.shape[0]}')\n",
    "print(f'Validation set size: {val_df.shape[0]}')\n",
    "print(f'Test set size: {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4db47f7-426f-47b6-aada-48b382ceed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External validation from eICU\n",
    "X_external = eicu_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_external = eicu_df['los']\n",
    "\n",
    "# Separate features and target for the training, validation, and test sets\n",
    "X_train = train_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_train = train_df['los']\n",
    "\n",
    "X_validate = val_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_validate = val_df['los']\n",
    "\n",
    "X_test = test_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_test = test_df['los']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a0783-9c31-47d9-8cb4-b093c68c1302",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load Train - Validation - Test & External Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53833f-0875-467d-a556-4e3508c362fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subfolder\n",
    "subfolder = \"o2_interpolation_impute\"\n",
    "\n",
    "# Load CSV files into corresponding variables\n",
    "X_external = pd.read_csv(f\"CSV/exports/impute/{subfolder}/X_external.csv\")\n",
    "y_external = pd.read_csv(f\"CSV/exports/impute/{subfolder}/y_external.csv\")\n",
    "X_train = pd.read_csv(f\"CSV/exports/impute/{subfolder}/X_train.csv\")\n",
    "y_train = pd.read_csv(f\"CSV/exports/impute/{subfolder}/y_train.csv\")\n",
    "X_validate = pd.read_csv(f\"CSV/exports/impute/{subfolder}/X_validate.csv\")\n",
    "y_validate = pd.read_csv(f\"CSV/exports/impute/{subfolder}/y_validate.csv\")\n",
    "X_test = pd.read_csv(f\"CSV/exports/impute/{subfolder}/X_test.csv\")\n",
    "y_test = pd.read_csv(f\"CSV/exports/impute/{subfolder}/y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1ea72-18c5-4f51-a463-15f83bb26771",
   "metadata": {},
   "source": [
    "# o01 Simple GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49001668-dc2b-4c05-8e19-9e2bcbfbbf8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Custom Dataset class for PyTorch\n",
    "def prepare_dataset(X):\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, data):\n",
    "            # Ensure data is converted to numeric type to avoid object type errors\n",
    "            self.data = torch.tensor(data.astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx]\n",
    "    \n",
    "    return CustomDataset(X)\n",
    "\n",
    "# Define Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# Define Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# GAN Imputation Function with Early Stopping\n",
    "def impute_missing_values_with_gan(X, epochs=1000, batch_size=64, learning_rate=0.0002, patience=10):\n",
    "    # Prepare data and mask for missing values\n",
    "    X_missing = X.copy()\n",
    "    mask = X_missing.isna()\n",
    "    X_missing.fillna(0, inplace=True)\n",
    "    \n",
    "    # Prepare dataset and dataloader\n",
    "    dataset = prepare_dataset(X_missing.values)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    # Initialize Generator and Discriminator\n",
    "    generator = Generator(input_dim)\n",
    "    discriminator = Discriminator(input_dim)\n",
    "\n",
    "    # Device placement\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Loss function\n",
    "    adversarial_loss = nn.BCELoss().to(device)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, real_data in enumerate(dataloader):\n",
    "            # Move data to the correct device\n",
    "            real_data = real_data.to(device)\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = torch.ones(real_data.size(0), 1).to(device)\n",
    "            fake = torch.zeros(real_data.size(0), 1).to(device)\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate data and replace missing values with generated data\n",
    "            gen_data = generator(real_data)\n",
    "            batch_mask = mask.iloc[i * batch_size:(i + 1) * batch_size].values\n",
    "            gen_data[batch_mask] = real_data[batch_mask]\n",
    "\n",
    "            # Generator loss\n",
    "            g_loss = adversarial_loss(discriminator(gen_data), valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Real and fake losses\n",
    "            real_loss = adversarial_loss(discriminator(real_data), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_data.detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # Logging progress\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        logging.info(f\"Epoch {epoch + 1}/{epochs} - Generator Loss: {g_loss.item()}, Discriminator Loss: {d_loss.item()}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if g_loss.item() < best_loss:\n",
    "            best_loss = g_loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            logging.info(f\"Early stopping at epoch {epoch + 1} - Generator Loss: {g_loss.item()}\")\n",
    "            break\n",
    "\n",
    "    # Impute missing values using the trained generator\n",
    "    X_imputed = X_missing.copy()\n",
    "    X_imputed = torch.tensor(X_imputed.values.astype(np.float32), dtype=torch.float32).to(device)\n",
    "    X_imputed = generator(X_imputed).detach().cpu().numpy()  # Move back to CPU for numpy compatibility\n",
    "    X_imputed[mask.values] = X_missing.values[mask.values]\n",
    "\n",
    "    return pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# Save models (optional)\n",
    "# torch.save(generator.state_dict(), \"generator.pth\")\n",
    "# torch.save(discriminator.state_dict(), \"discriminator.pth\")\n",
    "\n",
    "# Impute missing values for each dataset separately\n",
    "X_external_imputed = impute_missing_values_with_gan(X_external)\n",
    "X_train_imputed = impute_missing_values_with_gan(X_train)\n",
    "X_validate_imputed = impute_missing_values_with_gan(X_validate)\n",
    "X_test_imputed = impute_missing_values_with_gan(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db63bc38-3870-4a3e-bc8e-95c90d03569b",
   "metadata": {},
   "source": [
    "# o02 Run GAN multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9044bafa-3401-42fd-b9f7-5be534ee19a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Imputation Pass 1/3 ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1/1000 - Generator Loss: 1.0082497596740723, Discriminator Loss: 0.5422036647796631\n",
      "INFO:root:Epoch 2/1000 - Generator Loss: 1.0394421815872192, Discriminator Loss: 0.6764670014381409\n",
      "INFO:root:Epoch 3/1000 - Generator Loss: 2.589684247970581, Discriminator Loss: 0.4176105558872223\n",
      "INFO:root:Epoch 4/1000 - Generator Loss: 1.7230149507522583, Discriminator Loss: 0.39542990922927856\n",
      "INFO:root:Epoch 5/1000 - Generator Loss: 2.265847682952881, Discriminator Loss: 0.5392922163009644\n",
      "INFO:root:Epoch 6/1000 - Generator Loss: 2.0950677394866943, Discriminator Loss: 0.5000324845314026\n",
      "INFO:root:Epoch 7/1000 - Generator Loss: 1.3824129104614258, Discriminator Loss: 0.450406938791275\n",
      "INFO:root:Epoch 8/1000 - Generator Loss: 0.7340129613876343, Discriminator Loss: 0.8645924925804138\n",
      "INFO:root:Epoch 9/1000 - Generator Loss: 1.25557541847229, Discriminator Loss: 0.47079285979270935\n",
      "INFO:root:Epoch 10/1000 - Generator Loss: 2.1566567420959473, Discriminator Loss: 0.3441897928714752\n",
      "INFO:root:Epoch 11/1000 - Generator Loss: 2.0789411067962646, Discriminator Loss: 0.5789149403572083\n",
      "INFO:root:Epoch 12/1000 - Generator Loss: 0.9646023511886597, Discriminator Loss: 0.5202510356903076\n",
      "INFO:root:Epoch 13/1000 - Generator Loss: 1.5166096687316895, Discriminator Loss: 0.4051719903945923\n",
      "INFO:root:Epoch 14/1000 - Generator Loss: 1.2203748226165771, Discriminator Loss: 0.5885395407676697\n",
      "INFO:root:Epoch 15/1000 - Generator Loss: 1.6781470775604248, Discriminator Loss: 0.3789719045162201\n",
      "INFO:root:Epoch 16/1000 - Generator Loss: 1.8159362077713013, Discriminator Loss: 0.36083921790122986\n",
      "INFO:root:Epoch 17/1000 - Generator Loss: 1.1933339834213257, Discriminator Loss: 0.4197888970375061\n",
      "INFO:root:Epoch 18/1000 - Generator Loss: 1.1662495136260986, Discriminator Loss: 0.5557641983032227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18 - Generator Loss: 1.1662495136260986\n",
      "\n",
      "--- Imputation Pass 2/3 ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1/1000 - Generator Loss: 2.394395589828491, Discriminator Loss: 0.44875815510749817\n",
      "INFO:root:Epoch 2/1000 - Generator Loss: 1.752920150756836, Discriminator Loss: 0.430203378200531\n",
      "INFO:root:Epoch 3/1000 - Generator Loss: 1.6148054599761963, Discriminator Loss: 0.4096754789352417\n",
      "INFO:root:Epoch 4/1000 - Generator Loss: 1.868181824684143, Discriminator Loss: 0.22493763267993927\n",
      "INFO:root:Epoch 5/1000 - Generator Loss: 1.9125481843948364, Discriminator Loss: 0.3880309462547302\n",
      "INFO:root:Epoch 6/1000 - Generator Loss: 1.5893750190734863, Discriminator Loss: 0.3431832790374756\n",
      "INFO:root:Epoch 7/1000 - Generator Loss: 2.5374791622161865, Discriminator Loss: 0.2048741579055786\n",
      "INFO:root:Epoch 8/1000 - Generator Loss: 1.961550235748291, Discriminator Loss: 0.2524179220199585\n",
      "INFO:root:Epoch 9/1000 - Generator Loss: 0.9677797555923462, Discriminator Loss: 0.5013019442558289\n",
      "INFO:root:Epoch 10/1000 - Generator Loss: 0.8962404131889343, Discriminator Loss: 0.5983597040176392\n",
      "INFO:root:Epoch 11/1000 - Generator Loss: 0.7006934881210327, Discriminator Loss: 1.0445986986160278\n",
      "INFO:root:Epoch 12/1000 - Generator Loss: 1.5131888389587402, Discriminator Loss: 0.6445538997650146\n",
      "INFO:root:Epoch 13/1000 - Generator Loss: 1.3893340826034546, Discriminator Loss: 0.39015471935272217\n",
      "INFO:root:Epoch 14/1000 - Generator Loss: 0.8545407652854919, Discriminator Loss: 0.6227015852928162\n",
      "INFO:root:Epoch 15/1000 - Generator Loss: 1.5754311084747314, Discriminator Loss: 0.3684942126274109\n",
      "INFO:root:Epoch 16/1000 - Generator Loss: 2.7824485301971436, Discriminator Loss: 0.2908570170402527\n",
      "INFO:root:Epoch 17/1000 - Generator Loss: 1.5861155986785889, Discriminator Loss: 0.2708261013031006\n",
      "INFO:root:Epoch 18/1000 - Generator Loss: 0.7138242721557617, Discriminator Loss: 0.8568638563156128\n",
      "INFO:root:Epoch 19/1000 - Generator Loss: 1.9994640350341797, Discriminator Loss: 0.47617918252944946\n",
      "INFO:root:Epoch 20/1000 - Generator Loss: 1.4287792444229126, Discriminator Loss: 0.4226667881011963\n",
      "INFO:root:Epoch 21/1000 - Generator Loss: 1.6633814573287964, Discriminator Loss: 0.45171964168548584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21 - Generator Loss: 1.6633814573287964\n",
      "\n",
      "--- Imputation Pass 3/3 ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1/1000 - Generator Loss: 1.829801082611084, Discriminator Loss: 0.4282008409500122\n",
      "INFO:root:Epoch 2/1000 - Generator Loss: 1.6474900245666504, Discriminator Loss: 0.5971099138259888\n",
      "INFO:root:Epoch 3/1000 - Generator Loss: 1.4369300603866577, Discriminator Loss: 0.32113298773765564\n",
      "INFO:root:Epoch 4/1000 - Generator Loss: 1.4276542663574219, Discriminator Loss: 0.5142741203308105\n",
      "INFO:root:Epoch 5/1000 - Generator Loss: 1.2652450799942017, Discriminator Loss: 0.6639271974563599\n",
      "INFO:root:Epoch 6/1000 - Generator Loss: 1.6130473613739014, Discriminator Loss: 0.34582555294036865\n",
      "INFO:root:Epoch 7/1000 - Generator Loss: 1.256985068321228, Discriminator Loss: 0.43211841583251953\n",
      "INFO:root:Epoch 8/1000 - Generator Loss: 1.5107439756393433, Discriminator Loss: 0.42448848485946655\n",
      "INFO:root:Epoch 9/1000 - Generator Loss: 1.7163219451904297, Discriminator Loss: 0.40907660126686096\n",
      "INFO:root:Epoch 10/1000 - Generator Loss: 1.5173746347427368, Discriminator Loss: 0.3813726007938385\n",
      "INFO:root:Epoch 11/1000 - Generator Loss: 0.6698030233383179, Discriminator Loss: 0.9398033618927002\n",
      "INFO:root:Epoch 12/1000 - Generator Loss: 1.142922282218933, Discriminator Loss: 0.6561764478683472\n",
      "INFO:root:Epoch 13/1000 - Generator Loss: 1.2325187921524048, Discriminator Loss: 0.574791669845581\n",
      "INFO:root:Epoch 14/1000 - Generator Loss: 2.0249433517456055, Discriminator Loss: 0.45276907086372375\n",
      "INFO:root:Epoch 15/1000 - Generator Loss: 1.414688229560852, Discriminator Loss: 0.49764662981033325\n",
      "INFO:root:Epoch 16/1000 - Generator Loss: 1.1836233139038086, Discriminator Loss: 0.5408703684806824\n",
      "INFO:root:Epoch 17/1000 - Generator Loss: 1.7611300945281982, Discriminator Loss: 0.4548623561859131\n",
      "INFO:root:Epoch 18/1000 - Generator Loss: 2.0885374546051025, Discriminator Loss: 0.43467792868614197\n",
      "INFO:root:Epoch 19/1000 - Generator Loss: 1.3221988677978516, Discriminator Loss: 0.42601579427719116\n",
      "INFO:root:Epoch 20/1000 - Generator Loss: 1.1205813884735107, Discriminator Loss: 0.4869871735572815\n",
      "INFO:root:Epoch 21/1000 - Generator Loss: 1.4881471395492554, Discriminator Loss: 0.3238973617553711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21 - Generator Loss: 1.4881471395492554\n",
      "\n",
      "--- Imputation Pass 1/3 ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1/1000 - Generator Loss: 2.4057345390319824, Discriminator Loss: 0.3147991895675659\n",
      "INFO:root:Epoch 2/1000 - Generator Loss: 0.8814415335655212, Discriminator Loss: 0.4204072952270508\n",
      "INFO:root:Epoch 3/1000 - Generator Loss: 0.9709899425506592, Discriminator Loss: 0.5099101662635803\n",
      "INFO:root:Epoch 4/1000 - Generator Loss: 1.5976848602294922, Discriminator Loss: 0.45520010590553284\n",
      "INFO:root:Epoch 5/1000 - Generator Loss: 1.5165610313415527, Discriminator Loss: 0.4048083424568176\n",
      "INFO:root:Epoch 6/1000 - Generator Loss: 2.6727261543273926, Discriminator Loss: 1.4006140232086182\n",
      "INFO:root:Epoch 7/1000 - Generator Loss: 2.068336009979248, Discriminator Loss: 0.2970495820045471\n",
      "INFO:root:Epoch 8/1000 - Generator Loss: 0.9433561563491821, Discriminator Loss: 1.268726110458374\n",
      "INFO:root:Epoch 9/1000 - Generator Loss: 3.3379454612731934, Discriminator Loss: 0.4845249056816101\n",
      "INFO:root:Epoch 10/1000 - Generator Loss: 2.6959967613220215, Discriminator Loss: 0.26887989044189453\n",
      "INFO:root:Epoch 11/1000 - Generator Loss: 2.7253034114837646, Discriminator Loss: 1.0957201719284058\n",
      "INFO:root:Epoch 12/1000 - Generator Loss: 2.4376678466796875, Discriminator Loss: 0.2295604944229126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 12 - Generator Loss: 2.4376678466796875\n",
      "\n",
      "--- Imputation Pass 2/3 ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1/1000 - Generator Loss: 2.684602737426758, Discriminator Loss: 0.2201017141342163\n",
      "INFO:root:Epoch 2/1000 - Generator Loss: 1.6774810552597046, Discriminator Loss: 0.5878006815910339\n",
      "INFO:root:Epoch 3/1000 - Generator Loss: 0.5851223468780518, Discriminator Loss: 0.5692627429962158\n",
      "INFO:root:Epoch 4/1000 - Generator Loss: 1.6462422609329224, Discriminator Loss: 0.33479762077331543\n",
      "INFO:root:Epoch 5/1000 - Generator Loss: 1.6301050186157227, Discriminator Loss: 0.30858755111694336\n",
      "INFO:root:Epoch 6/1000 - Generator Loss: 1.4128875732421875, Discriminator Loss: 0.5154665112495422\n",
      "INFO:root:Epoch 7/1000 - Generator Loss: 2.9197819232940674, Discriminator Loss: 0.28425318002700806\n",
      "INFO:root:Epoch 8/1000 - Generator Loss: 2.1828725337982178, Discriminator Loss: 0.3826148509979248\n",
      "INFO:root:Epoch 9/1000 - Generator Loss: 1.7192368507385254, Discriminator Loss: 0.2480093240737915\n",
      "INFO:root:Epoch 10/1000 - Generator Loss: 1.8703583478927612, Discriminator Loss: 0.2555408775806427\n",
      "INFO:root:Epoch 11/1000 - Generator Loss: 2.72147274017334, Discriminator Loss: 0.1876382827758789\n",
      "INFO:root:Epoch 12/1000 - Generator Loss: 1.733083724975586, Discriminator Loss: 0.22399482131004333\n",
      "INFO:root:Epoch 13/1000 - Generator Loss: 3.5306384563446045, Discriminator Loss: 0.1654047667980194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 13 - Generator Loss: 3.5306384563446045\n",
      "\n",
      "--- Imputation Pass 3/3 ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1/1000 - Generator Loss: 3.6860761642456055, Discriminator Loss: 0.11381968855857849\n",
      "INFO:root:Epoch 2/1000 - Generator Loss: 3.303877115249634, Discriminator Loss: 0.5801141262054443\n",
      "INFO:root:Epoch 3/1000 - Generator Loss: 0.8494479656219482, Discriminator Loss: 1.0243263244628906\n",
      "INFO:root:Epoch 4/1000 - Generator Loss: 1.9596960544586182, Discriminator Loss: 0.3046152591705322\n",
      "INFO:root:Epoch 5/1000 - Generator Loss: 1.6356396675109863, Discriminator Loss: 0.3846980333328247\n",
      "INFO:root:Epoch 6/1000 - Generator Loss: 3.7746381759643555, Discriminator Loss: 0.267703652381897\n",
      "INFO:root:Epoch 7/1000 - Generator Loss: 2.0661325454711914, Discriminator Loss: 0.15901914238929749\n",
      "INFO:root:Epoch 8/1000 - Generator Loss: 2.1351068019866943, Discriminator Loss: 0.2607773542404175\n",
      "INFO:root:Epoch 9/1000 - Generator Loss: 1.202926516532898, Discriminator Loss: 0.5531449317932129\n",
      "INFO:root:Epoch 10/1000 - Generator Loss: 2.2369563579559326, Discriminator Loss: 0.2535463571548462\n",
      "INFO:root:Epoch 11/1000 - Generator Loss: 2.9270148277282715, Discriminator Loss: 0.3134733736515045\n",
      "INFO:root:Epoch 12/1000 - Generator Loss: 3.6985278129577637, Discriminator Loss: 0.23549816012382507\n",
      "INFO:root:Epoch 13/1000 - Generator Loss: 1.7050392627716064, Discriminator Loss: 0.5931313037872314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 13 - Generator Loss: 1.7050392627716064\n",
      "\n",
      "--- Imputation Pass 1/3 ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1/1000 - Generator Loss: 1.297393798828125, Discriminator Loss: 0.44704121351242065\n",
      "INFO:root:Epoch 2/1000 - Generator Loss: 2.6656785011291504, Discriminator Loss: 0.09690233319997787\n",
      "INFO:root:Epoch 3/1000 - Generator Loss: 3.368907928466797, Discriminator Loss: 0.3314264118671417\n",
      "INFO:root:Epoch 4/1000 - Generator Loss: 2.142860174179077, Discriminator Loss: 0.42715418338775635\n",
      "INFO:root:Epoch 5/1000 - Generator Loss: 1.9437092542648315, Discriminator Loss: 0.29888617992401123\n",
      "INFO:root:Epoch 6/1000 - Generator Loss: 3.652674913406372, Discriminator Loss: 0.504962682723999\n",
      "INFO:root:Epoch 7/1000 - Generator Loss: 6.129988193511963, Discriminator Loss: 2.514206886291504\n",
      "INFO:root:Epoch 8/1000 - Generator Loss: 1.6948474645614624, Discriminator Loss: 0.710557758808136\n",
      "INFO:root:Epoch 9/1000 - Generator Loss: 4.771276950836182, Discriminator Loss: 0.4144764244556427\n",
      "INFO:root:Epoch 10/1000 - Generator Loss: 1.7355247735977173, Discriminator Loss: 0.33168894052505493\n",
      "INFO:root:Epoch 11/1000 - Generator Loss: 0.9946542382240295, Discriminator Loss: 0.5867871642112732\n",
      "INFO:root:Epoch 12/1000 - Generator Loss: 0.48649170994758606, Discriminator Loss: 1.3464608192443848\n",
      "INFO:root:Epoch 13/1000 - Generator Loss: 2.0368802547454834, Discriminator Loss: 1.6403928995132446\n",
      "INFO:root:Epoch 14/1000 - Generator Loss: 5.643978118896484, Discriminator Loss: 2.791905641555786\n",
      "INFO:root:Epoch 15/1000 - Generator Loss: 5.547069549560547, Discriminator Loss: 1.6266283988952637\n",
      "INFO:root:Epoch 16/1000 - Generator Loss: 2.8240115642547607, Discriminator Loss: 2.1800618171691895\n",
      "INFO:root:Epoch 17/1000 - Generator Loss: 2.139289140701294, Discriminator Loss: 1.2192461490631104\n",
      "INFO:root:Epoch 18/1000 - Generator Loss: 3.2412281036376953, Discriminator Loss: 0.20334972441196442\n",
      "INFO:root:Epoch 19/1000 - Generator Loss: 1.3415428400039673, Discriminator Loss: 0.6249374747276306\n",
      "INFO:root:Epoch 20/1000 - Generator Loss: 2.662642478942871, Discriminator Loss: 2.219119071960449\n",
      "INFO:root:Epoch 21/1000 - Generator Loss: 2.838639974594116, Discriminator Loss: 0.7450066804885864\n",
      "INFO:root:Epoch 22/1000 - Generator Loss: 2.573007345199585, Discriminator Loss: 1.8385646343231201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 22 - Generator Loss: 2.573007345199585\n",
      "\n",
      "--- Imputation Pass 2/3 ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1/1000 - Generator Loss: 0.9816348552703857, Discriminator Loss: 0.4399782121181488\n",
      "INFO:root:Epoch 2/1000 - Generator Loss: 1.7063993215560913, Discriminator Loss: 0.3047957122325897\n",
      "INFO:root:Epoch 3/1000 - Generator Loss: 3.127580404281616, Discriminator Loss: 0.3877939283847809\n",
      "INFO:root:Epoch 4/1000 - Generator Loss: 1.448107123374939, Discriminator Loss: 0.890807032585144\n",
      "INFO:root:Epoch 5/1000 - Generator Loss: 1.1145659685134888, Discriminator Loss: 0.38379257917404175\n",
      "INFO:root:Epoch 6/1000 - Generator Loss: 1.351382851600647, Discriminator Loss: 0.5762107372283936\n",
      "INFO:root:Epoch 7/1000 - Generator Loss: 1.2451422214508057, Discriminator Loss: 0.835567831993103\n",
      "INFO:root:Epoch 8/1000 - Generator Loss: 1.8186486959457397, Discriminator Loss: 0.6141198873519897\n",
      "INFO:root:Epoch 9/1000 - Generator Loss: 0.7294478416442871, Discriminator Loss: 0.7219887971878052\n",
      "INFO:root:Epoch 10/1000 - Generator Loss: 1.9627013206481934, Discriminator Loss: 0.6058574914932251\n",
      "INFO:root:Epoch 11/1000 - Generator Loss: 3.4432437419891357, Discriminator Loss: 2.8982715606689453\n",
      "INFO:root:Epoch 12/1000 - Generator Loss: 1.4977298974990845, Discriminator Loss: 0.9096109867095947\n",
      "INFO:root:Epoch 13/1000 - Generator Loss: 1.2679671049118042, Discriminator Loss: 0.6450842022895813\n",
      "INFO:root:Epoch 14/1000 - Generator Loss: 1.3738312721252441, Discriminator Loss: 0.9743668437004089\n",
      "INFO:root:Epoch 15/1000 - Generator Loss: 1.3839073181152344, Discriminator Loss: 1.2221601009368896\n",
      "INFO:root:Epoch 16/1000 - Generator Loss: 3.2514688968658447, Discriminator Loss: 2.4489073753356934\n",
      "INFO:root:Epoch 17/1000 - Generator Loss: 3.0557806491851807, Discriminator Loss: 0.33413779735565186\n",
      "INFO:root:Epoch 18/1000 - Generator Loss: 1.370557427406311, Discriminator Loss: 0.41702860593795776\n",
      "INFO:root:Epoch 19/1000 - Generator Loss: 1.9590567350387573, Discriminator Loss: 0.6055342555046082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 19 - Generator Loss: 1.9590567350387573\n",
      "\n",
      "--- Imputation Pass 3/3 ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1/1000 - Generator Loss: 3.8123714923858643, Discriminator Loss: 0.3910503685474396\n",
      "INFO:root:Epoch 2/1000 - Generator Loss: 2.2676925659179688, Discriminator Loss: 1.5035704374313354\n",
      "INFO:root:Epoch 3/1000 - Generator Loss: 0.5126456022262573, Discriminator Loss: 0.9762917160987854\n",
      "INFO:root:Epoch 4/1000 - Generator Loss: 1.528992772102356, Discriminator Loss: 0.4011877775192261\n",
      "INFO:root:Epoch 5/1000 - Generator Loss: 1.5248061418533325, Discriminator Loss: 0.3986969590187073\n",
      "INFO:root:Epoch 6/1000 - Generator Loss: 2.1725447177886963, Discriminator Loss: 0.6821770668029785\n",
      "INFO:root:Epoch 7/1000 - Generator Loss: 2.441396951675415, Discriminator Loss: 0.6338729858398438\n",
      "INFO:root:Epoch 8/1000 - Generator Loss: 1.8738764524459839, Discriminator Loss: 0.64334636926651\n",
      "INFO:root:Epoch 9/1000 - Generator Loss: 1.7176944017410278, Discriminator Loss: 0.64170241355896\n",
      "INFO:root:Epoch 10/1000 - Generator Loss: 0.628803551197052, Discriminator Loss: 2.8911068439483643\n",
      "INFO:root:Epoch 11/1000 - Generator Loss: 2.9162216186523438, Discriminator Loss: 0.3757544457912445\n",
      "INFO:root:Epoch 12/1000 - Generator Loss: 1.117119550704956, Discriminator Loss: 1.5210314989089966\n",
      "INFO:root:Epoch 13/1000 - Generator Loss: 1.6316622495651245, Discriminator Loss: 0.36570388078689575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 13 - Generator Loss: 1.6316622495651245\n",
      "\n",
      "--- Imputation Pass 1/3 ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1/1000 - Generator Loss: 4.151882648468018, Discriminator Loss: 0.6270756721496582\n",
      "INFO:root:Epoch 2/1000 - Generator Loss: 2.7622272968292236, Discriminator Loss: 0.1614200919866562\n",
      "INFO:root:Epoch 3/1000 - Generator Loss: 3.5745601654052734, Discriminator Loss: 0.05827218294143677\n",
      "INFO:root:Epoch 4/1000 - Generator Loss: 3.646433115005493, Discriminator Loss: 1.0920857191085815\n",
      "INFO:root:Epoch 5/1000 - Generator Loss: 2.3976762294769287, Discriminator Loss: 1.139224886894226\n",
      "INFO:root:Epoch 6/1000 - Generator Loss: 2.28826642036438, Discriminator Loss: 0.13038185238838196\n",
      "INFO:root:Epoch 7/1000 - Generator Loss: 2.066287040710449, Discriminator Loss: 0.19943952560424805\n",
      "INFO:root:Epoch 8/1000 - Generator Loss: 1.9305462837219238, Discriminator Loss: 0.40234142541885376\n",
      "INFO:root:Epoch 9/1000 - Generator Loss: 1.8910999298095703, Discriminator Loss: 2.280714988708496\n",
      "INFO:root:Epoch 10/1000 - Generator Loss: 5.218822479248047, Discriminator Loss: 1.0946160554885864\n",
      "INFO:root:Epoch 11/1000 - Generator Loss: 0.9811491370201111, Discriminator Loss: 1.278489351272583\n",
      "INFO:root:Epoch 12/1000 - Generator Loss: 3.7826242446899414, Discriminator Loss: 2.133124589920044\n",
      "INFO:root:Epoch 13/1000 - Generator Loss: 1.768162727355957, Discriminator Loss: 1.4022269248962402\n",
      "INFO:root:Epoch 14/1000 - Generator Loss: 5.782150745391846, Discriminator Loss: 1.2432498931884766\n",
      "INFO:root:Epoch 15/1000 - Generator Loss: 1.428676962852478, Discriminator Loss: 0.5064614415168762\n",
      "INFO:root:Epoch 16/1000 - Generator Loss: 1.6931082010269165, Discriminator Loss: 0.4204074442386627\n",
      "INFO:root:Epoch 17/1000 - Generator Loss: 2.117260217666626, Discriminator Loss: 0.571906566619873\n",
      "INFO:root:Epoch 18/1000 - Generator Loss: 2.3602828979492188, Discriminator Loss: 0.2701558470726013\n",
      "INFO:root:Epoch 19/1000 - Generator Loss: 0.7222023606300354, Discriminator Loss: 1.1752235889434814\n",
      "INFO:root:Epoch 20/1000 - Generator Loss: 0.9526824951171875, Discriminator Loss: 2.569568634033203\n",
      "INFO:root:Epoch 21/1000 - Generator Loss: 2.2113921642303467, Discriminator Loss: 0.938770055770874\n",
      "INFO:root:Epoch 22/1000 - Generator Loss: 1.250969648361206, Discriminator Loss: 1.1790211200714111\n",
      "INFO:root:Epoch 23/1000 - Generator Loss: 1.850905418395996, Discriminator Loss: 2.3253955841064453\n",
      "INFO:root:Epoch 24/1000 - Generator Loss: 0.9898754954338074, Discriminator Loss: 0.505226731300354\n",
      "INFO:root:Epoch 25/1000 - Generator Loss: 1.8029206991195679, Discriminator Loss: 0.34772631525993347\n",
      "INFO:root:Epoch 26/1000 - Generator Loss: 1.9281808137893677, Discriminator Loss: 0.23466962575912476\n",
      "INFO:root:Epoch 27/1000 - Generator Loss: 2.6927998065948486, Discriminator Loss: 0.2132258117198944\n",
      "INFO:root:Epoch 28/1000 - Generator Loss: 0.973545491695404, Discriminator Loss: 0.5212820172309875\n",
      "INFO:root:Epoch 29/1000 - Generator Loss: 2.106024980545044, Discriminator Loss: 0.36998894810676575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 29 - Generator Loss: 2.106024980545044\n",
      "\n",
      "--- Imputation Pass 2/3 ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1/1000 - Generator Loss: 5.5709919929504395, Discriminator Loss: 1.0839076042175293\n",
      "INFO:root:Epoch 2/1000 - Generator Loss: 2.964562177658081, Discriminator Loss: 0.0833107978105545\n",
      "INFO:root:Epoch 3/1000 - Generator Loss: 2.9127371311187744, Discriminator Loss: 0.2790181338787079\n",
      "INFO:root:Epoch 4/1000 - Generator Loss: 3.130985975265503, Discriminator Loss: 3.5889933109283447\n",
      "INFO:root:Epoch 5/1000 - Generator Loss: 3.240677833557129, Discriminator Loss: 0.18484221398830414\n",
      "INFO:root:Epoch 6/1000 - Generator Loss: 2.6178717613220215, Discriminator Loss: 0.26956987380981445\n",
      "INFO:root:Epoch 7/1000 - Generator Loss: 4.347444534301758, Discriminator Loss: 0.12983649969100952\n",
      "INFO:root:Epoch 8/1000 - Generator Loss: 1.6717772483825684, Discriminator Loss: 0.3316692113876343\n",
      "INFO:root:Epoch 9/1000 - Generator Loss: 2.439965009689331, Discriminator Loss: 0.2308366596698761\n",
      "INFO:root:Epoch 10/1000 - Generator Loss: 1.359175205230713, Discriminator Loss: 0.24769434332847595\n",
      "INFO:root:Epoch 11/1000 - Generator Loss: 3.358708143234253, Discriminator Loss: 0.13204200565814972\n",
      "INFO:root:Epoch 12/1000 - Generator Loss: 0.7555756568908691, Discriminator Loss: 0.9069691896438599\n",
      "INFO:root:Epoch 13/1000 - Generator Loss: 2.505873918533325, Discriminator Loss: 0.2971211075782776\n",
      "INFO:root:Epoch 14/1000 - Generator Loss: 3.074514627456665, Discriminator Loss: 0.2210788130760193\n",
      "INFO:root:Epoch 15/1000 - Generator Loss: 1.0006400346755981, Discriminator Loss: 2.8524396419525146\n",
      "INFO:root:Epoch 16/1000 - Generator Loss: 2.1239662170410156, Discriminator Loss: 0.4632316827774048\n",
      "INFO:root:Epoch 17/1000 - Generator Loss: 4.0688252449035645, Discriminator Loss: 3.440040111541748\n",
      "INFO:root:Epoch 18/1000 - Generator Loss: 1.9810868501663208, Discriminator Loss: 0.47382187843322754\n",
      "INFO:root:Epoch 19/1000 - Generator Loss: 3.1613552570343018, Discriminator Loss: 0.40583714842796326\n",
      "INFO:root:Epoch 20/1000 - Generator Loss: 2.0022361278533936, Discriminator Loss: 0.28980106115341187\n",
      "INFO:root:Epoch 21/1000 - Generator Loss: 1.5182833671569824, Discriminator Loss: 0.5221204161643982\n",
      "INFO:root:Epoch 22/1000 - Generator Loss: 1.8190360069274902, Discriminator Loss: 2.4130961894989014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 22 - Generator Loss: 1.8190360069274902\n",
      "\n",
      "--- Imputation Pass 3/3 ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1/1000 - Generator Loss: 2.461106300354004, Discriminator Loss: 0.108174629509449\n",
      "INFO:root:Epoch 2/1000 - Generator Loss: 3.7974936962127686, Discriminator Loss: 0.19220669567584991\n",
      "INFO:root:Epoch 3/1000 - Generator Loss: 2.274738073348999, Discriminator Loss: 0.11671611666679382\n",
      "INFO:root:Epoch 4/1000 - Generator Loss: 3.8480770587921143, Discriminator Loss: 0.06884589791297913\n",
      "INFO:root:Epoch 5/1000 - Generator Loss: 2.05767560005188, Discriminator Loss: 0.3324459195137024\n",
      "INFO:root:Epoch 6/1000 - Generator Loss: 2.1923422813415527, Discriminator Loss: 0.16906222701072693\n",
      "INFO:root:Epoch 7/1000 - Generator Loss: 0.9525883793830872, Discriminator Loss: 2.6885037422180176\n",
      "INFO:root:Epoch 8/1000 - Generator Loss: 2.0926873683929443, Discriminator Loss: 1.260503888130188\n",
      "INFO:root:Epoch 9/1000 - Generator Loss: 4.128427505493164, Discriminator Loss: 1.2086541652679443\n",
      "INFO:root:Epoch 10/1000 - Generator Loss: 4.917284965515137, Discriminator Loss: 1.1284611225128174\n",
      "INFO:root:Epoch 11/1000 - Generator Loss: 3.810129165649414, Discriminator Loss: 0.4191823899745941\n",
      "INFO:root:Epoch 12/1000 - Generator Loss: 2.6488542556762695, Discriminator Loss: 0.20135821402072906\n",
      "INFO:root:Epoch 13/1000 - Generator Loss: 2.6063430309295654, Discriminator Loss: 0.2387940138578415\n",
      "INFO:root:Epoch 14/1000 - Generator Loss: 1.8408045768737793, Discriminator Loss: 0.8605287671089172\n",
      "INFO:root:Epoch 15/1000 - Generator Loss: 3.311103582382202, Discriminator Loss: 0.20078732073307037\n",
      "INFO:root:Epoch 16/1000 - Generator Loss: 1.8331695795059204, Discriminator Loss: 1.2630975246429443\n",
      "INFO:root:Epoch 17/1000 - Generator Loss: 1.0050984621047974, Discriminator Loss: 0.4735427498817444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 17 - Generator Loss: 1.0050984621047974\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset class for PyTorch\n",
    "def prepare_dataset(X):\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, data):\n",
    "            # Ensure data is converted to numeric type to avoid object type errors\n",
    "            self.data = torch.tensor(data.astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx]\n",
    "    \n",
    "    return CustomDataset(X)\n",
    "\n",
    "# Define Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# Define Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# GAN Imputation Function with Multiple Passes and Early Stopping\n",
    "def impute_missing_values_with_multiple_passes(X, num_passes=3, epochs=1000, batch_size=64, learning_rate=0.0002, patience=10):\n",
    "    # Prepare data and mask for missing values\n",
    "    X_imputed = X.copy()\n",
    "    mask = X_imputed.isna()\n",
    "    X_imputed.fillna(0, inplace=True)\n",
    "    \n",
    "    # Input dimension\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    # Multiple passes\n",
    "    for pass_num in range(num_passes):\n",
    "        print(f\"\\n--- Imputation Pass {pass_num + 1}/{num_passes} ---\\n\")\n",
    "        \n",
    "        # Prepare dataset and dataloader\n",
    "        dataset = prepare_dataset(X_imputed.values)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Initialize Generator and Discriminator\n",
    "        generator = Generator(input_dim)\n",
    "        discriminator = Discriminator(input_dim)\n",
    "\n",
    "        # Optimizers\n",
    "        optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "        optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Loss function\n",
    "        adversarial_loss = nn.BCELoss()\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i, real_data in enumerate(dataloader):\n",
    "                # Adversarial ground truths\n",
    "                valid = torch.ones(real_data.size(0), 1)\n",
    "                fake = torch.zeros(real_data.size(0), 1)\n",
    "\n",
    "                # Train Generator\n",
    "                optimizer_G.zero_grad()\n",
    "\n",
    "                # Generate data and replace missing values with generated data\n",
    "                gen_data = generator(real_data)\n",
    "                gen_data[mask.iloc[i * batch_size:(i + 1) * batch_size].values] = real_data[mask.iloc[i * batch_size:(i + 1) * batch_size].values]\n",
    "\n",
    "                # Generator loss\n",
    "                g_loss = adversarial_loss(discriminator(gen_data), valid)\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "                # Train Discriminator\n",
    "                optimizer_D.zero_grad()\n",
    "\n",
    "                # Real and fake losses\n",
    "                real_loss = adversarial_loss(discriminator(real_data), valid)\n",
    "                fake_loss = adversarial_loss(discriminator(gen_data.detach()), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "            # Display progress\n",
    "            #print(f\"Epoch {epoch + 1}/{epochs} - Generator Loss: {g_loss.item()}, Discriminator Loss: {d_loss.item()}\")\n",
    "\n",
    "            # Logging progress\n",
    "            logging.basicConfig(level=logging.INFO)\n",
    "            logging.info(f\"Epoch {epoch + 1}/{epochs} - Generator Loss: {g_loss.item()}, Discriminator Loss: {d_loss.item()}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if g_loss.item() < best_loss:\n",
    "                best_loss = g_loss.item()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} - Generator Loss: {g_loss.item()}\")\n",
    "                break\n",
    "\n",
    "        # Update X_imputed with refined values\n",
    "        X_imputed_tensor = torch.tensor(X_imputed.values.astype(np.float32), dtype=torch.float32)\n",
    "        refined_data = generator(X_imputed_tensor).detach().numpy()\n",
    "        X_imputed.values[mask.values] = refined_data[mask.values]  # Only update missing values\n",
    "\n",
    "    return pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# Impute missing values for each dataset separately\n",
    "X_external_imputed = impute_missing_values_with_multiple_passes(X_external)\n",
    "X_train_imputed = impute_missing_values_with_multiple_passes(X_train)\n",
    "X_validate_imputed = impute_missing_values_with_multiple_passes(X_validate)\n",
    "X_test_imputed = impute_missing_values_with_multiple_passes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818233d3-e09a-41e1-ae33-e0b1ba6d8be1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# o03 Run GAN multiple times with more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533af569-8ca5-491a-a367-4aa460846525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Custom Dataset class for PyTorch\n",
    "def prepare_dataset(X):\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, data):\n",
    "            # Ensure data is converted to numeric type to avoid object type errors\n",
    "            self.data = torch.tensor(data.astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx]\n",
    "    \n",
    "    return CustomDataset(X)\n",
    "\n",
    "# Define Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# Define Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# GAN Imputation Function with Multiple Passes and Early Stopping\n",
    "def impute_missing_values_with_multiple_passes(X, num_passes=3, epochs=1000, batch_size=64, learning_rate=0.0002, patience=10):\n",
    "    # Prepare data and mask for missing values\n",
    "    X_imputed = X.copy()\n",
    "    mask = X_imputed.isna()\n",
    "    X_imputed.fillna(0, inplace=True)\n",
    "    \n",
    "    # Input dimension\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    # Multiple passes\n",
    "    for pass_num in range(num_passes):\n",
    "        print(f\"\\n--- Imputation Pass {pass_num + 1}/{num_passes} ---\\n\")\n",
    "        \n",
    "        # Prepare dataset and dataloader\n",
    "        dataset = prepare_dataset(X_imputed.values)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Initialize Generator and Discriminator\n",
    "        generator = Generator(input_dim)\n",
    "        discriminator = Discriminator(input_dim)\n",
    "\n",
    "        # Optimizers\n",
    "        optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "        optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Loss function\n",
    "        adversarial_loss = nn.BCELoss()\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i, real_data in enumerate(dataloader):\n",
    "                # Adversarial ground truths\n",
    "                valid = torch.ones(real_data.size(0), 1)\n",
    "                fake = torch.zeros(real_data.size(0), 1)\n",
    "\n",
    "                # Train Generator\n",
    "                optimizer_G.zero_grad()\n",
    "\n",
    "                # Generate data and replace missing values with generated data\n",
    "                gen_data = generator(real_data)\n",
    "                gen_data[mask.iloc[i * batch_size:(i + 1) * batch_size].values] = real_data[mask.iloc[i * batch_size:(i + 1) * batch_size].values]\n",
    "\n",
    "                # Generator loss\n",
    "                g_loss = adversarial_loss(discriminator(gen_data), valid)\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "                # Train Discriminator\n",
    "                optimizer_D.zero_grad()\n",
    "\n",
    "                # Real and fake losses\n",
    "                real_loss = adversarial_loss(discriminator(real_data), valid)\n",
    "                fake_loss = adversarial_loss(discriminator(gen_data.detach()), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "            # Display progress\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} - Generator Loss: {g_loss.item()}, Discriminator Loss: {d_loss.item()}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if g_loss.item() < best_loss:\n",
    "                best_loss = g_loss.item()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} - Generator Loss: {g_loss.item()}\")\n",
    "                break\n",
    "\n",
    "        # Update X_imputed with refined values\n",
    "        X_imputed_tensor = torch.tensor(X_imputed.values.astype(np.float32), dtype=torch.float32)\n",
    "        refined_data = generator(X_imputed_tensor).detach().numpy()\n",
    "        X_imputed.values[mask.values] = refined_data[mask.values]  # Only update missing values\n",
    "\n",
    "    return pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# Impute missing values for each dataset separately\n",
    "X_external_imputed = impute_missing_values_with_multiple_passes(X_external)\n",
    "X_train_imputed = impute_missing_values_with_multiple_passes(X_train)\n",
    "X_validate_imputed = impute_missing_values_with_multiple_passes(X_validate)\n",
    "X_test_imputed = impute_missing_values_with_multiple_passes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbac505-28a6-42b0-ba5e-b62c057d8cf3",
   "metadata": {},
   "source": [
    "# Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "974417b2-ddb0-4b83-a135-7e5717e726e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 missing values in external dataset\n",
      "\n",
      "0 missing values in train dataset\n",
      "\n",
      "0 missing values in validation dataset\n",
      "\n",
      "0 missing values in test dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "external_total_missing_values = X_external_imputed.isna().sum().sum()\n",
    "train_total_missing_values = X_train_imputed.isna().sum().sum()\n",
    "validation_total_missing_values = X_validate_imputed.isna().sum().sum()\n",
    "test_total_missing_values = X_test_imputed.isna().sum().sum()\n",
    "print(external_total_missing_values, 'missing values in external dataset\\n')\n",
    "print(train_total_missing_values, 'missing values in train dataset\\n')\n",
    "print(validation_total_missing_values, 'missing values in validation dataset\\n')\n",
    "print(test_total_missing_values, 'missing values in test dataset\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afd7711a-d7a0-427c-8f04-4fc318d32961",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_external = X_external.reset_index(drop=True)\n",
    "X_external_imputed = X_external_imputed.reset_index(drop=True)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_train_imputed = X_train_imputed.reset_index(drop=True)\n",
    "\n",
    "X_validate = X_validate.reset_index(drop=True)\n",
    "X_validate_imputed = X_validate_imputed.reset_index(drop=True)\n",
    "\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "X_test_imputed = X_test_imputed.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6cdd8a1-e7a7-4604-8ded-f9e741eebcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External replacement completed.\n"
     ]
    }
   ],
   "source": [
    "# List of columns to replace: 'age' and columns starting with 'race_'\n",
    "columns_to_replace = ['age'] + [col for col in X_external.columns if col.startswith('race_')]\n",
    "\n",
    "# Function to replace specified columns in the imputed dataframe with original data\n",
    "def replace_columns(imputed_df, original_df, columns):\n",
    "    imputed_df[columns] = original_df[columns]\n",
    "    return imputed_df\n",
    "\n",
    "X_external_imputed = replace_columns(X_external_imputed, X_external, columns_to_replace)\n",
    "print(\"External replacement completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7a70d77-2d6c-402a-af08-bcad0f6ad027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train replacement completed.\n"
     ]
    }
   ],
   "source": [
    "# List of columns to replace: 'age' and columns starting with 'race_'\n",
    "columns_to_replace = ['age'] + [col for col in X_train.columns if col.startswith('race_')]\n",
    "\n",
    "# Function to replace specified columns in the imputed dataframe with original data\n",
    "def replace_columns(imputed_df, original_df, columns):\n",
    "    imputed_df[columns] = original_df[columns]\n",
    "    return imputed_df\n",
    "\n",
    "\n",
    "X_train_imputed = replace_columns(X_train_imputed, X_train, columns_to_replace)\n",
    "print(\"Train replacement completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48721be6-08c3-4561-bdd1-084b02265bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate replacement completed.\n"
     ]
    }
   ],
   "source": [
    "# List of columns to replace: 'age' and columns starting with 'race_'\n",
    "columns_to_replace = ['age'] + [col for col in X_validate.columns if col.startswith('race_')]\n",
    "\n",
    "# Function to replace specified columns in the imputed dataframe with original data\n",
    "def replace_columns(imputed_df, original_df, columns):\n",
    "    imputed_df[columns] = original_df[columns]\n",
    "    return imputed_df\n",
    "\n",
    "\n",
    "X_validate_imputed = replace_columns(X_validate_imputed, X_validate, columns_to_replace)\n",
    "print(\"Validate replacement completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e4cf2c6-5da7-4006-aadf-bd7330a563aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test replacement completed.\n"
     ]
    }
   ],
   "source": [
    "# List of columns to replace: 'age' and columns starting with 'race_'\n",
    "columns_to_replace = ['age'] + [col for col in X_test.columns if col.startswith('race_')]\n",
    "\n",
    "# Function to replace specified columns in the imputed dataframe with original data\n",
    "def replace_columns(imputed_df, original_df, columns):\n",
    "    imputed_df[columns] = original_df[columns]\n",
    "    return imputed_df\n",
    "\n",
    "\n",
    "X_test_imputed = replace_columns(X_test_imputed, X_test, columns_to_replace)\n",
    "print(\"Test replacement completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae9e1e27-02f1-46a0-a457-3384d3876b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "save_path = 'CSV/exports/impute/o6_GAN/o02/'\n",
    "\n",
    "# Check if the directory exists, and if not, create it\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Save external validation set from eICU\n",
    "X_external_imputed.to_csv(save_path + 'X_external.csv', index=False)\n",
    "y_external.to_csv(save_path + 'y_external.csv', index=False)\n",
    "\n",
    "# Save training, validation, and test sets\n",
    "X_train_imputed.to_csv(save_path + 'X_train.csv', index=False)\n",
    "y_train.to_csv(save_path + 'y_train.csv', index=False)\n",
    "\n",
    "X_validate_imputed.to_csv(save_path + 'X_validate.csv', index=False)\n",
    "y_validate.to_csv(save_path + 'y_validate.csv', index=False)\n",
    "\n",
    "X_test_imputed.to_csv(save_path + 'X_test.csv', index=False)\n",
    "y_test.to_csv(save_path + 'y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f46f16-292a-4b63-8f7e-6187f8d6b98c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09cdb3-12a4-4e78-8012-b009603b28e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
