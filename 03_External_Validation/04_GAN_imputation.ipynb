{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27150fe1-2273-4084-bfdb-af667c2c54c2",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba9ff51-183c-48dd-ab85-986b02ba8647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterGrid, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c94c6d-1c45-4a23-8d0c-ced1519d7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MIMICs CSV file\n",
    "mimic_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\mimic_mean_median_min_max_final.csv\")\n",
    "\n",
    "# Read eICUs CSV file\n",
    "eicu_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\eicu_mean_median_min_max_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73ecd05-dc5d-4103-8959-4948a825288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 10\n",
    "\n",
    "# Filter icu stay less than 10 days\n",
    "mimic_df = mimic_df[mimic_df['los'] < day]\n",
    "\n",
    "# Filter icu stay less than 10 days\n",
    "eicu_df = eicu_df[eicu_df['los'] < day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f2142c-43bf-44fe-9838-b7be5302785d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 51040\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I'm gonna concat and split the mimic and icu\n",
    "at this point. I must create the same columns\n",
    "from the tranformation of categorical data.\n",
    "\"\"\"\n",
    "row_count = mimic_df.shape[0]\n",
    "print(f\"Row count: {row_count}\")\n",
    "\n",
    "# Concat dataframes\n",
    "df_combined = pd.concat([mimic_df, eicu_df], ignore_index=True)\n",
    "\n",
    "# Find all categorical columns in mimic\n",
    "categorical_columns = df_combined.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Apply one-hot encoding to all categorical columns\n",
    "df_encoded = pd.get_dummies(df_combined, columns=categorical_columns)\n",
    "\n",
    "# Split the concatenate dataframe\n",
    "mimic_df = df_encoded.iloc[:row_count, :]\n",
    "eicu_df = df_encoded.iloc[row_count:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01eb144-0185-4e6c-94f4-43104907fd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 40832\n",
      "Validation set size: 5104\n",
      "Test set size: 5104\n"
     ]
    }
   ],
   "source": [
    "total_test_val_perc = 0.2\n",
    "split_between_test_val_perc = 0.5\n",
    "\n",
    "# Group data by subject_id and hadm_id\n",
    "grouped_df = mimic_df.groupby(['subject_id', 'hadm_id'])\n",
    "\n",
    "# Get a new dataframe with one row per patient (subject_id, hadm_id) pair\n",
    "patient_df = grouped_df['hospital_expire_flag'].first().reset_index()\n",
    "\n",
    "# Split the patient_df into training (80%), validation (10%), and test (10%) while keeping the ratio of hospital_expired_flag\n",
    "train, temp = train_test_split(patient_df, test_size=total_test_val_perc, stratify=patient_df['hospital_expire_flag'], random_state=42)\n",
    "val, test = train_test_split(temp, test_size=split_between_test_val_perc, stratify=temp['hospital_expire_flag'], random_state=42)\n",
    "\n",
    "# Step 4: Merge back with the original df to get the rows for each patient in the splits\n",
    "train_df = mimic_df.merge(train[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "val_df = mimic_df.merge(val[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "test_df = mimic_df.merge(test[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "\n",
    "# Check the sizes of the splits\n",
    "print(f'Training set size: {train_df.shape[0]}')\n",
    "print(f'Validation set size: {val_df.shape[0]}')\n",
    "print(f'Test set size: {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4db47f7-426f-47b6-aada-48b382ceed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External validation from eICU\n",
    "X_external = eicu_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_external = eicu_df['los']\n",
    "\n",
    "# Separate features and target for the training, validation, and test sets\n",
    "X_train = train_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_train = train_df['los']\n",
    "\n",
    "X_validate = val_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_validate = val_df['los']\n",
    "\n",
    "X_test = test_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_test = test_df['los']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a0783-9c31-47d9-8cb4-b093c68c1302",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load Train - Validation - Test & External Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53833f-0875-467d-a556-4e3508c362fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subfolder\n",
    "subfolder = \"o2_interpolation_impute\"\n",
    "\n",
    "# Load CSV files into corresponding variables\n",
    "X_external = pd.read_csv(f\"CSV/exports/impute/{subfolder}/X_external.csv\")\n",
    "y_external = pd.read_csv(f\"CSV/exports/impute/{subfolder}/y_external.csv\")\n",
    "X_train = pd.read_csv(f\"CSV/exports/impute/{subfolder}/X_train.csv\")\n",
    "y_train = pd.read_csv(f\"CSV/exports/impute/{subfolder}/y_train.csv\")\n",
    "X_validate = pd.read_csv(f\"CSV/exports/impute/{subfolder}/X_validate.csv\")\n",
    "y_validate = pd.read_csv(f\"CSV/exports/impute/{subfolder}/y_validate.csv\")\n",
    "X_test = pd.read_csv(f\"CSV/exports/impute/{subfolder}/X_test.csv\")\n",
    "y_test = pd.read_csv(f\"CSV/exports/impute/{subfolder}/y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbac505-28a6-42b0-ba5e-b62c057d8cf3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974417b2-ddb0-4b83-a135-7e5717e726e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_total_missing_values = X_external_imputed.isna().sum().sum()\n",
    "train_total_missing_values = X_train_imputed.isna().sum().sum()\n",
    "validation_total_missing_values = X_validate_imputed.isna().sum().sum()\n",
    "test_total_missing_values = X_test_imputed.isna().sum().sum()\n",
    "print(external_total_missing_values, 'missing values in external dataset\\n')\n",
    "print(train_total_missing_values, 'missing values in train dataset\\n')\n",
    "print(validation_total_missing_values, 'missing values in validation dataset\\n')\n",
    "print(test_total_missing_values, 'missing values in test dataset\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7711a-d7a0-427c-8f04-4fc318d32961",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_external = X_external.reset_index(drop=True)\n",
    "X_external_imputed = X_external_imputed.reset_index(drop=True)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_train_imputed = X_train_imputed.reset_index(drop=True)\n",
    "\n",
    "X_validate = X_validate.reset_index(drop=True)\n",
    "X_validate_imputed = X_validate_imputed.reset_index(drop=True)\n",
    "\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "X_test_imputed = X_test_imputed.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cdd8a1-e7a7-4604-8ded-f9e741eebcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to replace: 'age' and columns starting with 'race_'\n",
    "columns_to_replace = ['age'] + [col for col in X_external.columns if col.startswith('race_')]\n",
    "\n",
    "# Function to replace specified columns in the imputed dataframe with original data\n",
    "def replace_columns(imputed_df, original_df, columns):\n",
    "    imputed_df[columns] = original_df[columns]\n",
    "    return imputed_df\n",
    "\n",
    "X_external_imputed = replace_columns(X_external_imputed, X_external, columns_to_replace)\n",
    "print(\"External replacement completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a70d77-2d6c-402a-af08-bcad0f6ad027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to replace: 'age' and columns starting with 'race_'\n",
    "columns_to_replace = ['age'] + [col for col in X_train.columns if col.startswith('race_')]\n",
    "\n",
    "# Function to replace specified columns in the imputed dataframe with original data\n",
    "def replace_columns(imputed_df, original_df, columns):\n",
    "    imputed_df[columns] = original_df[columns]\n",
    "    return imputed_df\n",
    "\n",
    "\n",
    "X_train_imputed = replace_columns(X_train_imputed, X_train, columns_to_replace)\n",
    "print(\"Train replacement completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48721be6-08c3-4561-bdd1-084b02265bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to replace: 'age' and columns starting with 'race_'\n",
    "columns_to_replace = ['age'] + [col for col in X_validate.columns if col.startswith('race_')]\n",
    "\n",
    "# Function to replace specified columns in the imputed dataframe with original data\n",
    "def replace_columns(imputed_df, original_df, columns):\n",
    "    imputed_df[columns] = original_df[columns]\n",
    "    return imputed_df\n",
    "\n",
    "\n",
    "X_validate_imputed = replace_columns(X_validate_imputed, X_validate, columns_to_replace)\n",
    "print(\"Validate replacement completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4cf2c6-5da7-4006-aadf-bd7330a563aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to replace: 'age' and columns starting with 'race_'\n",
    "columns_to_replace = ['age'] + [col for col in X_test.columns if col.startswith('race_')]\n",
    "\n",
    "# Function to replace specified columns in the imputed dataframe with original data\n",
    "def replace_columns(imputed_df, original_df, columns):\n",
    "    imputed_df[columns] = original_df[columns]\n",
    "    return imputed_df\n",
    "\n",
    "\n",
    "X_test_imputed = replace_columns(X_test_imputed, X_test, columns_to_replace)\n",
    "print(\"Test replacement completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e1e27-02f1-46a0-a457-3384d3876b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "save_path = 'CSV/exports/impute/o6_GAN/'\n",
    "\n",
    "# Check if the directory exists, and if not, create it\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Save external validation set from eICU\n",
    "X_external_imputed.to_csv(save_path + 'X_external.csv', index=False)\n",
    "y_external.to_csv(save_path + 'y_external.csv', index=False)\n",
    "\n",
    "# Save training, validation, and test sets\n",
    "X_train_imputed.to_csv(save_path + 'X_train.csv', index=False)\n",
    "y_train.to_csv(save_path + 'y_train.csv', index=False)\n",
    "\n",
    "X_validate_imputed.to_csv(save_path + 'X_validate.csv', index=False)\n",
    "y_validate.to_csv(save_path + 'y_validate.csv', index=False)\n",
    "\n",
    "X_test_imputed.to_csv(save_path + 'X_test.csv', index=False)\n",
    "y_test.to_csv(save_path + 'y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1ea72-18c5-4f51-a463-15f83bb26771",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GAN first approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49001668-dc2b-4c05-8e19-9e2bcbfbbf8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Custom Dataset class for PyTorch\n",
    "def prepare_dataset(X):\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, data):\n",
    "            # Ensure data is converted to numeric type to avoid object type errors\n",
    "            self.data = torch.tensor(data.astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx]\n",
    "    \n",
    "    return CustomDataset(X)\n",
    "\n",
    "# Define Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# Define Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# GAN Imputation Function with Early Stopping\n",
    "def impute_missing_values_with_gan(X, epochs=1000, batch_size=64, learning_rate=0.0002, patience=10):\n",
    "    # Prepare data and mask for missing values\n",
    "    X_missing = X.copy()\n",
    "    mask = X_missing.isna()\n",
    "    X_missing.fillna(0, inplace=True)\n",
    "    \n",
    "    # Prepare dataset and dataloader\n",
    "    dataset = prepare_dataset(X_missing.values)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    # Initialize Generator and Discriminator\n",
    "    generator = Generator(input_dim)\n",
    "    discriminator = Discriminator(input_dim)\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Loss function\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, real_data in enumerate(dataloader):\n",
    "            # Adversarial ground truths\n",
    "            valid = torch.ones(real_data.size(0), 1)\n",
    "            fake = torch.zeros(real_data.size(0), 1)\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate data and replace missing values with generated data\n",
    "            gen_data = generator(real_data)\n",
    "            gen_data[mask.iloc[i * batch_size:(i + 1) * batch_size].values] = real_data[mask.iloc[i * batch_size:(i + 1) * batch_size].values]\n",
    "\n",
    "            # Generator loss\n",
    "            g_loss = adversarial_loss(discriminator(gen_data), valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Real and fake losses\n",
    "            real_loss = adversarial_loss(discriminator(real_data), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_data.detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # Display progress\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Generator Loss: {g_loss.item()}, Discriminator Loss: {d_loss.item()}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if g_loss.item() < best_loss:\n",
    "            best_loss = g_loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} - Generator Loss: {g_loss.item()}\")\n",
    "            break\n",
    "\n",
    "    # Impute missing values using the trained generator\n",
    "    X_imputed = X_missing.copy()\n",
    "    X_imputed = torch.tensor(X_imputed.values.astype(np.float32), dtype=torch.float32)\n",
    "    X_imputed = generator(X_imputed).detach().numpy()\n",
    "    X_imputed[mask.values] = X_missing.values[mask.values]\n",
    "\n",
    "    return pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# Impute missing values for each dataset separately\n",
    "X_external_imputed = impute_missing_values_with_gan(X_external)\n",
    "X_train_imputed = impute_missing_values_with_gan(X_train)\n",
    "X_validate_imputed = impute_missing_values_with_gan(X_validate)\n",
    "X_test_imputed = impute_missing_values_with_gan(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db63bc38-3870-4a3e-bc8e-95c90d03569b",
   "metadata": {},
   "source": [
    "# Run GAN multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044bafa-3401-42fd-b9f7-5be534ee19a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Imputation Pass 1/3 ---\n",
      "\n",
      "Epoch 1/1000 - Generator Loss: 1.6034334897994995, Discriminator Loss: 0.9377586841583252\n",
      "Epoch 2/1000 - Generator Loss: 1.5101115703582764, Discriminator Loss: 0.47118115425109863\n",
      "Epoch 3/1000 - Generator Loss: 1.4963160753250122, Discriminator Loss: 0.4700693190097809\n",
      "Epoch 4/1000 - Generator Loss: 2.1910805702209473, Discriminator Loss: 0.2253631353378296\n",
      "Epoch 5/1000 - Generator Loss: 0.931593120098114, Discriminator Loss: 0.6955404281616211\n",
      "Epoch 6/1000 - Generator Loss: 1.7881419658660889, Discriminator Loss: 0.3181227743625641\n",
      "Epoch 7/1000 - Generator Loss: 1.4367799758911133, Discriminator Loss: 0.3280876874923706\n",
      "Epoch 8/1000 - Generator Loss: 1.332382082939148, Discriminator Loss: 0.4775434732437134\n",
      "Epoch 9/1000 - Generator Loss: 1.6364402770996094, Discriminator Loss: 0.4767865538597107\n",
      "Epoch 10/1000 - Generator Loss: 1.2618499994277954, Discriminator Loss: 0.5217132568359375\n",
      "Epoch 11/1000 - Generator Loss: 0.9649321436882019, Discriminator Loss: 0.5574689507484436\n",
      "Epoch 12/1000 - Generator Loss: 0.9722137451171875, Discriminator Loss: 0.5677946209907532\n",
      "Epoch 13/1000 - Generator Loss: 1.787104845046997, Discriminator Loss: 0.31168103218078613\n",
      "Epoch 14/1000 - Generator Loss: 1.0184141397476196, Discriminator Loss: 0.4883333444595337\n",
      "Epoch 15/1000 - Generator Loss: 1.8924862146377563, Discriminator Loss: 0.2638247013092041\n",
      "Early stopping at epoch 15 - Generator Loss: 1.8924862146377563\n",
      "\n",
      "--- Imputation Pass 2/3 ---\n",
      "\n",
      "Epoch 1/1000 - Generator Loss: 1.1362165212631226, Discriminator Loss: 1.4398794174194336\n",
      "Epoch 2/1000 - Generator Loss: 1.513779640197754, Discriminator Loss: 1.176546573638916\n",
      "Epoch 3/1000 - Generator Loss: 0.9309788346290588, Discriminator Loss: 0.5151770114898682\n",
      "Epoch 4/1000 - Generator Loss: 1.4890551567077637, Discriminator Loss: 0.4230167269706726\n",
      "Epoch 5/1000 - Generator Loss: 1.3810300827026367, Discriminator Loss: 0.43186140060424805\n",
      "Epoch 6/1000 - Generator Loss: 2.799375057220459, Discriminator Loss: 0.29768872261047363\n",
      "Epoch 7/1000 - Generator Loss: 1.5468977689743042, Discriminator Loss: 0.521338939666748\n",
      "Epoch 8/1000 - Generator Loss: 2.0198681354522705, Discriminator Loss: 1.796881079673767\n",
      "Epoch 9/1000 - Generator Loss: 2.3595099449157715, Discriminator Loss: 0.3996061086654663\n",
      "Epoch 10/1000 - Generator Loss: 1.7814021110534668, Discriminator Loss: 0.33643776178359985\n",
      "Epoch 11/1000 - Generator Loss: 1.3629230260849, Discriminator Loss: 0.5134323239326477\n",
      "Epoch 12/1000 - Generator Loss: 2.3030498027801514, Discriminator Loss: 0.3633018136024475\n",
      "Epoch 13/1000 - Generator Loss: 1.3281606435775757, Discriminator Loss: 0.28882521390914917\n",
      "Early stopping at epoch 13 - Generator Loss: 1.3281606435775757\n",
      "\n",
      "--- Imputation Pass 3/3 ---\n",
      "\n",
      "Epoch 1/1000 - Generator Loss: 3.3496904373168945, Discriminator Loss: 0.37368834018707275\n",
      "Epoch 2/1000 - Generator Loss: 1.909521460533142, Discriminator Loss: 0.41723209619522095\n",
      "Epoch 3/1000 - Generator Loss: 2.833038806915283, Discriminator Loss: 0.24063140153884888\n",
      "Epoch 4/1000 - Generator Loss: 2.1765050888061523, Discriminator Loss: 0.6430994272232056\n",
      "Epoch 5/1000 - Generator Loss: 2.3255858421325684, Discriminator Loss: 0.3126998543739319\n",
      "Epoch 6/1000 - Generator Loss: 0.9045647978782654, Discriminator Loss: 0.5385386943817139\n",
      "Epoch 7/1000 - Generator Loss: 1.4059484004974365, Discriminator Loss: 0.44377195835113525\n",
      "Epoch 8/1000 - Generator Loss: 2.0449461936950684, Discriminator Loss: 0.24856199324131012\n",
      "Epoch 9/1000 - Generator Loss: 1.7662267684936523, Discriminator Loss: 0.43886566162109375\n",
      "Epoch 10/1000 - Generator Loss: 2.145477056503296, Discriminator Loss: 0.6195065379142761\n",
      "Epoch 11/1000 - Generator Loss: 6.281660079956055, Discriminator Loss: 0.2098648101091385\n",
      "Epoch 12/1000 - Generator Loss: 1.4689500331878662, Discriminator Loss: 0.3692675828933716\n",
      "Epoch 13/1000 - Generator Loss: 1.224586009979248, Discriminator Loss: 0.5255898237228394\n",
      "Epoch 14/1000 - Generator Loss: 1.3962814807891846, Discriminator Loss: 1.2338721752166748\n",
      "Epoch 15/1000 - Generator Loss: 3.3106484413146973, Discriminator Loss: 0.18656408786773682\n",
      "Epoch 16/1000 - Generator Loss: 1.1905782222747803, Discriminator Loss: 0.44546711444854736\n",
      "Early stopping at epoch 16 - Generator Loss: 1.1905782222747803\n",
      "\n",
      "--- Imputation Pass 1/3 ---\n",
      "\n",
      "Epoch 1/1000 - Generator Loss: 2.3844103813171387, Discriminator Loss: 1.1435825824737549\n",
      "Epoch 2/1000 - Generator Loss: 2.2723140716552734, Discriminator Loss: 0.5778213739395142\n",
      "Epoch 3/1000 - Generator Loss: 0.9818607568740845, Discriminator Loss: 0.474947988986969\n",
      "Epoch 4/1000 - Generator Loss: 1.7800979614257812, Discriminator Loss: 1.3166298866271973\n",
      "Epoch 5/1000 - Generator Loss: 2.279453754425049, Discriminator Loss: 0.2205762267112732\n",
      "Epoch 6/1000 - Generator Loss: 2.3844175338745117, Discriminator Loss: 0.23264877498149872\n",
      "Epoch 7/1000 - Generator Loss: 2.754976511001587, Discriminator Loss: 0.26980698108673096\n",
      "Epoch 8/1000 - Generator Loss: 2.4158811569213867, Discriminator Loss: 0.39862892031669617\n",
      "Epoch 9/1000 - Generator Loss: 3.6678857803344727, Discriminator Loss: 0.1740161031484604\n",
      "Epoch 10/1000 - Generator Loss: 1.0594391822814941, Discriminator Loss: 0.475462943315506\n",
      "Epoch 11/1000 - Generator Loss: 0.9469903111457825, Discriminator Loss: 0.8489866852760315\n",
      "Epoch 12/1000 - Generator Loss: 1.7726973295211792, Discriminator Loss: 0.35443687438964844\n",
      "Epoch 13/1000 - Generator Loss: 2.6165730953216553, Discriminator Loss: 0.2834811806678772\n",
      "Epoch 14/1000 - Generator Loss: 2.3581531047821045, Discriminator Loss: 0.5573980212211609\n",
      "Epoch 15/1000 - Generator Loss: 2.808501958847046, Discriminator Loss: 0.2653995752334595\n",
      "Epoch 16/1000 - Generator Loss: 2.463968276977539, Discriminator Loss: 0.3024720549583435\n",
      "Epoch 17/1000 - Generator Loss: 2.696824312210083, Discriminator Loss: 0.20642662048339844\n",
      "Epoch 18/1000 - Generator Loss: 2.4849555492401123, Discriminator Loss: 0.30342358350753784\n",
      "Epoch 19/1000 - Generator Loss: 2.0870702266693115, Discriminator Loss: 0.26357248425483704\n",
      "Epoch 20/1000 - Generator Loss: 2.4468131065368652, Discriminator Loss: 0.23884965479373932\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset class for PyTorch\n",
    "def prepare_dataset(X):\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, data):\n",
    "            # Ensure data is converted to numeric type to avoid object type errors\n",
    "            self.data = torch.tensor(data.astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx]\n",
    "    \n",
    "    return CustomDataset(X)\n",
    "\n",
    "# Define Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# Define Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# GAN Imputation Function with Multiple Passes and Early Stopping\n",
    "def impute_missing_values_with_multiple_passes(X, num_passes=3, epochs=1000, batch_size=64, learning_rate=0.0002, patience=10):\n",
    "    # Prepare data and mask for missing values\n",
    "    X_imputed = X.copy()\n",
    "    mask = X_imputed.isna()\n",
    "    X_imputed.fillna(0, inplace=True)\n",
    "    \n",
    "    # Input dimension\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    # Multiple passes\n",
    "    for pass_num in range(num_passes):\n",
    "        print(f\"\\n--- Imputation Pass {pass_num + 1}/{num_passes} ---\\n\")\n",
    "        \n",
    "        # Prepare dataset and dataloader\n",
    "        dataset = prepare_dataset(X_imputed.values)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Initialize Generator and Discriminator\n",
    "        generator = Generator(input_dim)\n",
    "        discriminator = Discriminator(input_dim)\n",
    "\n",
    "        # Optimizers\n",
    "        optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "        optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Loss function\n",
    "        adversarial_loss = nn.BCELoss()\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i, real_data in enumerate(dataloader):\n",
    "                # Adversarial ground truths\n",
    "                valid = torch.ones(real_data.size(0), 1)\n",
    "                fake = torch.zeros(real_data.size(0), 1)\n",
    "\n",
    "                # Train Generator\n",
    "                optimizer_G.zero_grad()\n",
    "\n",
    "                # Generate data and replace missing values with generated data\n",
    "                gen_data = generator(real_data)\n",
    "                gen_data[mask.iloc[i * batch_size:(i + 1) * batch_size].values] = real_data[mask.iloc[i * batch_size:(i + 1) * batch_size].values]\n",
    "\n",
    "                # Generator loss\n",
    "                g_loss = adversarial_loss(discriminator(gen_data), valid)\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "                # Train Discriminator\n",
    "                optimizer_D.zero_grad()\n",
    "\n",
    "                # Real and fake losses\n",
    "                real_loss = adversarial_loss(discriminator(real_data), valid)\n",
    "                fake_loss = adversarial_loss(discriminator(gen_data.detach()), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "            # Display progress\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} - Generator Loss: {g_loss.item()}, Discriminator Loss: {d_loss.item()}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if g_loss.item() < best_loss:\n",
    "                best_loss = g_loss.item()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} - Generator Loss: {g_loss.item()}\")\n",
    "                break\n",
    "\n",
    "        # Update X_imputed with refined values\n",
    "        X_imputed_tensor = torch.tensor(X_imputed.values.astype(np.float32), dtype=torch.float32)\n",
    "        refined_data = generator(X_imputed_tensor).detach().numpy()\n",
    "        X_imputed.values[mask.values] = refined_data[mask.values]  # Only update missing values\n",
    "\n",
    "    return pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# Impute missing values for each dataset separately\n",
    "X_external_imputed = impute_missing_values_with_multiple_passes(X_external)\n",
    "X_train_imputed = impute_missing_values_with_multiple_passes(X_train)\n",
    "X_validate_imputed = impute_missing_values_with_multiple_passes(X_validate)\n",
    "X_test_imputed = impute_missing_values_with_multiple_passes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237bd1df-d699-4152-81e3-3e23324a43df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
