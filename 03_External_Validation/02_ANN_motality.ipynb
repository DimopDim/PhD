{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a44f2a-70cb-4d42-9a04-f3a5ceb9f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c075197-459e-4ec9-b20d-ede5421344d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MIMICs CSV file\n",
    "mimic_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\mimic_mean_final.csv\")\n",
    "\n",
    "# Read eICUs CSV file\n",
    "eicu_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\eicu_mean_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ef9f52-e355-471e-bd72-9731f9739f06",
   "metadata": {},
   "source": [
    "# Define Categorical - Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a485ffdd-da80-4859-8805-88a96e0d8b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define exclude columns\n",
    "exclude_columns = ['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone']\n",
    "\n",
    "# Separating categorical and numerical columns for MIMIC\n",
    "mimic_categorical_columns = mimic_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "mimic_numerical_columns = mimic_df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "for col in exclude_columns:\n",
    "    if col in mimic_numerical_columns:\n",
    "        mimic_numerical_columns.remove(col)\n",
    "    if col in mimic_categorical_columns:\n",
    "        mimic_categorical_columns.remove(col)\n",
    "\n",
    "# Separating categorical and numerical columns for eICU\n",
    "eicu_categorical_columns = eicu_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "eicu_numerical_columns = eicu_df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "for col in exclude_columns:\n",
    "    if col in eicu_numerical_columns:\n",
    "        eicu_numerical_columns.remove(col)\n",
    "    if col in eicu_categorical_columns:\n",
    "        eicu_categorical_columns.remove(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc17f01-819d-41e9-b4ee-9574b37235fb",
   "metadata": {},
   "source": [
    "# Separate Training - Validate - Test - External"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ee6a54-1e7d-4a01-8b3f-10b180b6e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by `subject_id` and `hadm_id` to get unique patient admission records\n",
    "unique_patients = mimic_df[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "\n",
    "# Split the unique patients into train, validation, and test sets\n",
    "train_patients, test_patients = train_test_split(unique_patients, test_size=0.10, random_state=42)\n",
    "train_patients, validate_patients = train_test_split(train_patients, test_size=0.11, random_state=42)  # 0.11 * 90% ~= 10%\n",
    "\n",
    "# Merge the patients back with the original data to get the full records\n",
    "train_set = mimic_df.merge(train_patients, on=['subject_id', 'hadm_id'])\n",
    "validate_set = mimic_df.merge(validate_patients, on=['subject_id', 'hadm_id'])\n",
    "test_set = mimic_df.merge(test_patients, on=['subject_id', 'hadm_id'])\n",
    "\n",
    "# External validation from eICU\n",
    "X_external = eicu_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "y_external = eicu_df['hospital_expire_flag']\n",
    "\n",
    "# Separate features and target for the training, validation, and test sets\n",
    "X_train = train_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "y_train = train_set['hospital_expire_flag']\n",
    "\n",
    "X_validate = validate_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "y_validate = validate_set['hospital_expire_flag']\n",
    "\n",
    "X_test = test_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "y_test = test_set['hospital_expire_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcc3fc-c89d-4c39-a81e-b8f76d3cfe3b",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d1cace-f559-4fa0-bf32-6c3fbafaf814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #('num', StandardScaler(), mimic_numerical_columns),   # Standardize numerical features\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), mimic_categorical_columns)   # One-hot encode categorical features\n",
    "    ])\n",
    "\n",
    "# Fit and transform the training set\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformations to validation, test, and external sets\n",
    "X_validate_preprocessed = preprocessor.transform(X_validate)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "X_external_preprocessed = preprocessor.transform(X_external)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7c286-c505-4dd8-be65-6d99e49ee8a2",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5423a6d3-05ae-489c-8cc3-db0fd392c1d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Build the ANN model\n",
    "def build_ann(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(input_dim,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    return model\n",
    "\n",
    "def compile_ann(model):\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Get the number of input features after preprocessing\n",
    "input_dim = X_train_preprocessed.shape[1]  # This is the number of features post-encoding\n",
    "\n",
    "# Build and compile the ANN\n",
    "model = build_ann(input_dim)\n",
    "model = compile_ann(model)\n",
    "\n",
    "# Train the model using early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_preprocessed, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_validate_preprocessed, y_validate),\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_preprocessed, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "# External validation on the eICU dataset\n",
    "external_loss, external_accuracy = model.evaluate(X_external_preprocessed, y_external)\n",
    "print(f\"External Validation Accuracy: {external_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a4610f-7fef-4c3a-9c4e-da9bb8da68e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7965 - loss: 0.5065 - val_accuracy: 0.7890 - val_loss: 0.5045\n",
      "Epoch 2/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7981 - loss: 0.4832 - val_accuracy: 0.7890 - val_loss: 0.5116\n",
      "Epoch 3/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7992 - loss: 0.4816 - val_accuracy: 0.7890 - val_loss: 0.5155\n",
      "Epoch 4/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7990 - loss: 0.4798 - val_accuracy: 0.7890 - val_loss: 0.5174\n",
      "Epoch 5/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7975 - loss: 0.4818 - val_accuracy: 0.7890 - val_loss: 0.5209\n",
      "Epoch 6/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8009 - loss: 0.4760 - val_accuracy: 0.7890 - val_loss: 0.5207\n",
      "Epoch 7/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8001 - loss: 0.4763 - val_accuracy: 0.7890 - val_loss: 0.5286\n",
      "Epoch 8/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7989 - loss: 0.4806 - val_accuracy: 0.7890 - val_loss: 0.5357\n",
      "Epoch 9/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7980 - loss: 0.4808 - val_accuracy: 0.7890 - val_loss: 0.5373\n",
      "Epoch 10/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7945 - loss: 0.4843 - val_accuracy: 0.7890 - val_loss: 0.5423\n",
      "Epoch 11/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7987 - loss: 0.4778 - val_accuracy: 0.7890 - val_loss: 0.5582\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8181 - loss: 0.4895\n",
      "Test Accuracy: 79.66%\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\n",
      "Test Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Survive       0.80      1.00      0.89      4448\n",
      " Not Survive       0.00      0.00      0.00      1136\n",
      "\n",
      "    accuracy                           0.80      5584\n",
      "   macro avg       0.40      0.50      0.44      5584\n",
      "weighted avg       0.63      0.80      0.71      5584\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4448    0]\n",
      " [1136    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2696/2696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9420 - loss: 0.3141\n",
      "External Validation Accuracy: 93.43%\n",
      "\u001b[1m2696/2696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "\n",
      "External Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Survive       0.93      1.00      0.97     80608\n",
      " Not Survive       0.00      0.00      0.00      5664\n",
      "\n",
      "    accuracy                           0.93     86272\n",
      "   macro avg       0.47      0.50      0.48     86272\n",
      "weighted avg       0.87      0.93      0.90     86272\n",
      "\n",
      "Confusion Matrix:\n",
      " [[80608     0]\n",
      " [ 5664     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "def build_ann(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(input_dim,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    return model\n",
    "\n",
    "def compile_ann(model):\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Get the number of input features after preprocessing\n",
    "input_dim = X_train_preprocessed.shape[1]  # This is the number of features post-encoding\n",
    "\n",
    "# Build and compile the ANN\n",
    "model = build_ann(input_dim)\n",
    "model = compile_ann(model)\n",
    "\n",
    "# Train the model using early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_preprocessed, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_validate_preprocessed, y_validate),\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_preprocessed, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_prob = model.predict(X_test_preprocessed).ravel()\n",
    "y_test_pred = (y_test_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate precision, recall, F1 score, and confusion matrix\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Survive', 'Not Survive']))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# External validation on the eICU dataset\n",
    "external_loss, external_accuracy = model.evaluate(X_external_preprocessed, y_external)\n",
    "print(f\"External Validation Accuracy: {external_accuracy*100:.2f}%\")\n",
    "\n",
    "# Predict on the external validation set\n",
    "y_external_prob = model.predict(X_external_preprocessed).ravel()\n",
    "y_external_pred = (y_external_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate precision, recall, F1 score, and confusion matrix for external validation\n",
    "print(\"\\nExternal Validation Metrics:\")\n",
    "print(classification_report(y_external, y_external_pred, target_names=['Survive', 'Not Survive']))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_external, y_external_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8c6c4-6115-415a-9368-9791c40030c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
