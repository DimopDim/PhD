{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76222dd9-97e3-4d6b-9fe8-2ebc10d94290",
   "metadata": {},
   "source": [
    "# ANN Mortality\n",
    "## Transform Categorical to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a44f2a-70cb-4d42-9a04-f3a5ceb9f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c075197-459e-4ec9-b20d-ede5421344d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MIMICs CSV file\n",
    "mimic_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\mimic_mean_final.csv\")\n",
    "\n",
    "# Read eICUs CSV file\n",
    "eicu_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\eicu_mean_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ef9f52-e355-471e-bd72-9731f9739f06",
   "metadata": {},
   "source": [
    "# Define Categorical - Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a485ffdd-da80-4859-8805-88a96e0d8b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define exclude columns\n",
    "exclude_columns = ['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone']\n",
    "\n",
    "# Separating categorical and numerical columns for MIMIC\n",
    "mimic_categorical_columns = mimic_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "mimic_numerical_columns = mimic_df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "for col in exclude_columns:\n",
    "    if col in mimic_numerical_columns:\n",
    "        mimic_numerical_columns.remove(col)\n",
    "    if col in mimic_categorical_columns:\n",
    "        mimic_categorical_columns.remove(col)\n",
    "\n",
    "# Separating categorical and numerical columns for eICU\n",
    "eicu_categorical_columns = eicu_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "eicu_numerical_columns = eicu_df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "for col in exclude_columns:\n",
    "    if col in eicu_numerical_columns:\n",
    "        eicu_numerical_columns.remove(col)\n",
    "    if col in eicu_categorical_columns:\n",
    "        eicu_categorical_columns.remove(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc17f01-819d-41e9-b4ee-9574b37235fb",
   "metadata": {},
   "source": [
    "# Separate Training - Validate - Test - External"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ee6a54-1e7d-4a01-8b3f-10b180b6e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by `subject_id` and `hadm_id` to get unique patient admission records\n",
    "unique_patients = mimic_df[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "\n",
    "# Split the unique patients into train, validation, and test sets\n",
    "train_patients, test_patients = train_test_split(unique_patients, test_size=0.10, random_state=42)\n",
    "train_patients, validate_patients = train_test_split(train_patients, test_size=0.11, random_state=42)  # 0.11 * 90% ~= 10%\n",
    "\n",
    "# Merge the patients back with the original data to get the full records\n",
    "train_set = mimic_df.merge(train_patients, on=['subject_id', 'hadm_id'])\n",
    "validate_set = mimic_df.merge(validate_patients, on=['subject_id', 'hadm_id'])\n",
    "test_set = mimic_df.merge(test_patients, on=['subject_id', 'hadm_id'])\n",
    "\n",
    "# External validation from eICU\n",
    "X_external = eicu_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "y_external = eicu_df['hospital_expire_flag']\n",
    "\n",
    "# Separate features and target for the training, validation, and test sets\n",
    "X_train = train_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "y_train = train_set['hospital_expire_flag']\n",
    "\n",
    "X_validate = validate_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "y_validate = validate_set['hospital_expire_flag']\n",
    "\n",
    "X_test = test_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "y_test = test_set['hospital_expire_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcc3fc-c89d-4c39-a81e-b8f76d3cfe3b",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d1cace-f559-4fa0-bf32-6c3fbafaf814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #('num', StandardScaler(), mimic_numerical_columns),   # Standardize numerical features\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), mimic_categorical_columns)   # One-hot encode categorical features\n",
    "    ])\n",
    "\n",
    "# Fit and transform the training set\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformations to validation, test, and external sets\n",
    "X_validate_preprocessed = preprocessor.transform(X_validate)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "X_external_preprocessed = preprocessor.transform(X_external)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7c286-c505-4dd8-be65-6d99e49ee8a2",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a4610f-7fef-4c3a-9c4e-da9bb8da68e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7984 - loss: 0.5019 - val_accuracy: 0.7890 - val_loss: 0.5022\n",
      "Epoch 2/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7959 - loss: 0.4892 - val_accuracy: 0.7890 - val_loss: 0.5076\n",
      "Epoch 3/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7996 - loss: 0.4829 - val_accuracy: 0.7890 - val_loss: 0.5119\n",
      "Epoch 4/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7995 - loss: 0.4799 - val_accuracy: 0.7890 - val_loss: 0.5122\n",
      "Epoch 5/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8004 - loss: 0.4791 - val_accuracy: 0.7890 - val_loss: 0.5166\n",
      "Epoch 6/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8005 - loss: 0.4790 - val_accuracy: 0.7890 - val_loss: 0.5199\n",
      "Epoch 7/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8000 - loss: 0.4779 - val_accuracy: 0.7890 - val_loss: 0.5233\n",
      "Epoch 8/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7959 - loss: 0.4839 - val_accuracy: 0.7890 - val_loss: 0.5257\n",
      "Epoch 9/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7983 - loss: 0.4784 - val_accuracy: 0.7890 - val_loss: 0.5294\n",
      "Epoch 10/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7988 - loss: 0.4794 - val_accuracy: 0.7890 - val_loss: 0.5352\n",
      "Epoch 11/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7999 - loss: 0.4779 - val_accuracy: 0.7890 - val_loss: 0.5321\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8181 - loss: 0.4865\n",
      "Test Accuracy: 79.66%\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\n",
      "Test Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Survive       0.80      1.00      0.89      4448\n",
      " Not Survive       0.00      0.00      0.00      1136\n",
      "\n",
      "    accuracy                           0.80      5584\n",
      "   macro avg       0.40      0.50      0.44      5584\n",
      "weighted avg       0.63      0.80      0.71      5584\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4448    0]\n",
      " [1136    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2696/2696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9420 - loss: 0.3065\n",
      "External Validation Accuracy: 93.43%\n",
      "\u001b[1m2696/2696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "\n",
      "External Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Survive       0.93      1.00      0.97     80608\n",
      " Not Survive       0.00      0.00      0.00      5664\n",
      "\n",
      "    accuracy                           0.93     86272\n",
      "   macro avg       0.47      0.50      0.48     86272\n",
      "weighted avg       0.87      0.93      0.90     86272\n",
      "\n",
      "Confusion Matrix:\n",
      " [[80608     0]\n",
      " [ 5664     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "def build_ann(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(input_dim,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    return model\n",
    "\n",
    "def compile_ann(model):\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Get the number of input features after preprocessing\n",
    "input_dim = X_train_preprocessed.shape[1]  # This is the number of features post-encoding\n",
    "\n",
    "# Build and compile the ANN\n",
    "model = build_ann(input_dim)\n",
    "model = compile_ann(model)\n",
    "\n",
    "# Train the model using early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_preprocessed, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_validate_preprocessed, y_validate),\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_preprocessed, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_prob = model.predict(X_test_preprocessed).ravel()\n",
    "y_test_pred = (y_test_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate precision, recall, F1 score, and confusion matrix\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Survive', 'Not Survive']))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# External validation on the eICU dataset\n",
    "external_loss, external_accuracy = model.evaluate(X_external_preprocessed, y_external)\n",
    "print(f\"External Validation Accuracy: {external_accuracy*100:.2f}%\")\n",
    "\n",
    "# Predict on the external validation set\n",
    "y_external_prob = model.predict(X_external_preprocessed).ravel()\n",
    "y_external_pred = (y_external_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate precision, recall, F1 score, and confusion matrix for external validation\n",
    "print(\"\\nExternal Validation Metrics:\")\n",
    "print(classification_report(y_external, y_external_pred, target_names=['Survive', 'Not Survive']))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_external, y_external_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f20e15-65ff-40f6-afc5-31532063bb7c",
   "metadata": {},
   "source": [
    "# Another Approch\n",
    "## Transform Categorical to Features.\n",
    "## I have concatenate the mimic and eicu, transform the categorical to features and separate them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b17f0d-dc01-45f4-8364-bff5175af4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae8d241d-5cd6-4d62-88cb-5af8a0a1c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MIMICs CSV file\n",
    "mimic_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\mimic_mean_final.csv\")\n",
    "\n",
    "# Read eICUs CSV file\n",
    "eicu_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\eicu_mean_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be3234a7-4dae-4507-abd4-75eaeac27b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframes\n",
    "df_combined = pd.concat([mimic_df, eicu_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69abccc4-8931-438d-8a51-1f625603295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all categorical columns in mimic\n",
    "categorical_columns = df_combined.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Apply one-hot encoding to all categorical columns\n",
    "df_encoded = pd.get_dummies(df_combined, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc0c6e5c-a12e-4c5f-9454-c8a184757020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the concatenate dataframe\n",
    "mimic_df = df_encoded.iloc[:55792, :]  # Rows from 0 to 55791\n",
    "eicu_df = df_encoded.iloc[55792:, :]  # Rows from 55792 to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a52d2ee-44c2-4a05-a34d-ed022bcd00fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by `subject_id` and `hadm_id` to get unique patient admission records\n",
    "unique_patients = mimic_df[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "\n",
    "# Split the unique patients into train, validation, and test sets\n",
    "train_patients, test_patients = train_test_split(unique_patients, test_size=0.10, random_state=42)\n",
    "train_patients, validate_patients = train_test_split(train_patients, test_size=0.11, random_state=42)  # 0.11 * 90% ~= 10%\n",
    "\n",
    "# Merge the patients back with the original data to get the full records\n",
    "train_set = mimic_df.merge(train_patients, on=['subject_id', 'hadm_id'])\n",
    "validate_set = mimic_df.merge(validate_patients, on=['subject_id', 'hadm_id'])\n",
    "test_set = mimic_df.merge(test_patients, on=['subject_id', 'hadm_id'])\n",
    "\n",
    "# External validation from eICU\n",
    "X_external = eicu_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "y_external = eicu_df['hospital_expire_flag']\n",
    "\n",
    "# Separate features and target for the training, validation, and test sets\n",
    "X_train = train_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "y_train = train_set['hospital_expire_flag']\n",
    "\n",
    "X_validate = validate_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "y_validate = validate_set['hospital_expire_flag']\n",
    "\n",
    "X_test = test_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count'])\n",
    "y_test = test_set['hospital_expire_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "953eb7ee-1657-4be5-9e16-904f81e4b582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8037 - loss: 0.6226 - val_accuracy: 0.7890 - val_loss: 0.5270\n",
      "Epoch 2/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7994 - loss: 0.5096 - val_accuracy: 0.7890 - val_loss: 0.5153\n",
      "Epoch 3/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7992 - loss: 0.5017 - val_accuracy: 0.7890 - val_loss: 0.5154\n",
      "Epoch 4/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7987 - loss: 0.5022 - val_accuracy: 0.7890 - val_loss: 0.5155\n",
      "Epoch 5/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8008 - loss: 0.4993 - val_accuracy: 0.7890 - val_loss: 0.5154\n",
      "Epoch 6/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7972 - loss: 0.5043 - val_accuracy: 0.7890 - val_loss: 0.5156\n",
      "Epoch 7/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8011 - loss: 0.4989 - val_accuracy: 0.7890 - val_loss: 0.5154\n",
      "Epoch 8/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7991 - loss: 0.5017 - val_accuracy: 0.7890 - val_loss: 0.5157\n",
      "Epoch 9/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8003 - loss: 0.5001 - val_accuracy: 0.7890 - val_loss: 0.5156\n",
      "Epoch 10/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7961 - loss: 0.5058 - val_accuracy: 0.7890 - val_loss: 0.5155\n",
      "Epoch 11/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7984 - loss: 0.5027 - val_accuracy: 0.7890 - val_loss: 0.5156\n",
      "Epoch 12/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8018 - loss: 0.4980 - val_accuracy: 0.7890 - val_loss: 0.5154\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8181 - loss: 0.4775\n",
      "Test Accuracy: 79.66%\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "Test Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Survive       0.80      1.00      0.89      4448\n",
      " Not Survive       0.00      0.00      0.00      1136\n",
      "\n",
      "    accuracy                           0.80      5584\n",
      "   macro avg       0.40      0.50      0.44      5584\n",
      "weighted avg       0.63      0.80      0.71      5584\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4448    0]\n",
      " [1136    0]]\n",
      "\u001b[1m   1/2696\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.2407"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2696/2696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9420 - loss: 0.3163\n",
      "External Validation Accuracy: 93.43%\n",
      "\u001b[1m2696/2696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "\n",
      "External Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Survive       0.93      1.00      0.97     80608\n",
      " Not Survive       0.00      0.00      0.00      5664\n",
      "\n",
      "    accuracy                           0.93     86272\n",
      "   macro avg       0.47      0.50      0.48     86272\n",
      "weighted avg       0.87      0.93      0.90     86272\n",
      "\n",
      "Confusion Matrix:\n",
      " [[80608     0]\n",
      " [ 5664     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "def build_ann(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(input_dim,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    return model\n",
    "\n",
    "def compile_ann(model):\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Get the number of input features after preprocessing\n",
    "input_dim = X_train.shape[1]  # Number of features after one-hot encoding\n",
    "\n",
    "# Build and compile the ANN\n",
    "model = build_ann(input_dim)\n",
    "model = compile_ann(model)\n",
    "\n",
    "# Train the model using early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_validate, y_validate),\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_prob = model.predict(X_test).ravel()\n",
    "y_test_pred = (y_test_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate precision, recall, F1 score, and confusion matrix\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Survive', 'Not Survive']))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# External validation on the eICU dataset\n",
    "external_loss, external_accuracy = model.evaluate(X_external, y_external)\n",
    "print(f\"External Validation Accuracy: {external_accuracy*100:.2f}%\")\n",
    "\n",
    "# Predict on the external validation set\n",
    "y_external_prob = model.predict(X_external).ravel()\n",
    "y_external_pred = (y_external_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate precision, recall, F1 score, and confusion matrix for external validation\n",
    "print(\"\\nExternal Validation Metrics:\")\n",
    "print(classification_report(y_external, y_external_pred, target_names=['Survive', 'Not Survive']))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_external, y_external_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad6a01-21a6-4959-bb84-ae3229e27eb8",
   "metadata": {},
   "source": [
    "# Lets Scale the numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aadeb86-82d8-4b90-ade1-de8879c35f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8006 - loss: 0.6219 - val_accuracy: 0.7890 - val_loss: 0.5263\n",
      "Epoch 2/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7974 - loss: 0.5115 - val_accuracy: 0.7890 - val_loss: 0.5153\n",
      "Epoch 3/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7977 - loss: 0.5038 - val_accuracy: 0.7890 - val_loss: 0.5157\n",
      "Epoch 4/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7974 - loss: 0.5041 - val_accuracy: 0.7890 - val_loss: 0.5156\n",
      "Epoch 5/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8002 - loss: 0.5002 - val_accuracy: 0.7890 - val_loss: 0.5155\n",
      "Epoch 6/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7964 - loss: 0.5053 - val_accuracy: 0.7890 - val_loss: 0.5155\n",
      "Epoch 7/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8007 - loss: 0.4994 - val_accuracy: 0.7890 - val_loss: 0.5154\n",
      "Epoch 8/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7993 - loss: 0.5014 - val_accuracy: 0.7890 - val_loss: 0.5156\n",
      "Epoch 9/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8025 - loss: 0.4970 - val_accuracy: 0.7890 - val_loss: 0.5155\n",
      "Epoch 10/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7991 - loss: 0.5017 - val_accuracy: 0.7890 - val_loss: 0.5156\n",
      "Epoch 11/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7969 - loss: 0.5047 - val_accuracy: 0.7890 - val_loss: 0.5156\n",
      "Epoch 12/100\n",
      "\u001b[1m1396/1396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7977 - loss: 0.5037 - val_accuracy: 0.7890 - val_loss: 0.5157\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8181 - loss: 0.4770\n",
      "Test Accuracy: 79.66%\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "Test Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Survive       0.80      1.00      0.89      4448\n",
      " Not Survive       0.00      0.00      0.00      1136\n",
      "\n",
      "    accuracy                           0.80      5584\n",
      "   macro avg       0.40      0.50      0.44      5584\n",
      "weighted avg       0.63      0.80      0.71      5584\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4448    0]\n",
      " [1136    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2696/2696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9420 - loss: 0.3137\n",
      "External Validation Accuracy: 93.43%\n",
      "\u001b[1m2696/2696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "\n",
      "External Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Survive       0.93      1.00      0.97     80608\n",
      " Not Survive       0.00      0.00      0.00      5664\n",
      "\n",
      "    accuracy                           0.93     86272\n",
      "   macro avg       0.47      0.50      0.48     86272\n",
      "weighted avg       0.87      0.93      0.90     86272\n",
      "\n",
      "Confusion Matrix:\n",
      " [[80608     0]\n",
      " [ 5664     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_columns = X_train.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the scaler on the training data\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])\n",
    "\n",
    "# Transform the validation and test data\n",
    "X_validate_scaled = X_validate.copy()\n",
    "X_validate_scaled[numeric_columns] = scaler.transform(X_validate[numeric_columns])\n",
    "\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[numeric_columns] = scaler.transform(X_test[numeric_columns])\n",
    "\n",
    "# Transform the external validation set\n",
    "X_external_scaled = X_external.copy()\n",
    "X_external_scaled[numeric_columns] = scaler.transform(X_external[numeric_columns])\n",
    "\n",
    "# Get the number of input features after preprocessing\n",
    "input_dim = X_train_scaled.shape[1]  # Number of features after scaling and encoding\n",
    "\n",
    "# Build and compile the ANN\n",
    "model = build_ann(input_dim)\n",
    "model = compile_ann(model)\n",
    "\n",
    "# Train the model using early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_validate_scaled, y_validate),\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_prob = model.predict(X_test_scaled).ravel()\n",
    "y_test_pred = (y_test_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate precision, recall, F1 score, and confusion matrix\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Survive', 'Not Survive']))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# External validation on the eICU dataset\n",
    "external_loss, external_accuracy = model.evaluate(X_external_scaled, y_external)\n",
    "print(f\"External Validation Accuracy: {external_accuracy*100:.2f}%\")\n",
    "\n",
    "# Predict on the external validation set\n",
    "y_external_prob = model.predict(X_external_scaled).ravel()\n",
    "y_external_pred = (y_external_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate precision, recall, F1 score, and confusion matrix for external validation\n",
    "print(\"\\nExternal Validation Metrics:\")\n",
    "print(classification_report(y_external, y_external_pred, target_names=['Survive', 'Not Survive']))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_external, y_external_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c051cd48-e020-45cc-a8e7-d0891ea99471",
   "metadata": {},
   "source": [
    "# Checking how many survivers and non are in the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb58a3d1-9f15-417f-b124-cb12e1725886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of survivors in the training set: 35680\n",
      "Number of non-survivors in the training set: 8992\n",
      "\n",
      "Number of survivors in the test set: 4448\n",
      "Number of non-survivors in the test set: 1136\n",
      "\n",
      "Number of survivors in the validation set: 4368\n",
      "Number of non-survivors in the validation set: 1168\n",
      "\n",
      "Number of survivors in the external validation set: 80608\n",
      "Number of non-survivors in the external validation set: 5664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count in the training set\n",
    "survivors_train = y_train.value_counts().get(0, 0)\n",
    "print(f\"Number of survivors in the training set: {survivors_train}\")\n",
    "\n",
    "non_survivors_train = y_train.value_counts().get(1, 0)\n",
    "print(f\"Number of non-survivors in the training set: {non_survivors_train}\\n\")\n",
    "\n",
    "\n",
    "# Count non-survivors in the test set\n",
    "survivors_test = y_test.value_counts().get(0, 0)\n",
    "print(f\"Number of survivors in the test set: {survivors_test}\")\n",
    "\n",
    "non_survivors_test = y_test.value_counts().get(1, 0)\n",
    "print(f\"Number of non-survivors in the test set: {non_survivors_test}\\n\")\n",
    "\n",
    "\n",
    "# Count in the validation set\n",
    "survivors_validate = y_validate.value_counts().get(0, 0)\n",
    "print(f\"Number of survivors in the validation set: {survivors_validate}\")\n",
    "\n",
    "non_survivors_validate = y_validate.value_counts().get(1, 0)\n",
    "print(f\"Number of non-survivors in the validation set: {non_survivors_validate}\\n\")\n",
    "\n",
    "\n",
    "# Count in the external validation set\n",
    "survivors_external = y_external.value_counts().get(0, 0)\n",
    "print(f\"Number of survivors in the external validation set: {survivors_external}\")\n",
    "\n",
    "non_survivors_external = y_external.value_counts().get(1, 0)\n",
    "print(f\"Number of non-survivors in the external validation set: {non_survivors_external}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0baf194-3448-4ed7-9201-ac6df3c4d690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
