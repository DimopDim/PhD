{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6243b02-165d-496a-a198-f00704a71eb6",
   "metadata": {},
   "source": [
    "# Imports | Reads | Filter Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6671463d-f9c0-4542-b28c-eec0dfb36e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterGrid, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error, mean_squared_log_error, r2_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a182eba9-001e-43e2-b928-7bdd96cf667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MIMICs CSV file\n",
    "mimic_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\mimic_mean_final.csv\")\n",
    "\n",
    "# Read eICUs CSV file\n",
    "eicu_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\eicu_mean_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abae581b-694b-43c7-9805-90856381db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 10\n",
    "\n",
    "# Filter icu stay less than 10 days\n",
    "mimic_df = mimic_df[mimic_df['los'] < day]\n",
    "\n",
    "# Filter icu stay less than 10 days\n",
    "eicu_df = eicu_df[eicu_df['los'] < day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bc6cae-2821-43c1-a6b1-04c79b38273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Time Zone\n",
    "\n",
    "#time_zone = 16\n",
    "#mimic_df = mimic_df[mimic_df['Time_Zone'] == time_zone]\n",
    "#eicu_df = eicu_df[eicu_df['Time_Zone'] == time_zone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "894f6245-e142-45de-971e-fb52f9a1ffda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 48992\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I'm gonna concat and split the mimic and icu\n",
    "at this point. I must create the same columns\n",
    "from the tranformation of categorical data.\n",
    "\"\"\"\n",
    "row_count = mimic_df.shape[0]\n",
    "print(f\"Row count: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9462d7a8-d964-43b5-9e61-6064e02f0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dataframes\n",
    "df_combined = pd.concat([mimic_df, eicu_df], ignore_index=True)\n",
    "\n",
    "# Find all categorical columns in mimic\n",
    "categorical_columns = df_combined.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Apply one-hot encoding to all categorical columns\n",
    "df_encoded = pd.get_dummies(df_combined, columns=categorical_columns)\n",
    "\n",
    "# Split the concatenate dataframe\n",
    "mimic_df = df_encoded.iloc[:row_count, :]\n",
    "eicu_df = df_encoded.iloc[row_count:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1786ba0-60a9-4593-87be-689ee23fffe2",
   "metadata": {},
   "source": [
    "# Split Training - Validation - Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c308615a-2615-468c-a99f-a0b2bf265697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 39184\n",
      "Validation set size: 4896\n",
      "Test set size: 4912\n"
     ]
    }
   ],
   "source": [
    "total_test_val_perc = 0.2\n",
    "split_between_test_val_perc = 0.5\n",
    "\n",
    "# Group data by subject_id and hadm_id\n",
    "grouped_df = mimic_df.groupby(['subject_id', 'hadm_id'])\n",
    "\n",
    "# Get a new dataframe with one row per patient (subject_id, hadm_id) pair\n",
    "patient_df = grouped_df['hospital_expire_flag'].first().reset_index()\n",
    "\n",
    "# Split the patient_df into training (80%), validation (10%), and test (10%) while keeping the ratio of hospital_expired_flag\n",
    "train, temp = train_test_split(patient_df, test_size=total_test_val_perc, stratify=patient_df['hospital_expire_flag'], random_state=42)\n",
    "val, test = train_test_split(temp, test_size=split_between_test_val_perc, stratify=temp['hospital_expire_flag'], random_state=42)\n",
    "\n",
    "# Step 4: Merge back with the original df to get the rows for each patient in the splits\n",
    "train_df = mimic_df.merge(train[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "val_df = mimic_df.merge(val[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "test_df = mimic_df.merge(test[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "\n",
    "# Check the sizes of the splits\n",
    "print(f'Training set size: {train_df.shape[0]}')\n",
    "print(f'Validation set size: {val_df.shape[0]}')\n",
    "print(f'Test set size: {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47462f19-6295-4cb1-a8f7-64b8bd104609",
   "metadata": {},
   "source": [
    "# Check ratio and unique patients between sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "729ed3bc-b7ae-48fe-892f-66b6f814c5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "Survive: 1949.0\n",
      "Non-survive: 500.0\n",
      "Ratio Train Set: 3.90:1\n",
      "\n",
      "Validation Set\n",
      "Survive: 244.0\n",
      "Non-survive: 62.0\n",
      "Ratio Train Set: 3.94:1\n",
      "\n",
      "Test Set\n",
      "Survive: 244.0\n",
      "Non-survive: 63.0\n",
      "Ratio Train Set: 3.87:1\n"
     ]
    }
   ],
   "source": [
    "# Count on Training set survive and non-survive\n",
    "survival_counts = train_df['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/16\n",
    "temp_non_survive = survival_counts.get(1, 0)/16\n",
    "\n",
    "# Display the results\n",
    "print(f'Train Set')\n",
    "print(f'Survive: {temp_survive}')\n",
    "print(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "print(f'Ratio Train Set: {ratio:.2f}:1')\n",
    "\n",
    "\"\"\"----------------------------\"\"\"\n",
    "\n",
    "# Count on validation set survive and non-survive\n",
    "survival_counts = val_df['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/16\n",
    "temp_non_survive = survival_counts.get(1, 0)/16\n",
    "\n",
    "# Display the results\n",
    "print(f'\\nValidation Set')\n",
    "print(f'Survive: {temp_survive}')\n",
    "print(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "print(f'Ratio Train Set: {ratio:.2f}:1')\n",
    "\n",
    "\"\"\"----------------------------\"\"\"\n",
    "\n",
    "# Count on validation set survive and non-survive\n",
    "survival_counts = test_df['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/16\n",
    "temp_non_survive = survival_counts.get(1, 0)/16\n",
    "\n",
    "# Display the results\n",
    "print(f'\\nTest Set')\n",
    "print(f'Survive: {temp_survive}')\n",
    "print(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "print(f'Ratio Train Set: {ratio:.2f}:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73205561-9bcc-42fe-acc4-8bcd1616d9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between training and validation sets: 0\n",
      "Overlap between training and test sets: 0\n",
      "Overlap between validation and test sets: 0\n"
     ]
    }
   ],
   "source": [
    "# Mine unique subject_id from sets\n",
    "train_subjects = set(train_df['subject_id'].unique())\n",
    "val_subjects = set(val_df['subject_id'].unique())\n",
    "test_subjects = set(test_df['subject_id'].unique())\n",
    "\n",
    "# Check if there are overlaping subject_id\n",
    "train_val_overlap = train_subjects.intersection(val_subjects)\n",
    "train_test_overlap = train_subjects.intersection(test_subjects)\n",
    "val_test_overlap = val_subjects.intersection(test_subjects)\n",
    "\n",
    "# Display the results\n",
    "print(f'Overlap between training and validation sets: {len(train_val_overlap)}')\n",
    "print(f'Overlap between training and test sets: {len(train_test_overlap)}')\n",
    "print(f'Overlap between validation and test sets: {len(val_test_overlap)}')\n",
    "\n",
    "# print overlaping\n",
    "if train_val_overlap:\n",
    "    print(f'Subjects in both training and validation: {train_val_overlap}')\n",
    "if train_test_overlap:\n",
    "    print(f'Subjects in both training and test: {train_test_overlap}')\n",
    "if val_test_overlap:\n",
    "    print(f'Subjects in both validation and test: {val_test_overlap}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d473a75a-86eb-4dac-be9a-b73cf80b111b",
   "metadata": {},
   "source": [
    "# Split label from Train - Validation - Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee62e244-e475-4222-802b-57d374b63c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External validation from eICU\n",
    "X_external = eicu_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_external = eicu_df['los']\n",
    "\n",
    "# Separate features and target for the training, validation, and test sets\n",
    "X_train = train_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_train = train_df['los']\n",
    "\n",
    "X_validate = val_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_validate = val_df['los']\n",
    "\n",
    "X_test = test_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_test = test_df['los']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75bc50d-e5e2-44a3-9eb9-57a3adcdb72b",
   "metadata": {},
   "source": [
    "# Fill Missing Values by Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e36f677-2e54-47f6-ab8b-79c38660092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer object and fit it to the training data\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train_imputed, columns=X_train.columns)\n",
    "\n",
    "# Transform the validation and test sets using the same imputer\n",
    "X_validate_imputed = imputer.transform(X_validate)\n",
    "X_validate = pd.DataFrame(X_validate_imputed, columns=X_validate.columns)\n",
    "\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test_imputed, columns=X_test.columns)\n",
    "\n",
    "# Transform the external validation set using the same imputer\n",
    "X_external_imputed = imputer.transform(X_external)\n",
    "X_external = pd.DataFrame(X_external_imputed, columns=X_external.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab85f8-775d-4aaa-8338-ecb29e51acdb",
   "metadata": {},
   "source": [
    "# Scale and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a08718dd-5868-431f-b55d-f6bbd9468ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify binary categorical features\n",
    "def identify_binary_categorical_features(df):\n",
    "    binary_categorical_features = []\n",
    "    for col in df.columns:\n",
    "        unique_values = df[col].unique()\n",
    "        if set(unique_values) == {0.0, 1.0}:\n",
    "            binary_categorical_features.append(col)\n",
    "    return binary_categorical_features\n",
    "\n",
    "\n",
    "# Identify binary categorical features\n",
    "binary_categorical_features = identify_binary_categorical_features(X_train)\n",
    "\n",
    "# Exclude binary categorical features from scaling\n",
    "numerical_features = [col for col in X_train.columns if col not in binary_categorical_features]\n",
    "\n",
    "# Create scalers\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "# Fit and transform only numerical features\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[numerical_features] = standard_scaler.fit_transform(X_train[numerical_features])\n",
    "X_train = X_train_scaled\n",
    "\n",
    "# Optionally, apply Min-Max normalization\n",
    "#X_train_normalized = X_train_scaled.copy()\n",
    "#X_train_normalized[numerical_features] = minmax_scaler.fit_transform(X_train_scaled[numerical_features])\n",
    "#X_train = X_train_normalized\n",
    "\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "\n",
    "# Fit and transform only numerical features\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[numerical_features] = standard_scaler.fit_transform(X_test[numerical_features])\n",
    "X_test = X_test_scaled\n",
    "\n",
    "# Optionally, apply Min-Max normalization\n",
    "#X_test_normalized = X_test_scaled.copy()\n",
    "#X_test_normalized[numerical_features] = minmax_scaler.fit_transform(X_test_scaled[numerical_features])\n",
    "#X_test = X_test_normalized\n",
    "\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "\n",
    "# Fit and transform only numerical features\n",
    "X_validate_scaled = X_validate.copy()\n",
    "X_validate_scaled[numerical_features] = standard_scaler.fit_transform(X_validate[numerical_features])\n",
    "X_validate = X_validate_scaled\n",
    "\n",
    "# Optionally, apply Min-Max normalization\n",
    "#X_validate_normalized = X_validate_scaled.copy()\n",
    "#X_validate_normalized[numerical_features] = minmax_scaler.fit_transform(X_validate_scaled[numerical_features])\n",
    "#X_validate = X_validate_normalized\n",
    "\n",
    "\n",
    "\"\"\"---------------------\"\"\"\n",
    "\n",
    "# Fit and transform only numerical features\n",
    "X_external_scaled = X_external.copy()\n",
    "X_external_scaled[numerical_features] = standard_scaler.fit_transform(X_external[numerical_features])\n",
    "X_external = X_external_scaled\n",
    "\n",
    "# Optionally, apply Min-Max normalization\n",
    "#X_external_normalized = X_external_scaled.copy()\n",
    "#X_external_normalized[numerical_features] = minmax_scaler.fit_transform(X_external_scaled[numerical_features])\n",
    "#X_external = X_external_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f727d0-6369-4e0e-9d2f-ae447500734d",
   "metadata": {},
   "source": [
    "# Train Model with HP GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e5d12-f1bd-4242-9d64-35a14ba91996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36450 candidates, totalling 109350 fits\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A smaller learning rate makes the boosting\n",
    "process more robust and can lead to better\n",
    "generalization but requires more trees\n",
    "(higher n_estimators) to achieve the same result.\n",
    "A larger learning rate speeds up training bu\n",
    "may risk overfitting.\n",
    "\"\"\"\n",
    "\n",
    "# Define the parameter grid\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300], # controls the total number of trees in the ensemble\n",
    "    'learning_rate': np.arange(0.01, 1.02, 0.2),\n",
    "    'max_depth': np.arange(1, 10, 1),\n",
    "    'reg_lambda': np.arange(0.1, 15.1, 1),\n",
    "    'reg_alpha': np.arange(0.1, 15.1, 1)\n",
    "}\n",
    "\n",
    "# Create an XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', \n",
    "                           cv=3,  # Number of folds for cross-validation\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score (negative MSE):\", grid_search.best_score_)\n",
    "\n",
    "# Predict on the validation set with the best model\n",
    "y_pred_validate = grid_search.predict(X_validate)\n",
    "\n",
    "# Optionally: Evaluate the model on the validation set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_validate, y_pred_validate)\n",
    "print(\"Validation MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7116677-cd16-4f97-a843-b3eeb7738051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4d189-bba3-4b44-934a-37c2f5777439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6110f0b-10a3-44c5-a8d8-10709fa9550b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fill Empty Training Cells using ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb4682-9f02-4fae-9995-13f278d0bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill training set missing values\n",
    "\n",
    "# Step 1: Identify columns with missing values in X_train\n",
    "missing_columns = X_train.columns[X_train.isnull().any()].tolist()\n",
    "print(f\"Columns with missing values: {missing_columns}\")\n",
    "\n",
    "# Step 2: Loop through each column with missing values and build an ANN to predict missing values\n",
    "for col in missing_columns:\n",
    "    print(f\"Filling missing values in column: {col}\")\n",
    "    \n",
    "    # Separate rows with and without missing values in the current column\n",
    "    missing_rows = X_train[X_train[col].isnull()]\n",
    "    non_missing_rows = X_train[~X_train[col].isnull()]\n",
    "    \n",
    "    # Skip the column if no data is available for training\n",
    "    if len(missing_rows) == 0 or len(non_missing_rows) == 0:\n",
    "        print(f\"Skipping {col}, insufficient data\")\n",
    "        continue\n",
    "    \n",
    "    # Separate features and target for non-missing rows\n",
    "    X_train_missing = non_missing_rows.drop(columns=missing_columns)  # Exclude other missing columns from features\n",
    "    y_train_missing = non_missing_rows[col]  # Target is the column we're filling\n",
    "    \n",
    "    # Features for the rows with missing values (we'll predict the column for these rows)\n",
    "    X_test_missing = missing_rows.drop(columns=missing_columns)\n",
    "    \n",
    "    # Step 3: Preprocess the data (Standard Scaling)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_missing_scaled = scaler.fit_transform(X_train_missing)\n",
    "    X_test_missing_scaled = scaler.transform(X_test_missing)\n",
    "    \n",
    "    # Step 4: Build the ANN model for filling missing values\n",
    "    model_missing = Sequential()\n",
    "    model_missing.add(Input(shape=(X_train_missing_scaled.shape[1],)))  # Use Input layer instead of input_shape in Dense\n",
    "    model_missing.add(Dense(units=64, activation='relu'))\n",
    "    model_missing.add(Dropout(0.3))\n",
    "    model_missing.add(Dense(units=32, activation='relu'))\n",
    "    model_missing.add(Dropout(0.3))\n",
    "    model_missing.add(Dense(units=1, activation='linear'))  # Linear activation for regression tasks\n",
    "    \n",
    "    # Compile the model\n",
    "    model_missing.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Step 5: Train the model\n",
    "    model_missing.fit(X_train_missing_scaled, y_train_missing, epochs=50, batch_size=32, validation_split=0.1, verbose=0)\n",
    "    \n",
    "    # Step 6: Predict the missing values\n",
    "    predicted_values = model_missing.predict(X_test_missing_scaled)\n",
    "    \n",
    "    # Step 7: Fill the missing values in X_train\n",
    "    X_train.loc[X_train[col].isnull(), col] = predicted_values\n",
    "    \n",
    "    print(f\"Filled missing values in column: {col}\")\n",
    "\n",
    "# Verify if there are any remaining missing values in X_train\n",
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0d8b5-2e98-40ee-a51b-cd34e9c6cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display (X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec904fb7-507d-4d25-9b4c-279757ff2e3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fill Empty Training Cells by Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2c7bf5-e098-428a-a3c8-ad843e1e66c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3dbad-7aa9-4a1b-a95d-06d188a87551",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fill Empty Cells of Train - Validate - Test \"Model-Based Imputation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e396792-da55-48bc-95c4-2d4a8bbcee5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# List of columns with missing values\n",
    "missing_cols = X_train.columns[X_train.isna().any()].tolist()\n",
    "\n",
    "# Dictionary to hold imputation models\n",
    "imputation_models = {}\n",
    "\n",
    "# Step through each column with missing values\n",
    "for col in missing_cols:\n",
    "    # Separate the data with missing values\n",
    "    X_train_missing = X_train[X_train[col].isna()]\n",
    "    X_train_non_missing = X_train.dropna(subset=[col])\n",
    "    \n",
    "    # Prepare the data for training\n",
    "    X_train_non_missing_features = X_train_non_missing.drop(col, axis=1)\n",
    "    y_train_non_missing = X_train_non_missing[col]\n",
    "    \n",
    "    # Train a RandomForestRegressor model to predict missing values\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_non_missing_features, y_train_non_missing)\n",
    "    \n",
    "    # Predict missing values in the training set\n",
    "    X_train_missing_features = X_train_missing.drop(col, axis=1)\n",
    "    X_train.loc[X_train[col].isna(), col] = model.predict(X_train_missing_features)\n",
    "    \n",
    "    # Store the model for later use\n",
    "    imputation_models[col] = model\n",
    "\n",
    "# Apply the same imputation models to validation and test sets\n",
    "for col, model in imputation_models.items():\n",
    "    # Predict and fill missing values in validation and test sets\n",
    "    X_validate_missing = X_validate[X_validate[col].isna()]\n",
    "    X_validate_non_missing_features = X_validate.dropna(subset=[col]).drop(col, axis=1)\n",
    "    X_validate[col].loc[X_validate[col].isna()] = model.predict(X_validate_missing.drop(col, axis=1))\n",
    "    \n",
    "    X_test_missing = X_test[X_test[col].isna()]\n",
    "    X_test_non_missing_features = X_test.dropna(subset=[col]).drop(col, axis=1)\n",
    "    X_test[col].loc[X_test[col].isna()] = model.predict(X_test_missing.drop(col, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6707c-e12f-4da0-8e10-e5284c55150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display (X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493f917-45cb-478c-af02-6116cb50953d",
   "metadata": {},
   "source": [
    "# Train Model without HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec099179-7c37-4f77-9229-a2da9069d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default XGBoost Model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set (eICU data)\n",
    "y_pred_external = model.predict(X_external)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276416cb-20b3-432a-a01f-26e8a0194a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc2519-0d79-4c33-9738-1ff16d3e14f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb7bf9-73dd-46aa-a605-19b9051125dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d4d71-1373-48eb-9a69-a5c678372041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc091ad-470b-41aa-9f85-299b67820df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde0371-85d2-4601-8943-ff21bed5f776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8268bc-8455-4a27-a8f9-41f91d73497b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb150e-9f75-4004-b545-67144c93ed33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4856fe-8224-4bd3-a6ba-0b40cee22855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de097a08-e1d3-4b2d-8b49-ff8ec1e8c901",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train Model with HP RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9d4dd-5eed-4f18-99e7-582fdfd0c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_dist = {\n",
    "    'learning_rate': np.arange(0.01, 1.01, 0.1),\n",
    "    'max_depth': np.arange(1, 11, 1),\n",
    "    'min_child_weight': np.arange(1, 6, 1),\n",
    "    'reg_lambda': np.arange(0.1, 15.1, 1),\n",
    "    'reg_alpha': np.arange(0.1, 15.1, 1),\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Set up RandomizedSearchCV with tqdm integration\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, \n",
    "                                    param_distributions=param_dist, \n",
    "                                    n_iter=100,  # Number of random samples to try\n",
    "                                    scoring='neg_mean_squared_error',  # Use negative MSE for minimization\n",
    "                                    cv=2,  # Number of folds for cross-validation\n",
    "                                    n_jobs=-1,  # Use all available cores\n",
    "                                    verbose=1,  # Print progress\n",
    "                                    random_state=42)  # Set seed for reproducibility\n",
    "\n",
    "# Perform the RandomizedSearchCV\n",
    "with tqdm(total=100, desc=\"Hyperparameter Tuning\") as pbar:\n",
    "    random_search.fit(X_train, y_train)\n",
    "    pbar.update(100)\n",
    "\n",
    "# Retrieve the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "best_score = -random_search.best_score_  # Convert from negative MSE to positive MSE\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best validation MSE: {best_score}\")\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set with the best model\n",
    "y_pred_external = best_model.predict(X_external)\n",
    "\n",
    "# Evaluation on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "r2_test = r2_score(y_test, y_pred_test) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse_test}\")\n",
    "print(f\"Test Set RMSE: {rmse_test}\")\n",
    "print(f\"Test Set MAE: {mae_test}\")\n",
    "print(f\"Test Set R2 Score: {r2_test}\")\n",
    "\n",
    "# Evaluation on the external validation set\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "rmse_external = np.sqrt(mse_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "r2_external = r2_score(y_external, y_pred_external) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external}\")\n",
    "print(f\"External Validation Set MAE: {mae_external}\")\n",
    "print(f\"External Validation Set R2 Score: {r2_external}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40be9029-bb02-4cc2-8346-fd023e69f99b",
   "metadata": {},
   "source": [
    "# Train Model with HP gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca65fe49-cf5b-4254-b2a9-d3208485b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': np.arange(0.01, 1.01, 0.3),\n",
    "    'max_depth': np.arange(1, 11, 1),\n",
    "    'min_child_weight': np.arange(1, 6, 1),\n",
    "    'reg_lambda': np.arange(0.1, 15.1, 1),\n",
    "    'reg_alpha': np.arange(0.1, 15.1, 1),\n",
    "    'n_estimators': [100, 200, 300] \n",
    "}\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Set up GridSearchCV with tqdm integration\n",
    "grid_search = GridSearchCV(estimator=xgb_model, \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error',  # Use negative MSE for minimization\n",
    "                           cv=2,  # Number of folds for cross-validation\n",
    "                           n_jobs=-1,  # Use all available cores\n",
    "                           verbose=1)  # Disable default verbosity\n",
    "\n",
    "# Perform the GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best model and hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_  # Convert from negative MSE to positive MSE\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best validation MSE: {best_score}\")\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set with the best model\n",
    "y_pred_external = best_model.predict(X_external)\n",
    "\n",
    "# Evaluation on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "rmse_test = root_mean_squared_error(y_test, y_pred_test)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "r2_test = r2_score(y_test, y_pred_test) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse_test}\")\n",
    "print(f\"Test Set RMSE: {rmse_test}\")\n",
    "print(f\"Test Set MAE: {mae_test}\")\n",
    "print(f\"Test Set R2 Score: {r2_test}\")\n",
    "\n",
    "# Evaluation on the external validation set\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "rmse_external = root_mean_squared_error(y_external, y_pred_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "r2_external = r2_score(y_external, y_pred_external) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external}\")\n",
    "print(f\"External Validation Set MAE: {mae_external}\")\n",
    "print(f\"External Validation Set R2 Score: {r2_external}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddece62-f4c2-4e5c-897b-f0fa49d5a394",
   "metadata": {},
   "source": [
    "# Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a810c9c-58b5-41f7-b700-e62558004bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "save_path = 'CSV/exports/impute/all_features/o3_mean_scale_impute/'\n",
    "\n",
    "# Check if the directory exists, and if not, create it\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Save external validation set from eICU\n",
    "X_external.to_csv(save_path + 'X_external.csv', index=False)\n",
    "y_external.to_csv(save_path + 'y_external.csv', index=False)\n",
    "\n",
    "# Save training, validation, and test sets\n",
    "X_train.to_csv(save_path + 'X_train.csv', index=False)\n",
    "y_train.to_csv(save_path + 'y_train.csv', index=False)\n",
    "\n",
    "X_validate.to_csv(save_path + 'X_validate.csv', index=False)\n",
    "y_validate.to_csv(save_path + 'y_validate.csv', index=False)\n",
    "\n",
    "X_test.to_csv(save_path + 'X_test.csv', index=False)\n",
    "y_test.to_csv(save_path + 'y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1530e609-4239-4d84-88f6-48ed91bf5a28",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e03eb8-70ee-4578-bd93-2b7032252210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and file path\n",
    "\n",
    "name = 'o3_mean_impute_scale_default_xgboost.json'\n",
    "directory = 'models'\n",
    "\n",
    "file_path = os.path.join(directory, name)\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the model as a JSON file\n",
    "model.save_model(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c859dc-94ca-4033-a6cd-b91249f23693",
   "metadata": {},
   "source": [
    "# Test Set Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bfe86-fa9d-4eae-b671-86c3c19ba10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics calculation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse}\")\n",
    "print(f\"Test Set MAE: {mae}\")\n",
    "print(f\"Test Set RMSE: {rmse}\")\n",
    "print(f\"Test Set R2: {r2}\")\n",
    "\n",
    "# Plotting MSLE if applicable\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_pred)\n",
    "    print(f\"Test Set MSLE: {msle}\")\n",
    "    \n",
    "    # Adding MSLE to the bar plot\n",
    "    error_metrics = ['MSE', 'MAE', 'RMSE', 'MSLE']\n",
    "    values = [mse, mae, rmse, msle]\n",
    "    \n",
    "    # Plot updated error metrics with MSLE\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'])  # Added orange for MSLE\n",
    "    plt.xlabel('Error Metric')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Comparison of Error Metrics (with MSLE)')\n",
    "    plt.show()\n",
    "\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")\n",
    "\n",
    "# Plotting R-squared (R2) for the test set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if r2 >= 0:\n",
    "    plt.pie([r2, 100 - r2], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Explained Variance by R-squared (R2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353110ac-0992-4807-9c0f-a88ec34b2c5a",
   "metadata": {},
   "source": [
    "# External Validation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d35f0-0758-45ed-a510-b70a5e0af7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the external validation set (eICU data)\n",
    "y_pred_external = model.predict(X_external)\n",
    "\n",
    "# Metrics for external validation set\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "rmse_external = np.sqrt(mse_external)\n",
    "r2_external = r2_score(y_external, y_pred_external) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external}\")\n",
    "print(f\"External Validation Set MAE: {mae_external}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external}\")\n",
    "print(f\"External Validation Set R2: {r2_external}\")\n",
    "\n",
    "# Plotting MSLE if applicable\n",
    "try:\n",
    "    msle_external = mean_squared_log_error(y_test, y_pred)\n",
    "    print(f\"Test Set MSLE: {msle_external}\")\n",
    "    \n",
    "    # Adding MSLE to the bar plot\n",
    "    error_metrics = ['MSE', 'MAE', 'RMSE', 'MSLE']\n",
    "    values = [mse, mae_external, rmse_external, msle_external]\n",
    "    \n",
    "    # Plot updated error metrics with MSLE\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'])  # Added orange for MSLE\n",
    "    plt.xlabel('Error Metric')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Comparison of Error Metrics (with MSLE)')\n",
    "    plt.show()\n",
    "\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")\n",
    "\n",
    "# Plotting R-squared (R2) for the test set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if r2_external >= 0:\n",
    "    plt.pie([r2_external, 100 - r2_external], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Explained Variance by R-squared (R2)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5ee89-6e54-49fa-8e9b-40c5629f7ee6",
   "metadata": {},
   "source": [
    "# Most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f15d96-8eab-4129-9b21-4932c1701469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "most_important_df = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances along with their corresponding names\n",
    "most_important_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': most_important_df})\n",
    "\n",
    "# Sort the DataFrame by feature importance in descending order\n",
    "most_important_df = most_important_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Scale the importance\n",
    "most_important_df['Importance'] *= 100000\n",
    "\n",
    "# Print the top N most important features\n",
    "top_n = 20  # set features number\n",
    "print(f\"Top {top_n} most important features:\")\n",
    "print(most_important_df.head(top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf81e5-689e-4426-80ac-52c1c4ce470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style and remove gridlines\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Top 10 most important features\n",
    "top_10_features = most_important_df.head(20)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))  # Reduce figure size\n",
    "plot = sns.barplot(x='Importance', y='Feature', data=top_10_features, hue='Feature', palette=\"Blues\", legend=False)\n",
    "\n",
    "# Reduce font size slightly \n",
    "plt.xlabel('Importance', fontsize=18)\n",
    "plt.ylabel('Feature', fontsize=18)\n",
    "plt.title('Top 20 Features with Highest Importance', fontsize=20)\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "\n",
    "# Save the plot in high resolution\n",
    "#plt.savefig('plots/top_20_most_important_features.jpeg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80d79c-d34f-4cb1-a33e-72d9c12519b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
