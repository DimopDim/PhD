{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6243b02-165d-496a-a198-f00704a71eb6",
   "metadata": {},
   "source": [
    "# Imports | Reads | Filter Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671463d-f9c0-4542-b28c-eec0dfb36e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterGrid, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error, mean_squared_log_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a182eba9-001e-43e2-b928-7bdd96cf667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MIMICs CSV file\n",
    "mimic_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\mimic_mean_final.csv\")\n",
    "\n",
    "# Read eICUs CSV file\n",
    "eicu_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\eicu_mean_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae581b-694b-43c7-9805-90856381db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 10\n",
    "\n",
    "# Filter icu stay less than 10 days\n",
    "mimic_df = mimic_df[mimic_df['los'] < day]\n",
    "\n",
    "# Filter icu stay less than 10 days\n",
    "eicu_df = eicu_df[eicu_df['los'] < day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc6cae-2821-43c1-a6b1-04c79b38273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Time Zone\n",
    "\n",
    "#time_zone = 16\n",
    "#mimic_df = mimic_df[mimic_df['Time_Zone'] == time_zone]\n",
    "#eicu_df = eicu_df[eicu_df['Time_Zone'] == time_zone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894f6245-e142-45de-971e-fb52f9a1ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I'm gonna concat and split the mimic and icu\n",
    "at this point. I must create the same columns\n",
    "from the tranformation of categorical data.\n",
    "\"\"\"\n",
    "row_count = mimic_df.shape[0]\n",
    "print(f\"Row count: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9462d7a8-d964-43b5-9e61-6064e02f0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dataframes\n",
    "df_combined = pd.concat([mimic_df, eicu_df], ignore_index=True)\n",
    "\n",
    "# Find all categorical columns in mimic\n",
    "categorical_columns = df_combined.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Apply one-hot encoding to all categorical columns\n",
    "df_encoded = pd.get_dummies(df_combined, columns=categorical_columns)\n",
    "\n",
    "# Split the concatenate dataframe\n",
    "mimic_df = df_encoded.iloc[:row_count, :]\n",
    "eicu_df = df_encoded.iloc[row_count:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412fbb1-4cab-4455-8ea6-671a318fddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mimic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308615a-2615-468c-a99f-a0b2bf265697",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_val_perc = 0.2\n",
    "split_between_test_val_perc = 0.5\n",
    "\n",
    "# Group data by subject_id and hadm_id\n",
    "grouped_df = mimic_df.groupby(['subject_id', 'hadm_id'])\n",
    "\n",
    "# Get a new dataframe with one row per patient (subject_id, hadm_id) pair\n",
    "patient_df = grouped_df['hospital_expire_flag'].first().reset_index()\n",
    "\n",
    "# Split the patient_df into training (80%), validation (10%), and test (10%) while keeping the ratio of hospital_expired_flag\n",
    "train, temp = train_test_split(patient_df, test_size=total_test_val_perc, stratify=patient_df['hospital_expire_flag'], random_state=42)\n",
    "val, test = train_test_split(temp, test_size=split_between_test_val_perc, stratify=temp['hospital_expire_flag'], random_state=42)\n",
    "\n",
    "# Step 4: Merge back with the original df to get the rows for each patient in the splits\n",
    "train_df = mimic_df.merge(train[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "val_df = mimic_df.merge(val[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "test_df = mimic_df.merge(test[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "\n",
    "# Check the sizes of the splits\n",
    "print(f'Training set size: {train_df.shape[0]}')\n",
    "print(f'Validation set size: {val_df.shape[0]}')\n",
    "print(f'Test set size: {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47462f19-6295-4cb1-a8f7-64b8bd104609",
   "metadata": {},
   "source": [
    "# Check ratio and unique patients between sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ed3bc-b7ae-48fe-892f-66b6f814c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count on Training set survive and non-survive\n",
    "survival_counts = train_df['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/16\n",
    "temp_non_survive = survival_counts.get(1, 0)/16\n",
    "\n",
    "# Display the results\n",
    "print(f'Train Set')\n",
    "print(f'Survive: {temp_survive}')\n",
    "print(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "print(f'Ratio Train Set: {ratio:.2f}:1')\n",
    "\n",
    "\"\"\"----------------------------\"\"\"\n",
    "\n",
    "# Count on validation set survive and non-survive\n",
    "survival_counts = val_df['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/16\n",
    "temp_non_survive = survival_counts.get(1, 0)/16\n",
    "\n",
    "# Display the results\n",
    "print(f'\\nValidation Set')\n",
    "print(f'Survive: {temp_survive}')\n",
    "print(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "print(f'Ratio Train Set: {ratio:.2f}:1')\n",
    "\n",
    "\"\"\"----------------------------\"\"\"\n",
    "\n",
    "# Count on validation set survive and non-survive\n",
    "survival_counts = test_df['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/16\n",
    "temp_non_survive = survival_counts.get(1, 0)/16\n",
    "\n",
    "# Display the results\n",
    "print(f'\\nTest Set')\n",
    "print(f'Survive: {temp_survive}')\n",
    "print(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "print(f'Ratio Train Set: {ratio:.2f}:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73205561-9bcc-42fe-acc4-8bcd1616d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mine unique subject_id from sets\n",
    "train_subjects = set(train_df['subject_id'].unique())\n",
    "val_subjects = set(val_df['subject_id'].unique())\n",
    "test_subjects = set(test_df['subject_id'].unique())\n",
    "\n",
    "# Check if there are overlaping subject_id\n",
    "train_val_overlap = train_subjects.intersection(val_subjects)\n",
    "train_test_overlap = train_subjects.intersection(test_subjects)\n",
    "val_test_overlap = val_subjects.intersection(test_subjects)\n",
    "\n",
    "# Display the results\n",
    "print(f'Overlap between training and validation sets: {len(train_val_overlap)}')\n",
    "print(f'Overlap between training and test sets: {len(train_test_overlap)}')\n",
    "print(f'Overlap between validation and test sets: {len(val_test_overlap)}')\n",
    "\n",
    "# print overlaping\n",
    "if train_val_overlap:\n",
    "    print(f'Subjects in both training and validation: {train_val_overlap}')\n",
    "if train_test_overlap:\n",
    "    print(f'Subjects in both training and test: {train_test_overlap}')\n",
    "if val_test_overlap:\n",
    "    print(f'Subjects in both validation and test: {val_test_overlap}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d473a75a-86eb-4dac-be9a-b73cf80b111b",
   "metadata": {},
   "source": [
    "# Prepare Train - Validation - Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62e244-e475-4222-802b-57d374b63c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External validation from eICU\n",
    "X_external = eicu_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_external = eicu_df['los']\n",
    "\n",
    "# Separate features and target for the training, validation, and test sets\n",
    "X_train = train_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_train = train_df['los']\n",
    "\n",
    "X_validate = val_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_validate = val_df['los']\n",
    "\n",
    "X_test = test_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_test = test_df['los']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6110f0b-10a3-44c5-a8d8-10709fa9550b",
   "metadata": {},
   "source": [
    "# Fill Empty Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb4682-9f02-4fae-9995-13f278d0bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill training set missing values\n",
    "\n",
    "# Step 1: Identify columns with missing values in X_train\n",
    "missing_columns = X_train.columns[X_train.isnull().any()].tolist()\n",
    "print(f\"Columns with missing values: {missing_columns}\")\n",
    "\n",
    "# Step 2: Loop through each column with missing values and build an ANN to predict missing values\n",
    "for col in missing_columns:\n",
    "    print(f\"Filling missing values in column: {col}\")\n",
    "    \n",
    "    # Separate rows with and without missing values in the current column\n",
    "    missing_rows = X_train[X_train[col].isnull()]\n",
    "    non_missing_rows = X_train[~X_train[col].isnull()]\n",
    "    \n",
    "    # Skip the column if no data is available for training\n",
    "    if len(missing_rows) == 0 or len(non_missing_rows) == 0:\n",
    "        print(f\"Skipping {col}, insufficient data\")\n",
    "        continue\n",
    "    \n",
    "    # Separate features and target for non-missing rows\n",
    "    X_train_missing = non_missing_rows.drop(columns=missing_columns)  # Exclude other missing columns from features\n",
    "    y_train_missing = non_missing_rows[col]  # Target is the column we're filling\n",
    "    \n",
    "    # Features for the rows with missing values (we'll predict the column for these rows)\n",
    "    X_test_missing = missing_rows.drop(columns=missing_columns)\n",
    "    \n",
    "    # Step 3: Preprocess the data (Standard Scaling)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_missing_scaled = scaler.fit_transform(X_train_missing)\n",
    "    X_test_missing_scaled = scaler.transform(X_test_missing)\n",
    "    \n",
    "    # Step 4: Build the ANN model for filling missing values\n",
    "    model_missing = Sequential()\n",
    "    model_missing.add(Input(shape=(X_train_missing_scaled.shape[1],)))  # Use Input layer instead of input_shape in Dense\n",
    "    model_missing.add(Dense(units=64, activation='relu'))\n",
    "    model_missing.add(Dropout(0.3))\n",
    "    model_missing.add(Dense(units=32, activation='relu'))\n",
    "    model_missing.add(Dropout(0.3))\n",
    "    model_missing.add(Dense(units=1, activation='linear'))  # Linear activation for regression tasks\n",
    "    \n",
    "    # Compile the model\n",
    "    model_missing.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Step 5: Train the model\n",
    "    model_missing.fit(X_train_missing_scaled, y_train_missing, epochs=50, batch_size=32, validation_split=0.1, verbose=0)\n",
    "    \n",
    "    # Step 6: Predict the missing values\n",
    "    predicted_values = model_missing.predict(X_test_missing_scaled)\n",
    "    \n",
    "    # Step 7: Fill the missing values in X_train\n",
    "    X_train.loc[X_train[col].isnull(), col] = predicted_values\n",
    "    \n",
    "    print(f\"Filled missing values in column: {col}\")\n",
    "\n",
    "# Verify if there are any remaining missing values in X_train\n",
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0d8b5-2e98-40ee-a51b-cd34e9c6cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display (X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493f917-45cb-478c-af02-6116cb50953d",
   "metadata": {},
   "source": [
    "# Train Model without HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec099179-7c37-4f77-9229-a2da9069d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default XGBoost Model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set (eICU data)\n",
    "y_pred_external = model.predict(X_external)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de097a08-e1d3-4b2d-8b49-ff8ec1e8c901",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train Model with HP RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9d4dd-5eed-4f18-99e7-582fdfd0c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_dist = {\n",
    "    'learning_rate': np.arange(0.01, 1.01, 0.1),\n",
    "    'max_depth': np.arange(1, 11, 1),\n",
    "    'min_child_weight': np.arange(1, 6, 1),\n",
    "    'reg_lambda': np.arange(0.1, 15.1, 1),\n",
    "    'reg_alpha': np.arange(0.1, 15.1, 1),\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Set up RandomizedSearchCV with tqdm integration\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, \n",
    "                                    param_distributions=param_dist, \n",
    "                                    n_iter=100,  # Number of random samples to try\n",
    "                                    scoring='neg_mean_squared_error',  # Use negative MSE for minimization\n",
    "                                    cv=2,  # Number of folds for cross-validation\n",
    "                                    n_jobs=-1,  # Use all available cores\n",
    "                                    verbose=1,  # Print progress\n",
    "                                    random_state=42)  # Set seed for reproducibility\n",
    "\n",
    "# Perform the RandomizedSearchCV\n",
    "with tqdm(total=100, desc=\"Hyperparameter Tuning\") as pbar:\n",
    "    random_search.fit(X_train, y_train)\n",
    "    pbar.update(100)\n",
    "\n",
    "# Retrieve the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "best_score = -random_search.best_score_  # Convert from negative MSE to positive MSE\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best validation MSE: {best_score}\")\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set with the best model\n",
    "y_pred_external = best_model.predict(X_external)\n",
    "\n",
    "# Evaluation on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "r2_test = r2_score(y_test, y_pred_test) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse_test}\")\n",
    "print(f\"Test Set RMSE: {rmse_test}\")\n",
    "print(f\"Test Set MAE: {mae_test}\")\n",
    "print(f\"Test Set R2 Score: {r2_test}\")\n",
    "\n",
    "# Evaluation on the external validation set\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "rmse_external = np.sqrt(mse_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "r2_external = r2_score(y_external, y_pred_external) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external}\")\n",
    "print(f\"External Validation Set MAE: {mae_external}\")\n",
    "print(f\"External Validation Set R2 Score: {r2_external}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40be9029-bb02-4cc2-8346-fd023e69f99b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train Model with HP gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca65fe49-cf5b-4254-b2a9-d3208485b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': np.arange(0.01, 1.01, 0.3),\n",
    "    'max_depth': np.arange(1, 11, 1),\n",
    "    'min_child_weight': np.arange(1, 6, 1),\n",
    "    'reg_lambda': np.arange(0.1, 15.1, 1),\n",
    "    'reg_alpha': np.arange(0.1, 15.1, 1),\n",
    "    'n_estimators': [100, 200, 300]  # Keep n_estimators as is\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Set up GridSearchCV with tqdm integration\n",
    "grid_search = GridSearchCV(estimator=xgb_model, \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error',  # Use negative MSE for minimization\n",
    "                           cv=2,  # Number of folds for cross-validation\n",
    "                           n_jobs=-1,  # Use all available cores\n",
    "                           verbose=1)  # Disable default verbosity\n",
    "\n",
    "# Perform the GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best model and hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_  # Convert from negative MSE to positive MSE\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best validation MSE: {best_score}\")\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set with the best model\n",
    "y_pred_external = best_model.predict(X_external)\n",
    "\n",
    "# Evaluation on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "rmse_test = root_mean_squared_error(y_test, y_pred_test)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "r2_test = r2_score(y_test, y_pred_test) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse_test}\")\n",
    "print(f\"Test Set RMSE: {rmse_test}\")\n",
    "print(f\"Test Set MAE: {mae_test}\")\n",
    "print(f\"Test Set R2 Score: {r2_test}\")\n",
    "\n",
    "# Evaluation on the external validation set\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "rmse_external = root_mean_squared_error(y_external, y_pred_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "r2_external = r2_score(y_external, y_pred_external) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external}\")\n",
    "print(f\"External Validation Set MAE: {mae_external}\")\n",
    "print(f\"External Validation Set R2 Score: {r2_external}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c859dc-94ca-4033-a6cd-b91249f23693",
   "metadata": {},
   "source": [
    "# Test Set Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1edc9-0621-4568-89e6-b0121161b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse}\")\n",
    "print(f\"Test Set MAE: {mae}\")\n",
    "print(f\"Test Set RMSE: {rmse}\")\n",
    "print(f\"Test Set R2: {r2}\")\n",
    "\n",
    "# Plotting error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [mse, mae, rmse]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(error_metrics, values, color=['blue', 'green', 'red'])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie([r2, 100 - r2], labels=['Explained Variance (R2)', 'Unexplained Variance'], colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "plt.title('Explained Variance by R-squared (R2)')\n",
    "plt.show()\n",
    "\n",
    "# Plotting MSLE if applicable\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_pred)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(msle_values, marker='o', linestyle='-')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('MSLE')\n",
    "    plt.title('Mean Squared Logarithmic Error Across Predictions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353110ac-0992-4807-9c0f-a88ec34b2c5a",
   "metadata": {},
   "source": [
    "# External Validation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f72c8c-b4af-4f7f-8e33-427f61c40ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the external validation set (eICU data)\n",
    "y_pred_external = model.predict(X_external)\n",
    "\n",
    "# Metrics for external validation set\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "rmse_external = np.sqrt(mse_external)\n",
    "r2_external = r2_score(y_external, y_pred_external) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external}\")\n",
    "print(f\"External Validation Set MAE: {mae_external}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external}\")\n",
    "print(f\"External Validation Set R2: {r2_external}\")\n",
    "\n",
    "# Plotting error metrics for the external validation set\n",
    "error_metrics_external = ['MSE', 'MAE', 'RMSE']\n",
    "values_external = [mse_external, mae_external, rmse_external]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(error_metrics_external, values_external, color=['blue', 'green', 'red'])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics (External Validation Set)')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the external validation set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if r2_external >= 0:\n",
    "    plt.pie([r2_external, 100 - r2_external], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Explained Variance by R-squared (R2) - External Validation Set')\n",
    "plt.show()\n",
    "\n",
    "# Plotting MSLE for the external validation set if applicable\n",
    "try:\n",
    "    msle_external = mean_squared_log_error(y_external, y_pred_external)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_external, y_pred_external, marker='o', linestyle='-', label='MSLE')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('MSLE')\n",
    "    plt.title('Mean Squared Logarithmic Error (MSLE) - External Validation Set')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5ee89-6e54-49fa-8e9b-40c5629f7ee6",
   "metadata": {},
   "source": [
    "# Most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f15d96-8eab-4129-9b21-4932c1701469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "most_important_df = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances along with their corresponding names\n",
    "most_important_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': most_important_df})\n",
    "\n",
    "# Sort the DataFrame by feature importance in descending order\n",
    "most_important_df = most_important_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Scale the importance\n",
    "most_important_df['Importance'] *= 100000\n",
    "\n",
    "# Print the top N most important features\n",
    "top_n = 20  # set features number\n",
    "print(f\"Top {top_n} most important features:\")\n",
    "print(most_important_df.head(top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf81e5-689e-4426-80ac-52c1c4ce470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style and remove gridlines\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Top 10 most important features\n",
    "top_10_features = most_important_df.head(20)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))  # Reduce figure size\n",
    "plot = sns.barplot(x='Importance', y='Feature', data=top_10_features, hue='Feature', palette=\"Blues\", legend=False)\n",
    "\n",
    "# Reduce font size slightly \n",
    "plt.xlabel('Importance', fontsize=18)\n",
    "plt.ylabel('Feature', fontsize=18)\n",
    "plt.title('Top 20 Features with Highest Importance', fontsize=20)\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "\n",
    "# Save the plot in high resolution\n",
    "#plt.savefig('plots/top_20_most_important_features.jpeg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c292e-affe-47ca-841a-f2243d0f0af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff82c6-4d25-40f7-9df5-72d8ba694839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46397f6e-dfc2-47e4-915c-78a9a4daf720",
   "metadata": {},
   "source": [
    "# Testing field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e865875-9f52-4d07-bc64-630ec48ae959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter distributions\n",
    "param_grid = {\n",
    "    'learning_rate': np.linspace(0.01, 0.5, 10),\n",
    "    'max_depth': np.arange(1, 11),\n",
    "    'min_child_weight': np.arange(1, 6),\n",
    "    'reg_lambda': np.linspace(0.1, 15, 15),\n",
    "    'reg_alpha': np.linspace(0.1, 15, 15),\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Set up GridSearchCV with tqdm integration\n",
    "grid_search = GridSearchCV(estimator=xgb_model, \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error',  # Use negative MSE for minimization\n",
    "                           cv=2,  # Number of folds for cross-validation\n",
    "                           n_jobs=-1,  # Use all available cores\n",
    "                           verbose=1)  # Disable default verbosity\n",
    "\n",
    "# Perform the GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve best model and parameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "best_score = -random_search.best_score_\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best validation MSE: {best_score}\")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "y_pred_external = best_model.predict(X_external)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "r2_test = r2_score(y_test, y_pred_test) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse_test}\")\n",
    "print(f\"Test Set RMSE: {rmse_test}\")\n",
    "print(f\"Test Set MAE: {mae_test}\")\n",
    "print(f\"Test Set R2 Score: {r2_test}\")\n",
    "\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "rmse_external = np.sqrt(mse_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "r2_external = r2_score(y_external, y_pred_external) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external}\")\n",
    "print(f\"External Validation Set MAE: {mae_external}\")\n",
    "print(f\"External Validation Set R2 Score: {r2_external}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
