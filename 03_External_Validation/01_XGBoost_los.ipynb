{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6243b02-165d-496a-a198-f00704a71eb6",
   "metadata": {},
   "source": [
    "# Imports | Reads | Filter Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6671463d-f9c0-4542-b28c-eec0dfb36e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterGrid, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error, mean_squared_log_error, r2_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a182eba9-001e-43e2-b928-7bdd96cf667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MIMICs CSV file\n",
    "mimic_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\mimic_mean_final.csv\")\n",
    "\n",
    "# Read eICUs CSV file\n",
    "eicu_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\eicu_mean_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abae581b-694b-43c7-9805-90856381db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 10\n",
    "\n",
    "# Filter icu stay less than 10 days\n",
    "mimic_df = mimic_df[mimic_df['los'] < day]\n",
    "\n",
    "# Filter icu stay less than 10 days\n",
    "eicu_df = eicu_df[eicu_df['los'] < day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bc6cae-2821-43c1-a6b1-04c79b38273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Time Zone\n",
    "\n",
    "#time_zone = 16\n",
    "#mimic_df = mimic_df[mimic_df['Time_Zone'] == time_zone]\n",
    "#eicu_df = eicu_df[eicu_df['Time_Zone'] == time_zone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "894f6245-e142-45de-971e-fb52f9a1ffda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 48992\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I'm gonna concat and split the mimic and icu\n",
    "at this point. I must create the same columns\n",
    "from the tranformation of categorical data.\n",
    "\"\"\"\n",
    "row_count = mimic_df.shape[0]\n",
    "print(f\"Row count: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9462d7a8-d964-43b5-9e61-6064e02f0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dataframes\n",
    "df_combined = pd.concat([mimic_df, eicu_df], ignore_index=True)\n",
    "\n",
    "# Find all categorical columns in mimic\n",
    "categorical_columns = df_combined.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Apply one-hot encoding to all categorical columns\n",
    "df_encoded = pd.get_dummies(df_combined, columns=categorical_columns)\n",
    "\n",
    "# Split the concatenate dataframe\n",
    "mimic_df = df_encoded.iloc[:row_count, :]\n",
    "eicu_df = df_encoded.iloc[row_count:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4412fbb1-4cab-4455-8ea6-671a318fddad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_count</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>Time_Zone</th>\n",
       "      <th>age</th>\n",
       "      <th>Base Excess</th>\n",
       "      <th>Lactate</th>\n",
       "      <th>pCO2</th>\n",
       "      <th>Calculated Total CO2</th>\n",
       "      <th>BUN</th>\n",
       "      <th>...</th>\n",
       "      <th>race_PATIENT DECLINED TO ANSWER</th>\n",
       "      <th>race_PORTUGUESE</th>\n",
       "      <th>race_SOUTH AMERICAN</th>\n",
       "      <th>race_UNABLE TO OBTAIN</th>\n",
       "      <th>race_UNKNOWN</th>\n",
       "      <th>race_WHITE</th>\n",
       "      <th>race_WHITE - BRAZILIAN</th>\n",
       "      <th>race_WHITE - EASTERN EUROPEAN</th>\n",
       "      <th>race_WHITE - OTHER EUROPEAN</th>\n",
       "      <th>race_WHITE - RUSSIAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>38.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>38.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>38.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>38.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>38.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48987</th>\n",
       "      <td>55788</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>12</td>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48988</th>\n",
       "      <td>55789</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>13</td>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48989</th>\n",
       "      <td>55790</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>14</td>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48990</th>\n",
       "      <td>55791</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>15</td>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48991</th>\n",
       "      <td>55792</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>16</td>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48992 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_count  subject_id   hadm_id  Time_Zone  age  Base Excess  Lactate  \\\n",
       "0              1    10004733  27411876          1   51          0.0     0.80   \n",
       "1              2    10004733  27411876          2   51          0.0     0.75   \n",
       "2              3    10004733  27411876          3   51          0.0     0.80   \n",
       "3              4    10004733  27411876          4   51          0.0     0.75   \n",
       "4              5    10004733  27411876          5   51          0.0     0.75   \n",
       "...          ...         ...       ...        ...  ...          ...      ...   \n",
       "48987      55788    19999987  23865745         12   57          1.0      NaN   \n",
       "48988      55789    19999987  23865745         13   57          1.0      NaN   \n",
       "48989      55790    19999987  23865745         14   57          1.0      NaN   \n",
       "48990      55791    19999987  23865745         15   57          1.0      NaN   \n",
       "48991      55792    19999987  23865745         16   57          1.0      NaN   \n",
       "\n",
       "       pCO2  Calculated Total CO2   BUN  ...  race_PATIENT DECLINED TO ANSWER  \\\n",
       "0      38.0                  26.0  44.0  ...                            False   \n",
       "1      38.0                  26.0  44.0  ...                            False   \n",
       "2      38.0                  26.0  44.0  ...                            False   \n",
       "3      38.0                  26.0  44.0  ...                            False   \n",
       "4      38.0                  26.0  44.0  ...                            False   \n",
       "...     ...                   ...   ...  ...                              ...   \n",
       "48987  44.0                  28.0  18.0  ...                            False   \n",
       "48988  44.0                  28.0  18.0  ...                            False   \n",
       "48989  44.0                  28.0  18.0  ...                            False   \n",
       "48990  44.0                  28.0  18.0  ...                            False   \n",
       "48991  44.0                  28.0  18.0  ...                            False   \n",
       "\n",
       "       race_PORTUGUESE  race_SOUTH AMERICAN  race_UNABLE TO OBTAIN  \\\n",
       "0                False                False                  False   \n",
       "1                False                False                  False   \n",
       "2                False                False                  False   \n",
       "3                False                False                  False   \n",
       "4                False                False                  False   \n",
       "...                ...                  ...                    ...   \n",
       "48987            False                False                  False   \n",
       "48988            False                False                  False   \n",
       "48989            False                False                  False   \n",
       "48990            False                False                  False   \n",
       "48991            False                False                  False   \n",
       "\n",
       "       race_UNKNOWN  race_WHITE  race_WHITE - BRAZILIAN  \\\n",
       "0              True       False                   False   \n",
       "1              True       False                   False   \n",
       "2              True       False                   False   \n",
       "3              True       False                   False   \n",
       "4              True       False                   False   \n",
       "...             ...         ...                     ...   \n",
       "48987          True       False                   False   \n",
       "48988          True       False                   False   \n",
       "48989          True       False                   False   \n",
       "48990          True       False                   False   \n",
       "48991          True       False                   False   \n",
       "\n",
       "       race_WHITE - EASTERN EUROPEAN  race_WHITE - OTHER EUROPEAN  \\\n",
       "0                              False                        False   \n",
       "1                              False                        False   \n",
       "2                              False                        False   \n",
       "3                              False                        False   \n",
       "4                              False                        False   \n",
       "...                              ...                          ...   \n",
       "48987                          False                        False   \n",
       "48988                          False                        False   \n",
       "48989                          False                        False   \n",
       "48990                          False                        False   \n",
       "48991                          False                        False   \n",
       "\n",
       "       race_WHITE - RUSSIAN  \n",
       "0                     False  \n",
       "1                     False  \n",
       "2                     False  \n",
       "3                     False  \n",
       "4                     False  \n",
       "...                     ...  \n",
       "48987                 False  \n",
       "48988                 False  \n",
       "48989                 False  \n",
       "48990                 False  \n",
       "48991                 False  \n",
       "\n",
       "[48992 rows x 119 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mimic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c308615a-2615-468c-a99f-a0b2bf265697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 39184\n",
      "Validation set size: 4896\n",
      "Test set size: 4912\n"
     ]
    }
   ],
   "source": [
    "total_test_val_perc = 0.2\n",
    "split_between_test_val_perc = 0.5\n",
    "\n",
    "# Group data by subject_id and hadm_id\n",
    "grouped_df = mimic_df.groupby(['subject_id', 'hadm_id'])\n",
    "\n",
    "# Get a new dataframe with one row per patient (subject_id, hadm_id) pair\n",
    "patient_df = grouped_df['hospital_expire_flag'].first().reset_index()\n",
    "\n",
    "# Split the patient_df into training (80%), validation (10%), and test (10%) while keeping the ratio of hospital_expired_flag\n",
    "train, temp = train_test_split(patient_df, test_size=total_test_val_perc, stratify=patient_df['hospital_expire_flag'], random_state=42)\n",
    "val, test = train_test_split(temp, test_size=split_between_test_val_perc, stratify=temp['hospital_expire_flag'], random_state=42)\n",
    "\n",
    "# Step 4: Merge back with the original df to get the rows for each patient in the splits\n",
    "train_df = mimic_df.merge(train[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "val_df = mimic_df.merge(val[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "test_df = mimic_df.merge(test[['subject_id', 'hadm_id']], on=['subject_id', 'hadm_id'], how='inner')\n",
    "\n",
    "# Check the sizes of the splits\n",
    "print(f'Training set size: {train_df.shape[0]}')\n",
    "print(f'Validation set size: {val_df.shape[0]}')\n",
    "print(f'Test set size: {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47462f19-6295-4cb1-a8f7-64b8bd104609",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Check ratio and unique patients between sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729ed3bc-b7ae-48fe-892f-66b6f814c5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "Survive: 1949.0\n",
      "Non-survive: 500.0\n",
      "Ratio Train Set: 3.90:1\n",
      "\n",
      "Validation Set\n",
      "Survive: 244.0\n",
      "Non-survive: 62.0\n",
      "Ratio Train Set: 3.94:1\n",
      "\n",
      "Test Set\n",
      "Survive: 244.0\n",
      "Non-survive: 63.0\n",
      "Ratio Train Set: 3.87:1\n"
     ]
    }
   ],
   "source": [
    "# Count on Training set survive and non-survive\n",
    "survival_counts = train_df['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/16\n",
    "temp_non_survive = survival_counts.get(1, 0)/16\n",
    "\n",
    "# Display the results\n",
    "print(f'Train Set')\n",
    "print(f'Survive: {temp_survive}')\n",
    "print(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "print(f'Ratio Train Set: {ratio:.2f}:1')\n",
    "\n",
    "\"\"\"----------------------------\"\"\"\n",
    "\n",
    "# Count on validation set survive and non-survive\n",
    "survival_counts = val_df['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/16\n",
    "temp_non_survive = survival_counts.get(1, 0)/16\n",
    "\n",
    "# Display the results\n",
    "print(f'\\nValidation Set')\n",
    "print(f'Survive: {temp_survive}')\n",
    "print(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "print(f'Ratio Train Set: {ratio:.2f}:1')\n",
    "\n",
    "\"\"\"----------------------------\"\"\"\n",
    "\n",
    "# Count on validation set survive and non-survive\n",
    "survival_counts = test_df['hospital_expire_flag'].value_counts()\n",
    "temp_survive = survival_counts.get(0, 0)/16\n",
    "temp_non_survive = survival_counts.get(1, 0)/16\n",
    "\n",
    "# Display the results\n",
    "print(f'\\nTest Set')\n",
    "print(f'Survive: {temp_survive}')\n",
    "print(f'Non-survive: {temp_non_survive}')\n",
    "\n",
    "# Check if temp_non_survive is not zero to avoid division by zero\n",
    "if temp_non_survive != 0:\n",
    "    ratio = temp_survive / temp_non_survive\n",
    "else:\n",
    "    ratio = float('inf')  # Set ratio to infinity if there are no non-survivors\n",
    "\n",
    "# Display the ratio\n",
    "print(f'Ratio Train Set: {ratio:.2f}:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73205561-9bcc-42fe-acc4-8bcd1616d9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between training and validation sets: 0\n",
      "Overlap between training and test sets: 0\n",
      "Overlap between validation and test sets: 0\n"
     ]
    }
   ],
   "source": [
    "# Mine unique subject_id from sets\n",
    "train_subjects = set(train_df['subject_id'].unique())\n",
    "val_subjects = set(val_df['subject_id'].unique())\n",
    "test_subjects = set(test_df['subject_id'].unique())\n",
    "\n",
    "# Check if there are overlaping subject_id\n",
    "train_val_overlap = train_subjects.intersection(val_subjects)\n",
    "train_test_overlap = train_subjects.intersection(test_subjects)\n",
    "val_test_overlap = val_subjects.intersection(test_subjects)\n",
    "\n",
    "# Display the results\n",
    "print(f'Overlap between training and validation sets: {len(train_val_overlap)}')\n",
    "print(f'Overlap between training and test sets: {len(train_test_overlap)}')\n",
    "print(f'Overlap between validation and test sets: {len(val_test_overlap)}')\n",
    "\n",
    "# print overlaping\n",
    "if train_val_overlap:\n",
    "    print(f'Subjects in both training and validation: {train_val_overlap}')\n",
    "if train_test_overlap:\n",
    "    print(f'Subjects in both training and test: {train_test_overlap}')\n",
    "if val_test_overlap:\n",
    "    print(f'Subjects in both validation and test: {val_test_overlap}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d473a75a-86eb-4dac-be9a-b73cf80b111b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prepare Train - Validation - Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee62e244-e475-4222-802b-57d374b63c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External validation from eICU\n",
    "X_external = eicu_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_external = eicu_df['los']\n",
    "\n",
    "# Separate features and target for the training, validation, and test sets\n",
    "X_train = train_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_train = train_df['los']\n",
    "\n",
    "X_validate = val_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_validate = val_df['los']\n",
    "\n",
    "X_test = test_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_test = test_df['los']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6110f0b-10a3-44c5-a8d8-10709fa9550b",
   "metadata": {},
   "source": [
    "# Fill Empty Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb4682-9f02-4fae-9995-13f278d0bd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values: ['Base Excess', 'Lactate', 'pCO2', 'Calculated Total CO2', 'BUN', 'pH', 'pO2', 'Alanine Aminotransferase (ALT)', 'Alkaline Phosphatase', 'Anion Gap', 'Asparate Aminotransferase (AST)', 'Bicarbonate', 'Chloride', 'Creatinine', 'Glucose', 'Magnesium', 'Phosphate', 'Potassium', 'Sodium', 'Hematocrit', 'Hemoglobin', 'INR(PT)', 'MCH', 'MCHC', 'MCV', 'Platelet Count', 'PT', 'PTT', 'RDW', 'Red Blood Cells', 'White Blood Cells', 'Non Invasive Blood Pressure systolic (mmHg)', 'Non Invasive Blood Pressure diastolic (mmHg)', 'Non Invasive Blood Pressure mean (mmHg)', 'Respiratory Rate (insp/min)', 'O2 saturation pulseoxymetry (%)', 'Chloride (serum)', 'Calcium non-ionized', 'CK (CPK)', 'Temperature Fahrenheit (F)', 'Pain Level', 'O2 Flow (L/min)', 'Inspired O2 Fraction', 'Ionized Calcium', 'Albumin', 'GCS', 'Total Bilirubin', 'LDH', 'ETOH', 'Arterial Blood Pressure systolic (mmHg)', 'Arterial Blood Pressure diastolic (mmHg)', 'Arterial Blood Pressure mean (mmHg)', 'Serum Osmolality', 'Troponin-T', 'Uric Acid', 'Ammonia', 'C Reactive Protein (CRP)', 'Fibrinogen', 'Pulmonary Artery Pressure systolic (mmHg)', 'Pulmonary Artery Pressure diastolic (mmHg)', 'Pulmonary Artery Pressure mean (mmHg)', 'Glucose finger stick (range 70-100)', 'Reticulocyte Count Automated', 'Differential-Basos', 'Differential-Eos', 'Differential-Lymphs', 'Differential-Monos', 'Differential-Neuts', 'Haptoglobin', 'Bilirubin Direct', 'Thyroxine (T4) Free', 'Sedimentation Rate', 'CK-MB', 'Amylase', 'PEEP set (cmH2O)', 'Central Venous Pressure (mmHg)']\n",
      "Filling missing values in column: Base Excess\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Filled missing values in column: Base Excess\n",
      "Filling missing values in column: Lactate\n",
      "\u001b[1m735/735\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Filled missing values in column: Lactate\n",
      "Filling missing values in column: pCO2\n"
     ]
    }
   ],
   "source": [
    "# Fill training set missing values\n",
    "\n",
    "# Step 1: Identify columns with missing values in X_train\n",
    "missing_columns = X_train.columns[X_train.isnull().any()].tolist()\n",
    "print(f\"Columns with missing values: {missing_columns}\")\n",
    "\n",
    "# Step 2: Loop through each column with missing values and build an ANN to predict missing values\n",
    "for col in missing_columns:\n",
    "    print(f\"Filling missing values in column: {col}\")\n",
    "    \n",
    "    # Separate rows with and without missing values in the current column\n",
    "    missing_rows = X_train[X_train[col].isnull()]\n",
    "    non_missing_rows = X_train[~X_train[col].isnull()]\n",
    "    \n",
    "    # Skip the column if no data is available for training\n",
    "    if len(missing_rows) == 0 or len(non_missing_rows) == 0:\n",
    "        print(f\"Skipping {col}, insufficient data\")\n",
    "        continue\n",
    "    \n",
    "    # Separate features and target for non-missing rows\n",
    "    X_train_missing = non_missing_rows.drop(columns=missing_columns)  # Exclude other missing columns from features\n",
    "    y_train_missing = non_missing_rows[col]  # Target is the column we're filling\n",
    "    \n",
    "    # Features for the rows with missing values (we'll predict the column for these rows)\n",
    "    X_test_missing = missing_rows.drop(columns=missing_columns)\n",
    "    \n",
    "    # Step 3: Preprocess the data (Standard Scaling)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_missing_scaled = scaler.fit_transform(X_train_missing)\n",
    "    X_test_missing_scaled = scaler.transform(X_test_missing)\n",
    "    \n",
    "    # Step 4: Build the ANN model for filling missing values\n",
    "    model_missing = Sequential()\n",
    "    model_missing.add(Input(shape=(X_train_missing_scaled.shape[1],)))  # Use Input layer instead of input_shape in Dense\n",
    "    model_missing.add(Dense(units=64, activation='relu'))\n",
    "    model_missing.add(Dropout(0.3))\n",
    "    model_missing.add(Dense(units=32, activation='relu'))\n",
    "    model_missing.add(Dropout(0.3))\n",
    "    model_missing.add(Dense(units=1, activation='linear'))  # Linear activation for regression tasks\n",
    "    \n",
    "    # Compile the model\n",
    "    model_missing.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Step 5: Train the model\n",
    "    model_missing.fit(X_train_missing_scaled, y_train_missing, epochs=50, batch_size=32, validation_split=0.1, verbose=0)\n",
    "    \n",
    "    # Step 6: Predict the missing values\n",
    "    predicted_values = model_missing.predict(X_test_missing_scaled)\n",
    "    \n",
    "    # Step 7: Fill the missing values in X_train\n",
    "    X_train.loc[X_train[col].isnull(), col] = predicted_values\n",
    "    \n",
    "    print(f\"Filled missing values in column: {col}\")\n",
    "\n",
    "# Verify if there are any remaining missing values in X_train\n",
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0d8b5-2e98-40ee-a51b-cd34e9c6cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display (X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493f917-45cb-478c-af02-6116cb50953d",
   "metadata": {},
   "source": [
    "# Train Model without HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec099179-7c37-4f77-9229-a2da9069d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default XGBoost Model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set (eICU data)\n",
    "y_pred_external = model.predict(X_external)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de097a08-e1d3-4b2d-8b49-ff8ec1e8c901",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train Model with HP RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9d4dd-5eed-4f18-99e7-582fdfd0c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_dist = {\n",
    "    'learning_rate': np.arange(0.01, 1.01, 0.1),\n",
    "    'max_depth': np.arange(1, 11, 1),\n",
    "    'min_child_weight': np.arange(1, 6, 1),\n",
    "    'reg_lambda': np.arange(0.1, 15.1, 1),\n",
    "    'reg_alpha': np.arange(0.1, 15.1, 1),\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Set up RandomizedSearchCV with tqdm integration\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, \n",
    "                                    param_distributions=param_dist, \n",
    "                                    n_iter=100,  # Number of random samples to try\n",
    "                                    scoring='neg_mean_squared_error',  # Use negative MSE for minimization\n",
    "                                    cv=2,  # Number of folds for cross-validation\n",
    "                                    n_jobs=-1,  # Use all available cores\n",
    "                                    verbose=1,  # Print progress\n",
    "                                    random_state=42)  # Set seed for reproducibility\n",
    "\n",
    "# Perform the RandomizedSearchCV\n",
    "with tqdm(total=100, desc=\"Hyperparameter Tuning\") as pbar:\n",
    "    random_search.fit(X_train, y_train)\n",
    "    pbar.update(100)\n",
    "\n",
    "# Retrieve the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "best_score = -random_search.best_score_  # Convert from negative MSE to positive MSE\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best validation MSE: {best_score}\")\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set with the best model\n",
    "y_pred_external = best_model.predict(X_external)\n",
    "\n",
    "# Evaluation on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "r2_test = r2_score(y_test, y_pred_test) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse_test}\")\n",
    "print(f\"Test Set RMSE: {rmse_test}\")\n",
    "print(f\"Test Set MAE: {mae_test}\")\n",
    "print(f\"Test Set R2 Score: {r2_test}\")\n",
    "\n",
    "# Evaluation on the external validation set\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "rmse_external = np.sqrt(mse_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "r2_external = r2_score(y_external, y_pred_external) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external}\")\n",
    "print(f\"External Validation Set MAE: {mae_external}\")\n",
    "print(f\"External Validation Set R2 Score: {r2_external}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40be9029-bb02-4cc2-8346-fd023e69f99b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train Model with HP gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca65fe49-cf5b-4254-b2a9-d3208485b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': np.arange(0.01, 1.01, 0.3),\n",
    "    'max_depth': np.arange(1, 11, 1),\n",
    "    'min_child_weight': np.arange(1, 6, 1),\n",
    "    'reg_lambda': np.arange(0.1, 15.1, 1),\n",
    "    'reg_alpha': np.arange(0.1, 15.1, 1),\n",
    "    'n_estimators': [100, 200, 300]  # Keep n_estimators as is\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Set up GridSearchCV with tqdm integration\n",
    "grid_search = GridSearchCV(estimator=xgb_model, \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error',  # Use negative MSE for minimization\n",
    "                           cv=2,  # Number of folds for cross-validation\n",
    "                           n_jobs=-1,  # Use all available cores\n",
    "                           verbose=1)  # Disable default verbosity\n",
    "\n",
    "# Perform the GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best model and hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_  # Convert from negative MSE to positive MSE\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best validation MSE: {best_score}\")\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set with the best model\n",
    "y_pred_external = best_model.predict(X_external)\n",
    "\n",
    "# Evaluation on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "rmse_test = root_mean_squared_error(y_test, y_pred_test)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "r2_test = r2_score(y_test, y_pred_test) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse_test}\")\n",
    "print(f\"Test Set RMSE: {rmse_test}\")\n",
    "print(f\"Test Set MAE: {mae_test}\")\n",
    "print(f\"Test Set R2 Score: {r2_test}\")\n",
    "\n",
    "# Evaluation on the external validation set\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "rmse_external = root_mean_squared_error(y_external, y_pred_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "r2_external = r2_score(y_external, y_pred_external) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external}\")\n",
    "print(f\"External Validation Set MAE: {mae_external}\")\n",
    "print(f\"External Validation Set R2 Score: {r2_external}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c859dc-94ca-4033-a6cd-b91249f23693",
   "metadata": {},
   "source": [
    "# Test Set Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1edc9-0621-4568-89e6-b0121161b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse}\")\n",
    "print(f\"Test Set MAE: {mae}\")\n",
    "print(f\"Test Set RMSE: {rmse}\")\n",
    "print(f\"Test Set R2: {r2}\")\n",
    "\n",
    "# Plotting error metrics\n",
    "error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "values = [mse, mae, rmse]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(error_metrics, values, color=['blue', 'green', 'red'])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie([r2, 100 - r2], labels=['Explained Variance (R2)', 'Unexplained Variance'], colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "plt.title('Explained Variance by R-squared (R2)')\n",
    "plt.show()\n",
    "\n",
    "# Plotting MSLE if applicable\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_pred)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(msle_values, marker='o', linestyle='-')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('MSLE')\n",
    "    plt.title('Mean Squared Logarithmic Error Across Predictions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353110ac-0992-4807-9c0f-a88ec34b2c5a",
   "metadata": {},
   "source": [
    "# External Validation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f72c8c-b4af-4f7f-8e33-427f61c40ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the external validation set (eICU data)\n",
    "y_pred_external = model.predict(X_external)\n",
    "\n",
    "# Metrics for external validation set\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "rmse_external = np.sqrt(mse_external)\n",
    "r2_external = r2_score(y_external, y_pred_external) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external}\")\n",
    "print(f\"External Validation Set MAE: {mae_external}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external}\")\n",
    "print(f\"External Validation Set R2: {r2_external}\")\n",
    "\n",
    "# Plotting error metrics for the external validation set\n",
    "error_metrics_external = ['MSE', 'MAE', 'RMSE']\n",
    "values_external = [mse_external, mae_external, rmse_external]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(error_metrics_external, values_external, color=['blue', 'green', 'red'])\n",
    "plt.xlabel('Error Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of Error Metrics (External Validation Set)')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared (R2) for the external validation set\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "if r2_external >= 0:\n",
    "    plt.pie([r2_external, 100 - r2_external], \n",
    "            labels=['Explained Variance (R2)', 'Unexplained Variance'], \n",
    "            colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "else:\n",
    "    plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Explained Variance by R-squared (R2) - External Validation Set')\n",
    "plt.show()\n",
    "\n",
    "# Plotting MSLE for the external validation set if applicable\n",
    "try:\n",
    "    msle_external = mean_squared_log_error(y_external, y_pred_external)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_external, y_pred_external, marker='o', linestyle='-', label='MSLE')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('MSLE')\n",
    "    plt.title('Mean Squared Logarithmic Error (MSLE) - External Validation Set')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5ee89-6e54-49fa-8e9b-40c5629f7ee6",
   "metadata": {},
   "source": [
    "# Most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f15d96-8eab-4129-9b21-4932c1701469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "most_important_df = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances along with their corresponding names\n",
    "most_important_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': most_important_df})\n",
    "\n",
    "# Sort the DataFrame by feature importance in descending order\n",
    "most_important_df = most_important_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Scale the importance\n",
    "most_important_df['Importance'] *= 100000\n",
    "\n",
    "# Print the top N most important features\n",
    "top_n = 20  # set features number\n",
    "print(f\"Top {top_n} most important features:\")\n",
    "print(most_important_df.head(top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf81e5-689e-4426-80ac-52c1c4ce470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style and remove gridlines\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Top 10 most important features\n",
    "top_10_features = most_important_df.head(20)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))  # Reduce figure size\n",
    "plot = sns.barplot(x='Importance', y='Feature', data=top_10_features, hue='Feature', palette=\"Blues\", legend=False)\n",
    "\n",
    "# Reduce font size slightly \n",
    "plt.xlabel('Importance', fontsize=18)\n",
    "plt.ylabel('Feature', fontsize=18)\n",
    "plt.title('Top 20 Features with Highest Importance', fontsize=20)\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "\n",
    "# Save the plot in high resolution\n",
    "#plt.savefig('plots/top_20_most_important_features.jpeg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c292e-affe-47ca-841a-f2243d0f0af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff82c6-4d25-40f7-9df5-72d8ba694839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46397f6e-dfc2-47e4-915c-78a9a4daf720",
   "metadata": {},
   "source": [
    "# Testing field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e865875-9f52-4d07-bc64-630ec48ae959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter distributions\n",
    "param_grid = {\n",
    "    'learning_rate': np.linspace(0.01, 0.5, 10),\n",
    "    'max_depth': np.arange(1, 11),\n",
    "    'min_child_weight': np.arange(1, 6),\n",
    "    'reg_lambda': np.linspace(0.1, 15, 15),\n",
    "    'reg_alpha': np.linspace(0.1, 15, 15),\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Set up GridSearchCV with tqdm integration\n",
    "grid_search = GridSearchCV(estimator=xgb_model, \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error',  # Use negative MSE for minimization\n",
    "                           cv=2,  # Number of folds for cross-validation\n",
    "                           n_jobs=-1,  # Use all available cores\n",
    "                           verbose=1)  # Disable default verbosity\n",
    "\n",
    "# Perform the GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve best model and parameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "best_score = -random_search.best_score_\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best validation MSE: {best_score}\")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "y_pred_external = best_model.predict(X_external)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "r2_test = r2_score(y_test, y_pred_test) * 100\n",
    "\n",
    "print(f\"Test Set MSE: {mse_test}\")\n",
    "print(f\"Test Set RMSE: {rmse_test}\")\n",
    "print(f\"Test Set MAE: {mae_test}\")\n",
    "print(f\"Test Set R2 Score: {r2_test}\")\n",
    "\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "rmse_external = np.sqrt(mse_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "r2_external = r2_score(y_external, y_pred_external) * 100\n",
    "\n",
    "print(f\"External Validation Set MSE: {mse_external}\")\n",
    "print(f\"External Validation Set RMSE: {rmse_external}\")\n",
    "print(f\"External Validation Set MAE: {mae_external}\")\n",
    "print(f\"External Validation Set R2 Score: {r2_external}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
