{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbbd189-fc26-490c-8597-aff82efff426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.callbacks import EarlyStopping  # Use only this line\n",
    "\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "#from tensorflow.keras.layers import Dense, Dropout\n",
    "#from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3fd3c31-809e-47e7-ae6f-cb9cb74d9f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MIMICs CSV file\n",
    "mimic_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\mimic_mean_final.csv\")\n",
    "\n",
    "# Read eICUs CSV file\n",
    "eicu_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\eicu_mean_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df422077-0a58-4e6c-836b-b1bc9d3cb204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframes\n",
    "df_combined = pd.concat([mimic_df, eicu_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02cec144-a9ff-4605-b381-107015f0c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all categorical columns in mimic\n",
    "categorical_columns = df_combined.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Apply one-hot encoding to all categorical columns\n",
    "df_encoded = pd.get_dummies(df_combined, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e842e2-6900-4abd-8e17-39f7762dcf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the concatenate dataframe\n",
    "mimic_df = df_encoded.iloc[:55792, :]  # Rows from 0 to 55791\n",
    "eicu_df = df_encoded.iloc[55792:, :]  # Rows from 55792 to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35bde315-9442-4f4e-82c4-dbe00b3de105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by `subject_id` and `hadm_id` to get unique patient admission records\n",
    "unique_patients = mimic_df[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "\n",
    "# Split the unique patients into train, validation, and test sets\n",
    "train_patients, test_patients = train_test_split(unique_patients, test_size=0.10, random_state=42)\n",
    "train_patients, validate_patients = train_test_split(train_patients, test_size=0.11, random_state=42)  # 0.11 * 90% ~= 10%\n",
    "\n",
    "# Merge the patients back with the original data to get the full records\n",
    "train_set = mimic_df.merge(train_patients, on=['subject_id', 'hadm_id'])\n",
    "validate_set = mimic_df.merge(validate_patients, on=['subject_id', 'hadm_id'])\n",
    "test_set = mimic_df.merge(test_patients, on=['subject_id', 'hadm_id'])\n",
    "\n",
    "# External validation from eICU\n",
    "X_external = eicu_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_external = eicu_df['hospital_expire_flag']\n",
    "\n",
    "# Separate features and target for the training, validation, and test sets\n",
    "X_train = train_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_train = train_set['hospital_expire_flag']\n",
    "\n",
    "X_validate = validate_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_validate = validate_set['hospital_expire_flag']\n",
    "\n",
    "X_test = test_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_test = test_set['hospital_expire_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85acfd6e-cfe6-4cb3-81ec-44d7917d0947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Base Excess</th>\n",
       "      <th>Lactate</th>\n",
       "      <th>pCO2</th>\n",
       "      <th>Calculated Total CO2</th>\n",
       "      <th>BUN</th>\n",
       "      <th>pH</th>\n",
       "      <th>pO2</th>\n",
       "      <th>Alanine Aminotransferase (ALT)</th>\n",
       "      <th>Alkaline Phosphatase</th>\n",
       "      <th>...</th>\n",
       "      <th>race_PATIENT DECLINED TO ANSWER</th>\n",
       "      <th>race_PORTUGUESE</th>\n",
       "      <th>race_SOUTH AMERICAN</th>\n",
       "      <th>race_UNABLE TO OBTAIN</th>\n",
       "      <th>race_UNKNOWN</th>\n",
       "      <th>race_WHITE</th>\n",
       "      <th>race_WHITE - BRAZILIAN</th>\n",
       "      <th>race_WHITE - EASTERN EUROPEAN</th>\n",
       "      <th>race_WHITE - OTHER EUROPEAN</th>\n",
       "      <th>race_WHITE - RUSSIAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>38.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>7.43</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>38.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>7.43</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>38.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>7.43</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>38.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>7.43</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>38.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>7.43</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44667</th>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.39</td>\n",
       "      <td>114.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44668</th>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.39</td>\n",
       "      <td>114.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44669</th>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.39</td>\n",
       "      <td>114.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44670</th>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.39</td>\n",
       "      <td>114.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44671</th>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.39</td>\n",
       "      <td>114.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44672 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  Base Excess  Lactate  pCO2  Calculated Total CO2   BUN    pH  \\\n",
       "0       51          0.0     0.80  38.0                  26.0  44.0  7.43   \n",
       "1       51          0.0     0.75  38.0                  26.0  44.0  7.43   \n",
       "2       51          0.0     0.80  38.0                  26.0  44.0  7.43   \n",
       "3       51          0.0     0.75  38.0                  26.0  44.0  7.43   \n",
       "4       51          0.0     0.75  38.0                  26.0  44.0  7.43   \n",
       "...    ...          ...      ...   ...                   ...   ...   ...   \n",
       "44667   57          1.0      NaN  44.0                  28.0  18.0  7.39   \n",
       "44668   57          1.0      NaN  44.0                  28.0  18.0  7.39   \n",
       "44669   57          1.0      NaN  44.0                  28.0  18.0  7.39   \n",
       "44670   57          1.0      NaN  44.0                  28.0  18.0  7.39   \n",
       "44671   57          1.0      NaN  44.0                  28.0  18.0  7.39   \n",
       "\n",
       "         pO2  Alanine Aminotransferase (ALT)  Alkaline Phosphatase  ...  \\\n",
       "0      100.0                            46.0                 113.0  ...   \n",
       "1      100.0                            46.0                 113.0  ...   \n",
       "2      100.0                            46.0                 113.0  ...   \n",
       "3      100.0                            46.0                 113.0  ...   \n",
       "4      100.0                            46.0                 113.0  ...   \n",
       "...      ...                             ...                   ...  ...   \n",
       "44667  114.0                            63.0                 106.0  ...   \n",
       "44668  114.0                            63.0                 106.0  ...   \n",
       "44669  114.0                            63.0                 106.0  ...   \n",
       "44670  114.0                            63.0                 106.0  ...   \n",
       "44671  114.0                            63.0                 106.0  ...   \n",
       "\n",
       "       race_PATIENT DECLINED TO ANSWER  race_PORTUGUESE  race_SOUTH AMERICAN  \\\n",
       "0                                False            False                False   \n",
       "1                                False            False                False   \n",
       "2                                False            False                False   \n",
       "3                                False            False                False   \n",
       "4                                False            False                False   \n",
       "...                                ...              ...                  ...   \n",
       "44667                            False            False                False   \n",
       "44668                            False            False                False   \n",
       "44669                            False            False                False   \n",
       "44670                            False            False                False   \n",
       "44671                            False            False                False   \n",
       "\n",
       "       race_UNABLE TO OBTAIN  race_UNKNOWN  race_WHITE  \\\n",
       "0                      False          True       False   \n",
       "1                      False          True       False   \n",
       "2                      False          True       False   \n",
       "3                      False          True       False   \n",
       "4                      False          True       False   \n",
       "...                      ...           ...         ...   \n",
       "44667                  False          True       False   \n",
       "44668                  False          True       False   \n",
       "44669                  False          True       False   \n",
       "44670                  False          True       False   \n",
       "44671                  False          True       False   \n",
       "\n",
       "       race_WHITE - BRAZILIAN  race_WHITE - EASTERN EUROPEAN  \\\n",
       "0                       False                          False   \n",
       "1                       False                          False   \n",
       "2                       False                          False   \n",
       "3                       False                          False   \n",
       "4                       False                          False   \n",
       "...                       ...                            ...   \n",
       "44667                   False                          False   \n",
       "44668                   False                          False   \n",
       "44669                   False                          False   \n",
       "44670                   False                          False   \n",
       "44671                   False                          False   \n",
       "\n",
       "       race_WHITE - OTHER EUROPEAN  race_WHITE - RUSSIAN  \n",
       "0                            False                 False  \n",
       "1                            False                 False  \n",
       "2                            False                 False  \n",
       "3                            False                 False  \n",
       "4                            False                 False  \n",
       "...                            ...                   ...  \n",
       "44667                        False                 False  \n",
       "44668                        False                 False  \n",
       "44669                        False                 False  \n",
       "44670                        False                 False  \n",
       "44671                        False                 False  \n",
       "\n",
       "[44672 rows x 114 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display (X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f00892-6735-4a02-91b9-387b7aa1e0b7",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- Fill empty cell in training set\n",
    "\n",
    "- StandardScaler, a popular preprocessing technique, offers a simple yet effective method for standardizing feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9709c-f8eb-4c48-911d-7fb43f4b7447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values: ['Base Excess', 'Lactate', 'pCO2', 'Calculated Total CO2', 'BUN', 'pH', 'pO2', 'Alanine Aminotransferase (ALT)', 'Alkaline Phosphatase', 'Anion Gap', 'Asparate Aminotransferase (AST)', 'Bicarbonate', 'Chloride', 'Creatinine', 'Glucose', 'Magnesium', 'Phosphate', 'Potassium', 'Sodium', 'Hematocrit', 'Hemoglobin', 'INR(PT)', 'MCH', 'MCHC', 'MCV', 'Platelet Count', 'PT', 'PTT', 'RDW', 'Red Blood Cells', 'White Blood Cells', 'Non Invasive Blood Pressure systolic (mmHg)', 'Non Invasive Blood Pressure diastolic (mmHg)', 'Non Invasive Blood Pressure mean (mmHg)', 'Respiratory Rate (insp/min)', 'O2 saturation pulseoxymetry (%)', 'Chloride (serum)', 'Calcium non-ionized', 'CK (CPK)', 'Temperature Fahrenheit (F)', 'Pain Level', 'O2 Flow (L/min)', 'Inspired O2 Fraction', 'Ionized Calcium', 'Albumin', 'GCS', 'Total Bilirubin', 'LDH', 'ETOH', 'Arterial Blood Pressure systolic (mmHg)', 'Arterial Blood Pressure diastolic (mmHg)', 'Arterial Blood Pressure mean (mmHg)', 'Serum Osmolality', 'Troponin-T', 'Uric Acid', 'Ammonia', 'C Reactive Protein (CRP)', 'Fibrinogen', 'Pulmonary Artery Pressure systolic (mmHg)', 'Pulmonary Artery Pressure diastolic (mmHg)', 'Pulmonary Artery Pressure mean (mmHg)', 'Glucose finger stick (range 70-100)', 'Reticulocyte Count Automated', 'Differential-Basos', 'Differential-Eos', 'Differential-Lymphs', 'Differential-Monos', 'Differential-Neuts', 'Haptoglobin', 'Bilirubin Direct', 'Thyroxine (T4) Free', 'Sedimentation Rate', 'CK-MB', 'Amylase', 'PEEP set (cmH2O)', 'Central Venous Pressure (mmHg)']\n",
      "Filling missing values in column: Base Excess\n",
      "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 986us/step\n",
      "Filled missing values in column: Base Excess\n",
      "Filling missing values in column: Lactate\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825us/step\n",
      "Filled missing values in column: Lactate\n",
      "Filling missing values in column: pCO2\n"
     ]
    }
   ],
   "source": [
    "# Fill training set missing values\n",
    "\n",
    "# Step 1: Identify columns with missing values in X_train\n",
    "missing_columns = X_train.columns[X_train.isnull().any()].tolist()\n",
    "print(f\"Columns with missing values: {missing_columns}\")\n",
    "\n",
    "# Step 2: Loop through each column with missing values and build an ANN to predict missing values\n",
    "for col in missing_columns:\n",
    "    print(f\"Filling missing values in column: {col}\")\n",
    "    \n",
    "    # Separate rows with and without missing values in the current column\n",
    "    missing_rows = X_train[X_train[col].isnull()]\n",
    "    non_missing_rows = X_train[~X_train[col].isnull()]\n",
    "    \n",
    "    # Skip the column if no data is available for training\n",
    "    if len(missing_rows) == 0 or len(non_missing_rows) == 0:\n",
    "        print(f\"Skipping {col}, insufficient data\")\n",
    "        continue\n",
    "    \n",
    "    # Separate features and target for non-missing rows\n",
    "    X_train_missing = non_missing_rows.drop(columns=missing_columns)  # Exclude other missing columns from features\n",
    "    y_train_missing = non_missing_rows[col]  # Target is the column we're filling\n",
    "    \n",
    "    # Features for the rows with missing values (we'll predict the column for these rows)\n",
    "    X_test_missing = missing_rows.drop(columns=missing_columns)\n",
    "    \n",
    "    # Step 3: Preprocess the data (Standard Scaling)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_missing_scaled = scaler.fit_transform(X_train_missing)\n",
    "    X_test_missing_scaled = scaler.transform(X_test_missing)\n",
    "    \n",
    "    # Step 4: Build the ANN model for filling missing values\n",
    "    model_missing = Sequential()\n",
    "    model_missing.add(Input(shape=(X_train_missing_scaled.shape[1],)))  # Use Input layer instead of input_shape in Dense\n",
    "    model_missing.add(Dense(units=64, activation='relu'))\n",
    "    model_missing.add(Dropout(0.3))\n",
    "    model_missing.add(Dense(units=32, activation='relu'))\n",
    "    model_missing.add(Dropout(0.3))\n",
    "    model_missing.add(Dense(units=1, activation='linear'))  # Linear activation for regression tasks\n",
    "    \n",
    "    # Compile the model\n",
    "    model_missing.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Step 5: Train the model\n",
    "    model_missing.fit(X_train_missing_scaled, y_train_missing, epochs=50, batch_size=32, validation_split=0.1, verbose=0)\n",
    "    \n",
    "    # Step 6: Predict the missing values\n",
    "    predicted_values = model_missing.predict(X_test_missing_scaled)\n",
    "    \n",
    "    # Step 7: Fill the missing values in X_train\n",
    "    X_train.loc[X_train[col].isnull(), col] = predicted_values\n",
    "    \n",
    "    print(f\"Filled missing values in column: {col}\")\n",
    "\n",
    "# Verify if there are any remaining missing values in X_train\n",
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d49ec-15cb-482b-89bd-b835ac1470c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler only on the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformation to the validation and test data\n",
    "X_validate_scaled = scaler.transform(X_validate)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_external_scaled = scaler.transform(X_external)  # For external validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ff3fc-1402-4be1-9c3e-c9708a7447c5",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c11e82-238e-4877-978b-1dddf2d30a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ANN\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer (using Input(shape=...)) and first hidden layer\n",
    "model.add(Input(shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Second hidden layer with 32 neurons\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Third hidden layer with 16 neurons\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer with a single neuron for binary classification\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2543a929-47fa-4ebc-a875-495ed4998f09",
   "metadata": {},
   "source": [
    "## Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd2d7f-bf26-4a05-afda-418f8a7ef704",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640973d1-dd22-4067-a5dc-357437a8fef1",
   "metadata": {},
   "source": [
    "## Training the ANN\n",
    "\n",
    "EarlyStopping prevent overfitting by stopping training when the validation loss stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b3fa1-5c95-41d9-a3c9-9a3f4ddc5b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "\"\"\"\n",
    "Handle Class Imbalance:\n",
    "\n",
    "Class Weighting: During training penalize the\n",
    "misclassification of the minority class\n",
    "(non-survivors) more heavily.\n",
    "\"\"\"\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', \n",
    "                                                  classes=np.unique(y_train), \n",
    "                                                  y=y_train)\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    validation_data=(X_validate_scaled, y_validate),\n",
    "                    epochs=100, batch_size=32, \n",
    "                    callbacks=[early_stopping], \n",
    "                    class_weight={0: class_weights[0], 1: class_weights[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5afd3-376e-41b8-880d-7acadf9bdc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    validation_data=(X_validate_scaled, y_validate),\n",
    "                    epochs=100, batch_size=32, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be60365-d615-4f30-996b-11c8a06a51d7",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb587b0e-08d4-42a5-b94d-dc1264c7672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Evaluate on the external validation set\n",
    "external_loss, external_acc = model.evaluate(X_external_scaled, y_external)\n",
    "print(f\"External Validation Accuracy: {external_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81516790-1349-4dd0-a957-dc91fa42c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes on the test set\n",
    "#y_pred_test = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Classification report with zero_division parameter to handle undefined precision\n",
    "#print(classification_report(y_test, y_pred_test, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "#print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "\n",
    "# Predict probabilities instead of classes\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "\n",
    "# Adjust the threshold (e.g., 0.3)\n",
    "y_pred_adjusted = (y_pred_prob > 0.3).astype(\"int32\")\n",
    "\n",
    "# Classification report and confusion matrix with the adjusted threshold\n",
    "print(classification_report(y_test, y_pred_adjusted, zero_division=0))\n",
    "print(confusion_matrix(y_test, y_pred_adjusted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549629a-c078-469b-ae82-f78b77d43f58",
   "metadata": {},
   "source": [
    "# Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8b5a8-d672-41f0-b7ae-fdb09f0cb9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate rows with and without missing values in the 'Lactate' column (as an example)\n",
    "missing_rows = mimic_df[mimic_df['Lactate'].isnull()]\n",
    "non_missing_rows = mimic_df[~mimic_df['Lactate'].isnull()]\n",
    "\n",
    "# Separate features and target for non-missing rows\n",
    "X_train_missing = non_missing_rows.drop(columns=['Lactate', 'subject_id', 'hadm_id', 'row_count', 'hospital_expire_flag', 'los'])  # Features\n",
    "y_train_missing = non_missing_rows['Lactate']  # Target (Lactate)\n",
    "\n",
    "# Features for the rows with missing values (we'll predict Lactate for these)\n",
    "X_test_missing = missing_rows.drop(columns=['Lactate', 'subject_id', 'hadm_id', 'row_count', 'hospital_expire_flag', 'los'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79494e-91ae-40fe-af8f-89ae44f73227",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_test_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132befd-ae62-4db2-8606-95f2b8b22d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_missing_scaled = scaler.fit_transform(X_train_missing)\n",
    "X_test_missing_scaled = scaler.transform(X_test_missing)\n",
    "\n",
    "# Build the ANN model\n",
    "model_missing = Sequential()\n",
    "\n",
    "# Input layer and first hidden layer\n",
    "model_missing.add(Dense(units=64, activation='relu', input_shape=(X_train_missing_scaled.shape[1],)))\n",
    "model_missing.add(Dropout(0.3))\n",
    "\n",
    "# Second hidden layer\n",
    "model_missing.add(Dense(units=32, activation='relu'))\n",
    "model_missing.add(Dropout(0.3))\n",
    "\n",
    "# Output layer (predict Lactate)\n",
    "model_missing.add(Dense(units=1, activation='linear'))  # Linear activation for regression\n",
    "\n",
    "# Compile the model\n",
    "model_missing.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model_missing.fit(X_train_missing_scaled, y_train_missing, epochs=50, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e693d-f0c7-4ab8-a2e2-4d230def14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the missing Lactate values\n",
    "predicted_lactate = model_missing.predict(X_test_missing_scaled)\n",
    "\n",
    "# Fill the missing values back into the mimic_df\n",
    "mimic_df.loc[mimic_df['Lactate'].isnull(), 'Lactate'] = predicted_lactate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f7dcd-a135-413c-8d8a-05cfd0d12e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mimic_df.head(90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16537c70-afd7-4a37-a43f-c1223528fff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
