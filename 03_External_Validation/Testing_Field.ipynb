{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbbd189-fc26-490c-8597-aff82efff426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "\n",
    "\n",
    "#from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "\n",
    "#from keras.layers import Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd3c31-809e-47e7-ae6f-cb9cb74d9f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MIMICs CSV file\n",
    "mimic_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\mimic_mean_final.csv\")\n",
    "\n",
    "# Read eICUs CSV file\n",
    "eicu_df = pd.read_csv(\"CSV\\\\exports\\\\final\\\\eicu_mean_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df422077-0a58-4e6c-836b-b1bc9d3cb204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframes\n",
    "df_combined = pd.concat([mimic_df, eicu_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cec144-a9ff-4605-b381-107015f0c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all categorical columns in mimic\n",
    "categorical_columns = df_combined.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Apply one-hot encoding to all categorical columns\n",
    "df_encoded = pd.get_dummies(df_combined, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e842e2-6900-4abd-8e17-39f7762dcf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the concatenate dataframe\n",
    "mimic_df = df_encoded.iloc[:55792, :]  # Rows from 0 to 55791\n",
    "eicu_df = df_encoded.iloc[55792:, :]  # Rows from 55792 to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bde315-9442-4f4e-82c4-dbe00b3de105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by `subject_id` and `hadm_id` to get unique patient admission records\n",
    "unique_patients = mimic_df[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "\n",
    "# Split the unique patients into train, validation, and test sets\n",
    "train_patients, test_patients = train_test_split(unique_patients, test_size=0.10, random_state=42)\n",
    "train_patients, validate_patients = train_test_split(train_patients, test_size=0.11, random_state=42)  # 0.11 * 90% ~= 10%\n",
    "\n",
    "# Merge the patients back with the original data to get the full records\n",
    "train_set = mimic_df.merge(train_patients, on=['subject_id', 'hadm_id'])\n",
    "validate_set = mimic_df.merge(validate_patients, on=['subject_id', 'hadm_id'])\n",
    "test_set = mimic_df.merge(test_patients, on=['subject_id', 'hadm_id'])\n",
    "\n",
    "# External validation from eICU\n",
    "X_external = eicu_df.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_external = eicu_df['hospital_expire_flag']\n",
    "\n",
    "# Separate features and target for the training, validation, and test sets\n",
    "X_train = train_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_train = train_set['hospital_expire_flag']\n",
    "\n",
    "X_validate = validate_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_validate = validate_set['hospital_expire_flag']\n",
    "\n",
    "X_test = test_set.drop(columns=['hospital_expire_flag', 'los', 'subject_id', 'hadm_id', 'row_count', 'Time_Zone'])\n",
    "y_test = test_set['hospital_expire_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acfd6e-cfe6-4cb3-81ec-44d7917d0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "display (X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f00892-6735-4a02-91b9-387b7aa1e0b7",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- Fill empty cell in training set\n",
    "\n",
    "- StandardScaler, a popular preprocessing technique, offers a simple yet effective method for standardizing feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9709c-f8eb-4c48-911d-7fb43f4b7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill training set missing values\n",
    "\n",
    "# Step 1: Identify columns with missing values in X_train\n",
    "missing_columns = X_train.columns[X_train.isnull().any()].tolist()\n",
    "print(f\"Columns with missing values: {missing_columns}\")\n",
    "\n",
    "# Step 2: Loop through each column with missing values and build an ANN to predict missing values\n",
    "for col in missing_columns:\n",
    "    print(f\"Filling missing values in column: {col}\")\n",
    "    \n",
    "    # Separate rows with and without missing values in the current column\n",
    "    missing_rows = X_train[X_train[col].isnull()]\n",
    "    non_missing_rows = X_train[~X_train[col].isnull()]\n",
    "    \n",
    "    # Skip the column if no data is available for training\n",
    "    if len(missing_rows) == 0 or len(non_missing_rows) == 0:\n",
    "        print(f\"Skipping {col}, insufficient data\")\n",
    "        continue\n",
    "    \n",
    "    # Separate features and target for non-missing rows\n",
    "    X_train_missing = non_missing_rows.drop(columns=missing_columns)  # Exclude other missing columns from features\n",
    "    y_train_missing = non_missing_rows[col]  # Target is the column we're filling\n",
    "    \n",
    "    # Features for the rows with missing values (we'll predict the column for these rows)\n",
    "    X_test_missing = missing_rows.drop(columns=missing_columns)\n",
    "    \n",
    "    # Step 3: Preprocess the data (Standard Scaling)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_missing_scaled = scaler.fit_transform(X_train_missing)\n",
    "    X_test_missing_scaled = scaler.transform(X_test_missing)\n",
    "    \n",
    "    # Step 4: Build the ANN model for filling missing values\n",
    "    model_missing = Sequential()\n",
    "    model_missing.add(Input(shape=(X_train_missing_scaled.shape[1],)))  # Use Input layer instead of input_shape in Dense\n",
    "    model_missing.add(Dense(units=64, activation='relu'))\n",
    "    model_missing.add(Dropout(0.3))\n",
    "    model_missing.add(Dense(units=32, activation='relu'))\n",
    "    model_missing.add(Dropout(0.3))\n",
    "    model_missing.add(Dense(units=1, activation='linear'))  # Linear activation for regression tasks\n",
    "    \n",
    "    # Compile the model\n",
    "    model_missing.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Step 5: Train the model\n",
    "    model_missing.fit(X_train_missing_scaled, y_train_missing, epochs=50, batch_size=32, validation_split=0.1, verbose=0)\n",
    "    \n",
    "    # Step 6: Predict the missing values\n",
    "    predicted_values = model_missing.predict(X_test_missing_scaled)\n",
    "    \n",
    "    # Step 7: Fill the missing values in X_train\n",
    "    X_train.loc[X_train[col].isnull(), col] = predicted_values\n",
    "    \n",
    "    print(f\"Filled missing values in column: {col}\")\n",
    "\n",
    "# Verify if there are any remaining missing values in X_train\n",
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d49ec-15cb-482b-89bd-b835ac1470c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler only on the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformation to the validation and test data\n",
    "X_validate_scaled = scaler.transform(X_validate)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_external_scaled = scaler.transform(X_external)  # For external validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ff3fc-1402-4be1-9c3e-c9708a7447c5",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c11e82-238e-4877-978b-1dddf2d30a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ANN\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer (using Input(shape=...)) and first hidden layer\n",
    "model.add(Input(shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Second hidden layer with 32 neurons\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Third hidden layer with 16 neurons\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer with a single neuron for binary classification\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2543a929-47fa-4ebc-a875-495ed4998f09",
   "metadata": {},
   "source": [
    "## Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd2d7f-bf26-4a05-afda-418f8a7ef704",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640973d1-dd22-4067-a5dc-357437a8fef1",
   "metadata": {},
   "source": [
    "## Training the ANN\n",
    "\n",
    "EarlyStopping prevent overfitting by stopping training when the validation loss stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b3fa1-5c95-41d9-a3c9-9a3f4ddc5b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "\"\"\"\n",
    "Handle Class Imbalance:\n",
    "\n",
    "Class Weighting: During training penalize the\n",
    "misclassification of the minority class\n",
    "(non-survivors) more heavily.\n",
    "\"\"\"\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', \n",
    "                                                  classes=np.unique(y_train), \n",
    "                                                  y=y_train)\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    validation_data=(X_validate_scaled, y_validate),\n",
    "                    epochs=100, batch_size=32, \n",
    "                    callbacks=[early_stopping], \n",
    "                    class_weight={0: class_weights[0], 1: class_weights[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5afd3-376e-41b8-880d-7acadf9bdc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    validation_data=(X_validate_scaled, y_validate),\n",
    "                    epochs=100, batch_size=32, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be60365-d615-4f30-996b-11c8a06a51d7",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb587b0e-08d4-42a5-b94d-dc1264c7672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Evaluate on the external validation set\n",
    "external_loss, external_acc = model.evaluate(X_external_scaled, y_external)\n",
    "print(f\"External Validation Accuracy: {external_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81516790-1349-4dd0-a957-dc91fa42c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes on the test set\n",
    "#y_pred_test = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Classification report with zero_division parameter to handle undefined precision\n",
    "#print(classification_report(y_test, y_pred_test, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "#print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "\n",
    "# Predict probabilities instead of classes\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "\n",
    "# Adjust the threshold (e.g., 0.3)\n",
    "y_pred_adjusted = (y_pred_prob > 0.3).astype(\"int32\")\n",
    "\n",
    "# Classification report and confusion matrix with the adjusted threshold\n",
    "print(classification_report(y_test, y_pred_adjusted, zero_division=0))\n",
    "print(confusion_matrix(y_test, y_pred_adjusted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549629a-c078-469b-ae82-f78b77d43f58",
   "metadata": {},
   "source": [
    "# Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8b5a8-d672-41f0-b7ae-fdb09f0cb9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate rows with and without missing values in the 'Lactate' column (as an example)\n",
    "missing_rows = mimic_df[mimic_df['Lactate'].isnull()]\n",
    "non_missing_rows = mimic_df[~mimic_df['Lactate'].isnull()]\n",
    "\n",
    "# Separate features and target for non-missing rows\n",
    "X_train_missing = non_missing_rows.drop(columns=['Lactate', 'subject_id', 'hadm_id', 'row_count', 'hospital_expire_flag', 'los'])  # Features\n",
    "y_train_missing = non_missing_rows['Lactate']  # Target (Lactate)\n",
    "\n",
    "# Features for the rows with missing values (we'll predict Lactate for these)\n",
    "X_test_missing = missing_rows.drop(columns=['Lactate', 'subject_id', 'hadm_id', 'row_count', 'hospital_expire_flag', 'los'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79494e-91ae-40fe-af8f-89ae44f73227",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_test_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132befd-ae62-4db2-8606-95f2b8b22d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_missing_scaled = scaler.fit_transform(X_train_missing)\n",
    "X_test_missing_scaled = scaler.transform(X_test_missing)\n",
    "\n",
    "# Build the ANN model\n",
    "model_missing = Sequential()\n",
    "\n",
    "# Input layer and first hidden layer\n",
    "model_missing.add(Dense(units=64, activation='relu', input_shape=(X_train_missing_scaled.shape[1],)))\n",
    "model_missing.add(Dropout(0.3))\n",
    "\n",
    "# Second hidden layer\n",
    "model_missing.add(Dense(units=32, activation='relu'))\n",
    "model_missing.add(Dropout(0.3))\n",
    "\n",
    "# Output layer (predict Lactate)\n",
    "model_missing.add(Dense(units=1, activation='linear'))  # Linear activation for regression\n",
    "\n",
    "# Compile the model\n",
    "model_missing.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model_missing.fit(X_train_missing_scaled, y_train_missing, epochs=50, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e693d-f0c7-4ab8-a2e2-4d230def14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the missing Lactate values\n",
    "predicted_lactate = model_missing.predict(X_test_missing_scaled)\n",
    "\n",
    "# Fill the missing values back into the mimic_df\n",
    "mimic_df.loc[mimic_df['Lactate'].isnull(), 'Lactate'] = predicted_lactate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f7dcd-a135-413c-8d8a-05cfd0d12e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mimic_df.head(90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16537c70-afd7-4a37-a43f-c1223528fff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
