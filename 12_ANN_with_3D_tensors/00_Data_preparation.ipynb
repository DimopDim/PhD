{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190a95fd-5095-4c33-bebc-cc6c6c8dee2e",
   "metadata": {},
   "source": [
    "# ICU Data Loading and Cube Preparation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "543fcb3e-af79-445b-a01c-4faef7c4e361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis script automates the pipeline for loading, reshaping, validating, and preparing \\nclinical ICU data for deep learning models (ANN, RNN, etc.) that predict \\n**Length of Stay (LOS)** and **Mortality**.\\n\\nOutputs:\\n----------------------------------------------------------------------------------------------------\\n- numpy_cubes/train.npz + train.json\\n- numpy_cubes/validate.npz + validate.json\\n- numpy_cubes/test.npz + test.json\\n- numpy_cubes/external.npz + external.json\\n\\nQuick Peek Example:\\n----------------------------------------------------------------------------------------------------\\nBatch X: (32, 48, 345, 4), Batch y: (32,)\\n\\n====================================================================================================\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script automates the pipeline for loading, reshaping, validating, and preparing \n",
    "clinical ICU data for deep learning models (ANN, RNN, etc.) that predict \n",
    "**Length of Stay (LOS)** and **Mortality**.\n",
    "\n",
    "Outputs:\n",
    "----------------------------------------------------------------------------------------------------\n",
    "- numpy_cubes/train.npz + train.json\n",
    "- numpy_cubes/validate.npz + validate.json\n",
    "- numpy_cubes/test.npz + test.json\n",
    "- numpy_cubes/external.npz + external.json\n",
    "\n",
    "Quick Peek Example:\n",
    "----------------------------------------------------------------------------------------------------\n",
    "Batch X: (32, 48, 345, 4), Batch y: (32,)\n",
    "\n",
    "====================================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512b71a1-ef33-40bc-be07-aacd5c6ccc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44656d3c-89f4-4ea5-b827-aa0000c1cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"data_loading.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b27e035-288e-4938-aa5f-12c7fd4a8aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 12:53:00,166 - INFO - Loading... -> o1_X_external.csv\n",
      "2025-09-06 12:53:08,444 - INFO - Loading... -> o1_X_test.csv\n",
      "2025-09-06 12:53:09,073 - INFO - Loading... -> o1_X_train.csv\n",
      "2025-09-06 12:53:13,777 - INFO - Loading... -> o1_X_validate.csv\n",
      "2025-09-06 12:53:14,343 - INFO - Loading... -> o1_y_external_los.csv\n",
      "2025-09-06 12:53:14,397 - INFO - Loading... -> o1_y_external_mortality.csv\n",
      "2025-09-06 12:53:14,424 - INFO - Loading... -> o1_y_test_los.csv\n",
      "2025-09-06 12:53:14,433 - INFO - Loading... -> o1_y_test_mortality.csv\n",
      "2025-09-06 12:53:14,433 - INFO - Loading... -> o1_y_train_los.csv\n",
      "2025-09-06 12:53:14,478 - INFO - Loading... -> o1_y_train_mortality.csv\n",
      "2025-09-06 12:53:14,496 - INFO - Loading... -> o1_y_validate_los.csv\n",
      "2025-09-06 12:53:14,505 - INFO - Loading... -> o1_y_validate_mortality.csv\n",
      "2025-09-06 12:53:14,514 - INFO - Loading... -> o2_X_external.csv\n",
      "2025-09-06 12:53:18,571 - INFO - Loading... -> o2_X_test.csv\n",
      "2025-09-06 12:53:18,905 - INFO - Loading... -> o2_X_train.csv\n",
      "2025-09-06 12:53:21,118 - INFO - Loading... -> o2_X_validate.csv\n",
      "2025-09-06 12:53:21,415 - INFO - Loading... -> o2_y_external_los.csv\n",
      "2025-09-06 12:53:21,442 - INFO - Loading... -> o2_y_external_mortality.csv\n",
      "2025-09-06 12:53:21,460 - INFO - Loading... -> o2_y_test_los.csv\n",
      "2025-09-06 12:53:21,469 - INFO - Loading... -> o2_y_test_mortality.csv\n",
      "2025-09-06 12:53:21,469 - INFO - Loading... -> o2_y_train_los.csv\n",
      "2025-09-06 12:53:21,496 - INFO - Loading... -> o2_y_train_mortality.csv\n",
      "2025-09-06 12:53:21,505 - INFO - Loading... -> o2_y_validate_los.csv\n",
      "2025-09-06 12:53:21,514 - INFO - Loading... -> o2_y_validate_mortality.csv\n",
      "2025-09-06 12:53:21,514 - INFO - Loading... -> o3_X_external.csv\n",
      "2025-09-06 12:53:24,010 - INFO - Loading... -> o3_X_test.csv\n",
      "2025-09-06 12:53:24,217 - INFO - Loading... -> o3_X_train.csv\n",
      "2025-09-06 12:53:25,680 - INFO - Loading... -> o3_X_validate.csv\n",
      "2025-09-06 12:53:25,887 - INFO - Loading... -> o3_y_external_los.csv\n",
      "2025-09-06 12:53:25,914 - INFO - Loading... -> o3_y_external_mortality.csv\n",
      "2025-09-06 12:53:25,923 - INFO - Loading... -> o3_y_test_los.csv\n",
      "2025-09-06 12:53:25,932 - INFO - Loading... -> o3_y_test_mortality.csv\n",
      "2025-09-06 12:53:25,932 - INFO - Loading... -> o3_y_train_los.csv\n",
      "2025-09-06 12:53:25,950 - INFO - Loading... -> o3_y_train_mortality.csv\n",
      "2025-09-06 12:53:25,959 - INFO - Loading... -> o3_y_validate_los.csv\n",
      "2025-09-06 12:53:25,968 - INFO - Loading... -> o3_y_validate_mortality.csv\n",
      "2025-09-06 12:53:25,968 - INFO - Loading... -> o4_X_external.csv\n",
      "2025-09-06 12:53:28,033 - INFO - Loading... -> o4_X_test.csv\n",
      "2025-09-06 12:53:28,195 - INFO - Loading... -> o4_X_train.csv\n",
      "2025-09-06 12:53:29,329 - INFO - Loading... -> o4_X_validate.csv\n",
      "2025-09-06 12:53:29,518 - INFO - Loading... -> o4_y_external_los.csv\n",
      "2025-09-06 12:53:29,536 - INFO - Loading... -> o4_y_external_mortality.csv\n",
      "2025-09-06 12:53:29,545 - INFO - Loading... -> o4_y_test_los.csv\n",
      "2025-09-06 12:53:29,554 - INFO - Loading... -> o4_y_test_mortality.csv\n",
      "2025-09-06 12:53:29,554 - INFO - Loading... -> o4_y_train_los.csv\n",
      "2025-09-06 12:53:29,572 - INFO - Loading... -> o4_y_train_mortality.csv\n",
      "2025-09-06 12:53:29,581 - INFO - Loading... -> o4_y_validate_los.csv\n",
      "2025-09-06 12:53:29,581 - INFO - Loading... -> o4_y_validate_mortality.csv\n",
      "2025-09-06 12:53:29,590 - INFO - o1_X_external loaded successfully with shape (234720, 345)\n",
      "2025-09-06 12:53:29,590 - INFO - o1_X_test loaded successfully with shape (15312, 345)\n",
      "2025-09-06 12:53:29,590 - INFO - o1_X_train loaded successfully with shape (122496, 345)\n",
      "2025-09-06 12:53:29,590 - INFO - o1_X_validate loaded successfully with shape (15312, 345)\n",
      "2025-09-06 12:53:29,590 - INFO - o1_y_external_los loaded successfully with shape (234720, 1)\n",
      "2025-09-06 12:53:29,590 - INFO - o1_y_external_mortality loaded successfully with shape (234720, 1)\n",
      "2025-09-06 12:53:29,590 - INFO - o1_y_test_los loaded successfully with shape (15312, 1)\n",
      "2025-09-06 12:53:29,599 - INFO - o1_y_test_mortality loaded successfully with shape (15312, 1)\n",
      "2025-09-06 12:53:29,599 - INFO - o1_y_train_los loaded successfully with shape (122496, 1)\n",
      "2025-09-06 12:53:29,599 - INFO - o1_y_train_mortality loaded successfully with shape (122496, 1)\n",
      "2025-09-06 12:53:29,599 - INFO - o1_y_validate_los loaded successfully with shape (15312, 1)\n",
      "2025-09-06 12:53:29,599 - INFO - o1_y_validate_mortality loaded successfully with shape (15312, 1)\n",
      "2025-09-06 12:53:29,599 - INFO - o2_X_external loaded successfully with shape (117360, 345)\n",
      "2025-09-06 12:53:29,599 - INFO - o2_X_test loaded successfully with shape (7656, 345)\n",
      "2025-09-06 12:53:29,608 - INFO - o2_X_train loaded successfully with shape (61248, 345)\n",
      "2025-09-06 12:53:29,608 - INFO - o2_X_validate loaded successfully with shape (7656, 345)\n",
      "2025-09-06 12:53:29,608 - INFO - o2_y_external_los loaded successfully with shape (117360, 1)\n",
      "2025-09-06 12:53:29,608 - INFO - o2_y_external_mortality loaded successfully with shape (117360, 1)\n",
      "2025-09-06 12:53:29,608 - INFO - o2_y_test_los loaded successfully with shape (7656, 1)\n",
      "2025-09-06 12:53:29,608 - INFO - o2_y_test_mortality loaded successfully with shape (7656, 1)\n",
      "2025-09-06 12:53:29,608 - INFO - o2_y_train_los loaded successfully with shape (61248, 1)\n",
      "2025-09-06 12:53:29,608 - INFO - o2_y_train_mortality loaded successfully with shape (61248, 1)\n",
      "2025-09-06 12:53:29,617 - INFO - o2_y_validate_los loaded successfully with shape (7656, 1)\n",
      "2025-09-06 12:53:29,617 - INFO - o2_y_validate_mortality loaded successfully with shape (7656, 1)\n",
      "2025-09-06 12:53:29,617 - INFO - o3_X_external loaded successfully with shape (78240, 345)\n",
      "2025-09-06 12:53:29,617 - INFO - o3_X_test loaded successfully with shape (5104, 345)\n",
      "2025-09-06 12:53:29,617 - INFO - o3_X_train loaded successfully with shape (40832, 345)\n",
      "2025-09-06 12:53:29,617 - INFO - o3_X_validate loaded successfully with shape (5104, 345)\n",
      "2025-09-06 12:53:29,626 - INFO - o3_y_external_los loaded successfully with shape (78240, 1)\n",
      "2025-09-06 12:53:29,626 - INFO - o3_y_external_mortality loaded successfully with shape (78240, 1)\n",
      "2025-09-06 12:53:29,626 - INFO - o3_y_test_los loaded successfully with shape (5104, 1)\n",
      "2025-09-06 12:53:29,626 - INFO - o3_y_test_mortality loaded successfully with shape (5104, 1)\n",
      "2025-09-06 12:53:29,626 - INFO - o3_y_train_los loaded successfully with shape (40832, 1)\n",
      "2025-09-06 12:53:29,626 - INFO - o3_y_train_mortality loaded successfully with shape (40832, 1)\n",
      "2025-09-06 12:53:29,626 - INFO - o3_y_validate_los loaded successfully with shape (5104, 1)\n",
      "2025-09-06 12:53:29,635 - INFO - o3_y_validate_mortality loaded successfully with shape (5104, 1)\n",
      "2025-09-06 12:53:29,635 - INFO - o4_X_external loaded successfully with shape (58680, 345)\n",
      "2025-09-06 12:53:29,635 - INFO - o4_X_test loaded successfully with shape (3828, 345)\n",
      "2025-09-06 12:53:29,635 - INFO - o4_X_train loaded successfully with shape (30624, 345)\n",
      "2025-09-06 12:53:29,635 - INFO - o4_X_validate loaded successfully with shape (3828, 345)\n",
      "2025-09-06 12:53:29,635 - INFO - o4_y_external_los loaded successfully with shape (58680, 1)\n",
      "2025-09-06 12:53:29,635 - INFO - o4_y_external_mortality loaded successfully with shape (58680, 1)\n",
      "2025-09-06 12:53:29,644 - INFO - o4_y_test_los loaded successfully with shape (3828, 1)\n",
      "2025-09-06 12:53:29,644 - INFO - o4_y_test_mortality loaded successfully with shape (3828, 1)\n",
      "2025-09-06 12:53:29,644 - INFO - o4_y_train_los loaded successfully with shape (30624, 1)\n",
      "2025-09-06 12:53:29,644 - INFO - o4_y_train_mortality loaded successfully with shape (30624, 1)\n",
      "2025-09-06 12:53:29,644 - INFO - o4_y_validate_los loaded successfully with shape (3828, 1)\n",
      "2025-09-06 12:53:29,644 - INFO - o4_y_validate_mortality loaded successfully with shape (3828, 1)\n",
      "2025-09-06 12:53:29,644 - INFO - Load Complete.\n"
     ]
    }
   ],
   "source": [
    "path = \"CSV/Imports/original\"\n",
    "\n",
    "# Get all files in the directory\n",
    "all_files = os.listdir(path)\n",
    "\n",
    "# Store dictionary\n",
    "dataframes = {}\n",
    "\n",
    "# Load files one by one\n",
    "for file in all_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        # Create a variable name from the filename\n",
    "        var_name = file.replace(\".csv\", \"\").replace(\"-\", \"_\")\n",
    "        logging.info(f\"Loading... -> {file}\")\n",
    "        # Load the CSV into a pandas dataframe\n",
    "        dataframes[var_name] = pd.read_csv(os.path.join(path, file)).astype('float32')\n",
    "\n",
    "# Chech if they are load\n",
    "for var_name, df in dataframes.items():\n",
    "    globals()[var_name] = df  # Assign to global variables if needed\n",
    "    logging.info(f\"{var_name} loaded successfully with shape {df.shape}\")\n",
    "\n",
    "logging.info(\"Load Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e18b84d0-ea8e-4f65-a0ca-fbb2a8dc85b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 12:53:29,680 - INFO - Number of features: 345\n",
      "2025-09-06 12:53:30,868 - INFO - train: X_cube (2552, 48, 345, 4), ~644.9 MB, y_los (2552,), y_mort (2552,)\n",
      "2025-09-06 12:53:31,051 - INFO - validate: X_cube (319, 48, 345, 4), ~80.6 MB, y_los (319,), y_mort (319,)\n",
      "2025-09-06 12:53:31,195 - INFO - test: X_cube (319, 48, 345, 4), ~80.6 MB, y_los (319,), y_mort (319,)\n",
      "2025-09-06 12:53:34,046 - INFO - external: X_cube (4890, 48, 345, 4), ~1235.6 MB, y_los (4890,), y_mort (4890,)\n"
     ]
    }
   ],
   "source": [
    "# --- Config ---\n",
    "FEATURES = 345\n",
    "T_STEPS = {'o1': 48, 'o2': 24, 'o3': 16, 'o4': 12}\n",
    "UPSAMPLE = {'o1': 1, 'o2': 2, 'o3': 3, 'o4': 4}\n",
    "WINDOWS = ['o1', 'o2', 'o3', 'o4']\n",
    "SPLITS  = ['train', 'validate', 'test', 'external']\n",
    "\n",
    "# Save the feature names from one window (all are identical)\n",
    "FEATURE_NAMES = o1_X_train.columns.tolist()\n",
    "logging.info(f\"Number of features: {len(FEATURE_NAMES)}\")\n",
    "\n",
    "# --- Helpers ---\n",
    "def _reshape_X(dfX, t_steps, features=FEATURES):\n",
    "    rows = dfX.shape[0]\n",
    "    assert dfX.shape[1] == features, f\"Expected {features}, got {dfX.shape[1]}\"\n",
    "    assert rows % t_steps == 0, f\"Rows {rows} not divisible by {t_steps}\"\n",
    "    n_patients = rows // t_steps\n",
    "    return dfX.to_numpy(dtype=np.float32).reshape(n_patients, t_steps, features)\n",
    "\n",
    "def _reshape_y(dfY, t_steps):\n",
    "    y = dfY.to_numpy(dtype=np.float32).reshape(-1, t_steps, 1)\n",
    "    # verify consistency\n",
    "    if not np.allclose(y, y[:, :1, :]):\n",
    "        logging.warning(\"Some patients have non-constant target within block. Using first value.\")\n",
    "    return y[:, 0, 0]\n",
    "\n",
    "def _upsample_time(x3d, factor):\n",
    "    if factor == 1:\n",
    "        return x3d\n",
    "    return np.repeat(x3d, repeats=factor, axis=1)\n",
    "\n",
    "def build_split_cubes(split):\n",
    "    X_by_window = []\n",
    "    y_los = y_mort = None\n",
    "    n_patients_ref = None\n",
    "\n",
    "    for w in WINDOWS:\n",
    "        Xdf = globals()[f\"{w}_X_{split}\"]\n",
    "        Ylos = globals()[f\"{w}_y_{split}_los\"]\n",
    "        Ymor = globals()[f\"{w}_y_{split}_mortality\"]\n",
    "\n",
    "        Xw = _reshape_X(Xdf, T_STEPS[w], FEATURES)\n",
    "        yl = _reshape_y(Ylos, T_STEPS[w])\n",
    "        ym = _reshape_y(Ymor, T_STEPS[w])\n",
    "\n",
    "        Xw48 = _upsample_time(Xw, UPSAMPLE[w])  # (N,48,F)\n",
    "\n",
    "        if n_patients_ref is None:\n",
    "            n_patients_ref = Xw48.shape[0]\n",
    "            y_los, y_mort = yl, ym\n",
    "        else:\n",
    "            assert Xw48.shape[0] == n_patients_ref, \"Patient count mismatch\"\n",
    "            assert np.allclose(yl, y_los), \"LOS mismatch\"\n",
    "            assert np.allclose(ym, y_mort), \"Mortality mismatch\"\n",
    "\n",
    "        X_by_window.append(Xw48[..., np.newaxis])  # (N,48,F,1)\n",
    "\n",
    "    X_cube = np.concatenate(X_by_window, axis=-1).astype(np.float32)\n",
    "\n",
    "    mb = X_cube.size * 4 / (1024**2)\n",
    "    logging.info(f\"{split}: X_cube {X_cube.shape}, ~{mb:.1f} MB, y_los {y_los.shape}, y_mort {y_mort.shape}\")\n",
    "    \n",
    "    return {\n",
    "        \"X\": X_cube,\n",
    "        \"y_los\": y_los.astype(np.float32),\n",
    "        \"y_mort\": y_mort.astype(np.float32),\n",
    "        \"feature_names\": FEATURE_NAMES\n",
    "    }\n",
    "\n",
    "# --- Build all splits ---\n",
    "train_data = build_split_cubes('train')\n",
    "val_data   = build_split_cubes('validate')\n",
    "test_data  = build_split_cubes('test')\n",
    "ext_data   = build_split_cubes('external')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca56ead-477b-4532-ad4f-e38c9b71928b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names/order are identical across all windows & splits.\n",
      "Saved: numpy_cubes/{train,validate,test,external}.npz + .json\n",
      "Batch X: (32, 48, 345, 4) Batch y: (32,)\n"
     ]
    }
   ],
   "source": [
    "# 1) Sanity checks on features across windows/splits\n",
    "def check_feature_consistency():\n",
    "    problems = []\n",
    "    ref = o1_X_train.columns.tolist()\n",
    "    for w in ['o1','o2','o3','o4']:\n",
    "        for split in ['train','validate','test','external']:\n",
    "            cols = globals()[f\"{w}_X_{split}\"].columns.tolist()\n",
    "            if cols != ref:\n",
    "                problems.append((w, split))\n",
    "    if problems:\n",
    "        raise ValueError(f\"Feature name/order mismatch in: {problems}\")\n",
    "    print(\"Feature names/order are identical across all windows & splits.\")\n",
    "\n",
    "check_feature_consistency()\n",
    "\n",
    "# 2) Persist cubes + metadata to disk (compressed)\n",
    "os.makedirs(\"numpy_cubes\", exist_ok=True)\n",
    "\n",
    "def save_split(split_name, data_bundle):\n",
    "    # data_bundle is what build_split_cubes() returned earlier\n",
    "    X = data_bundle[\"X\"]\n",
    "    y_los = data_bundle[\"y_los\"]\n",
    "    y_mort = data_bundle[\"y_mort\"]\n",
    "    feature_names = data_bundle[\"feature_names\"]\n",
    "\n",
    "    np.savez_compressed(f\"numpy_cubes/{split_name}.npz\",\n",
    "                        X=X, y_los=y_los, y_mort=y_mort)\n",
    "    meta = {\n",
    "        \"split\": split_name,\n",
    "        \"shape\": tuple(int(v) for v in X.shape),\n",
    "        \"dtype\": str(X.dtype),\n",
    "        \"n_patients\": int(X.shape[0]),\n",
    "        \"timesteps\": int(X.shape[1]),\n",
    "        \"n_features\": int(X.shape[2]),\n",
    "        \"n_windows\": int(X.shape[3]),\n",
    "        \"feature_names\": feature_names\n",
    "    }\n",
    "    with open(f\"numpy_cubes/{split_name}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "save_split(\"train\",   train_data)\n",
    "save_split(\"validate\",val_data)\n",
    "save_split(\"test\",    test_data)\n",
    "save_split(\"external\",ext_data)\n",
    "\n",
    "print(\"Saved: numpy_cubes/{train,validate,test,external}.npz + .json\")\n",
    "\n",
    "# 3) Build a memory-safe TensorFlow dataset for ANN training\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_split_npz(split_name):\n",
    "    pack = np.load(f\"numpy_cubes/{split_name}.npz\")\n",
    "    return pack[\"X\"], pack[\"y_los\"], pack[\"y_mort\"]\n",
    "\n",
    "def make_tf_dataset(split_name, target=\"los\", batch_size=32, shuffle=True):\n",
    "    X, y_los, y_mort = load_split_npz(split_name)\n",
    "    y = y_los if target == \"los\" else y_mort\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(8192, X.shape[0]), reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# Example:\n",
    "train_ds_los = make_tf_dataset(\"train\", target=\"los\", batch_size=32, shuffle=True)\n",
    "val_ds_los   = make_tf_dataset(\"validate\", target=\"los\", batch_size=32, shuffle=False)\n",
    "\n",
    "# (Optional) quick peek to ensure shapes flow into the model:\n",
    "xb, yb = next(iter(train_ds_los))\n",
    "print(\"Batch X:\", xb.shape, \"Batch y:\", yb.shape)  # -> (B,48,345,4), (B,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
