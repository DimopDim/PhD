{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671463d-f9c0-4542-b28c-eec0dfb36e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a182eba9-001e-43e2-b928-7bdd96cf667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the stroke_diagnosis.csv file\n",
    "df = pd.read_csv(\"CSV\\imports\\o05_30_percent_filled_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de925e7b-1cdb-43d0-9eb4-b5d91aec3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter icu stay less than 10 days\n",
    "df = df[df['los'] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07bf1b7-547d-44bf-946e-d8afadc7be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47b5051-b8fb-4778-9cc2-a5ecbe315310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training percentage. The difference goes to test set\n",
    "training_percentage = 0.7\n",
    "\n",
    "# It's already sorted. Just for precaution. Sort by 'subject_id' and 'Time_Zone')\n",
    "df = df.sort_values(by=['subject_id', 'Time_Zone'])\n",
    "\n",
    "# Calculate the total number of unique subject IDs\n",
    "unique_subject_ids = df['subject_id'].nunique()\n",
    "\n",
    "# Calculate the number of unique subject IDs to include in the training set\n",
    "train_subject_ids_count = int(training_percentage * unique_subject_ids)\n",
    "\n",
    "# Initialize variables to track the number of subject IDs included in the training set\n",
    "subject_ids_in_training = 0\n",
    "\n",
    "# Initialize empty DataFrames for the training and test sets\n",
    "train_df = pd.DataFrame(columns=df.columns)\n",
    "test_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# Iterate through the sorted DataFrame\n",
    "for subject_id, subject_data in df.groupby('subject_id'):\n",
    "    if subject_ids_in_training < train_subject_ids_count:\n",
    "        # Add this subject's data to the training set\n",
    "        train_df = pd.concat([train_df, subject_data])\n",
    "        subject_ids_in_training += 1\n",
    "    else:\n",
    "        # Add this subject's data to the test set\n",
    "        test_df = pd.concat([test_df, subject_data])\n",
    "\n",
    "# Reset the index of the resulting DataFrames\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# I'm going to use those numbers as the split point in rapidminer filter operator\n",
    "display(\"The last row of the training set is -> \" + str(train_df.tail(1)[\"row_count\"].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eea57c-3f42-476c-ae67-10888d3ebcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train_df and test_df for consistent encoding of categorical variables\n",
    "combined_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['gender', 'language', 'marital_status', 'race']\n",
    "combined_df_encoded = pd.get_dummies(combined_df, columns=categorical_cols)\n",
    "\n",
    "# Convert 'age' column to numeric type\n",
    "combined_df_encoded['age'] = pd.to_numeric(combined_df_encoded['age'], errors='coerce')\n",
    "\n",
    "# Convert 'hospital_expire_flag' column to boolean type\n",
    "combined_df_encoded['hospital_expire_flag'] = combined_df_encoded['hospital_expire_flag'].astype(bool)\n",
    "\n",
    "\n",
    "# Split the dataframe at the original row index (before concatenation)\n",
    "combined_df_encoded_train = combined_df_encoded.iloc[:len(train_df)]\n",
    "combined_df_encoded_test = combined_df_encoded.iloc[len(train_df):]\n",
    "\n",
    "# Split data into features and target variable again\n",
    "X_train = combined_df_encoded_train.drop(['row_count', 'subject_id', 'hadm_id', 'Time_Zone', 'los'], axis=1)\n",
    "y_train = combined_df_encoded_train['los']\n",
    "X_test = combined_df_encoded_test.drop(['row_count', 'subject_id', 'hadm_id', 'Time_Zone', 'los'], axis=1)\n",
    "y_test = combined_df_encoded_test['los']\n",
    "\n",
    "# Train XGBoost model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "#model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.069)\n",
    "#model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.069, max_depth=6)\n",
    "#model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.069, max_depth=6, reg_lambda=4.7)\n",
    "#model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.069, max_depth=6, reg_lambda=4.7, reg_alpha=0.0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1edc9-0621-4568-89e6-b0121161b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "print(\"Mean Square Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error (RMSE):\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "r2 = r2_score(y_test, y_pred) * 100\n",
    "print(f\"R-squared (R2): {r2:.2f} %\")\n",
    "\n",
    "# MSLE calculation must not have negative values in y_test and y_pred\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Logarithmic Error (MSLE):\", msle)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5ee89-6e54-49fa-8e9b-40c5629f7ee6",
   "metadata": {},
   "source": [
    "# Most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f15d96-8eab-4129-9b21-4932c1701469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances along with their corresponding names\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n",
    "\n",
    "# Sort the DataFrame by feature importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the top N most important features\n",
    "top_n = 20  # set features number\n",
    "print(f\"Top {top_n} most important features:\")\n",
    "print(feature_importance_df.head(top_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b86e0f0-487e-4549-a183-11113684f3fb",
   "metadata": {},
   "source": [
    "# Stand Alone Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6ff9f-1aa9-4b92-bbb1-8e45e7a92f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing field\n",
    "\"\"\"\n",
    "\n",
    "cross_val=10,  # Number of folds in cross-validation\n",
    "score_metric='neg_mean_squared_error',  # Scoring metric\n",
    "verbosity=1,  # Controls the verbosity\n",
    "processors_jobs=-1)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': np.arange(0.01, 1.00, 0.01),  # Learning rate from 0.01 to 0.51 with step 0.01\n",
    "    #'learning_rate': [], # Learning rate set value\n",
    "    #------------------------------------------------\n",
    "    'max_depth': np.arange(1, 11, 1),  # Max depth from 1 to 10 with step 1\n",
    "    #'max_depth': [], # Max depth set value\n",
    "    #------------------------------------------------\n",
    "    'lambda': np.arange(0.0, 10.0, 0.1),  # L2 from 0.0 to 10.0 with step 0.1\n",
    "    #'lambda': [], # L2 set value\n",
    "    #------------------------------------------------\n",
    "    #'alpha': np.arange(0.0, 10.0, 0.1),  # L1 regularization from 0.0 to 10.0 with step 0.1\n",
    "    #'alpha': [], # L1 set value\n",
    "    #------------------------------------------------\n",
    "    #'n_estimators': np.arange(1, 100, 1), # Number of trees from 1 to 100 with step 1\n",
    "    #'n_estimators': [],  # Number of trees\n",
    "    #-------------------------------------------------\n",
    "    #'gamma': np.arange(0.0, 1.0, 0.1), # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    #'gamma': [0, 0.1, 0.2]  # Minimum loss reduction value\n",
    "}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror'),\n",
    "                            param_grid=param_grid,\n",
    "                            cv=cross_val,  # Number of folds in cross-validation\n",
    "                            scoring=score_metric,  # Scoring metric\n",
    "                            verbose=verbosity,  # Controls the verbosity\n",
    "                            n_jobs=processors_jobs)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search for best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on test set using the best model\n",
    "y_pred_best_stand_alone = best_model.predict(X_test)\n",
    "\n",
    "# Best model evaluation\n",
    "print(\"Mean Square Error (MSE):\", mean_squared_error(y_test, y_pred_best_stand_alone))\n",
    "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred_best_stand_alone))\n",
    "print(\"Root Mean Squared Error (RMSE):\", mean_squared_error(y_test, y_pred_best_stand_alone, squared=False))\n",
    "print(\"R-squared (R2):\", r2_score(y_test, y_pred_best_stand_alone))\n",
    "\n",
    "# MSLE calculation must not have negative values in y_test and y_pred\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_pred_best_stand_alone)\n",
    "    print(\"Mean Squared Logarithmic Error (MSLE):\", msle)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309165c-5a06-40e8-a91d-a1c0da0f3cd5",
   "metadata": {},
   "source": [
    "# Set all HyperParameters & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf1181-5225-41a0-8660-c739b744059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val=10,  # Number of folds in cross-validation\n",
    "score_metric='neg_mean_squared_error',  # Scoring metric\n",
    "verbosity=1,  # Controls the verbosity\n",
    "processors_jobs=-1)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Learning Rate HyperParameter\n",
    "param_grid_learning_rate = {\n",
    "    'learning_rate': np.arange(0.01, 1.01, 0.01),  # Learning rate from 0.01 to 1.00 with step 0.01\n",
    "}\n",
    "\n",
    "# Create a grid search object for learning rate\n",
    "grid_search_learning_rate = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror'),\n",
    "                            param_grid=param_grid_learning_rate,\n",
    "                            cv=cross_val,  # Number of folds in cross-validation\n",
    "                            scoring=score_metric,  # Scoring metric\n",
    "                            verbose=verbosity,  # Controls the verbosity\n",
    "                            n_jobs=processors_jobs)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search best learning rate\n",
    "grid_search_learning_rate.fit(X_train, y_train)\n",
    "\n",
    "# Best learning rate\n",
    "best_learning_rate = grid_search_learning_rate.best_params_['learning_rate']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Max Depth HyperParameter\n",
    "param_grid_max_depth = {\n",
    "    'max_depth': np.arange(1, 11, 1),  # Max depth from 1 to 10 with step 1\n",
    "}\n",
    "\n",
    "# Create a grid search object for max depth\n",
    "grid_search_max_depth = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate),\n",
    "                            param_grid=param_grid_max_depth,\n",
    "                            cv=cross_val,  # Number of folds in cross-validation\n",
    "                            scoring=score_metric,  # Scoring metric\n",
    "                            verbose=verbosity,  # Controls the verbosity\n",
    "                            n_jobs=processors_jobs)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search best max depth\n",
    "grid_search_max_depth.fit(X_train, y_train)\n",
    "\n",
    "# Best max depth\n",
    "best_max_depth = grid_search_max_depth.best_params_['max_depth']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# L2 HyperParameter\n",
    "param_grid_lambda = {\n",
    "    'reg_lambda': np.arange(0.0, 10.1, 0.1),  # Lambda (L2 regularization) from 0.0 to 10.0 with step 0.1\n",
    "}\n",
    "\n",
    "# Create a grid search object for lambda\n",
    "grid_search_lambda = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate, max_depth=best_max_depth),\n",
    "                            param_grid=param_grid_lambda,\n",
    "                            cv=cross_val,  # Number of folds in cross-validation\n",
    "                            scoring=score_metric,  # Scoring metric\n",
    "                            verbose=verbosity,  # Controls the verbosity\n",
    "                            n_jobs=processors_jobs)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search for best L2\n",
    "grid_search_lambda.fit(X_train, y_train)\n",
    "\n",
    "# Best L2\n",
    "best_lambda = grid_search_lambda.best_params_['reg_lambda']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# L1 HyperParameter\n",
    "param_grid_alpha = {\n",
    "    'reg_alpha': np.arange(0.0, 10.1, 0.1),  # Alpha (L1 regularization) from 0.0 to 10.0 with step 0.1\n",
    "}\n",
    "\n",
    "# Create a grid search object for alpha\n",
    "grid_search_alpha = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate, max_depth=best_max_depth, reg_lambda=best_lambda),\n",
    "                            param_grid=param_grid_alpha,\n",
    "                            cv=cross_val,  # Number of folds in cross-validation\n",
    "                            scoring=score_metric,  # Scoring metric\n",
    "                            verbose=verbosity,  # Controls the verbosity\n",
    "                            n_jobs=processors_jobs)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search for best L1\n",
    "grid_search_alpha.fit(X_train, y_train)\n",
    "\n",
    "# Best L1\n",
    "best_alpha = grid_search_alpha.best_params_['reg_alpha']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Gamma HyperParameter\n",
    "param_grid_gamma = {\n",
    "    'gamma': np.arange(0.0, 1.1, 0.1),  # Gamma from 0.0 to 1.0 with step 0.1\n",
    "}\n",
    "\n",
    "# Create a grid search object for gamma\n",
    "grid_search_gamma = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate, max_depth=best_max_depth, reg_lambda=best_lambda, alpha=best_alpha),\n",
    "                            param_grid=param_grid_gamma,\n",
    "                            cv=cross_val,  # Number of folds in cross-validation\n",
    "                            scoring=score_metric,  # Scoring metric\n",
    "                            verbose=verbosity,  # Controls the verbosity\n",
    "                            n_jobs=processors_jobs)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search for best gamma\n",
    "grid_search_gamma.fit(X_train, y_train)\n",
    "\n",
    "# Best gamma\n",
    "best_gamma = grid_search_gamma.best_params_['gamma']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Train the model using the best hyperparameter values\n",
    "best_model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate, max_depth=best_max_depth, reg_lambda=best_lambda, reg_alpha=best_alpha, gamma=best_gamma)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set using the best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Best model evaluation\n",
    "print(\"Mean Square Error (MSE):\", mean_squared_error(y_test, y_pred_best))\n",
    "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred_best))\n",
    "print(\"Root Mean Squared Error (RMSE):\", mean_squared_error(y_test, y_pred_best, squared=False))\n",
    "r2 = r2_score(y_test, y_pred_best) * 100\n",
    "print(f\"R-squared (R2): {r2:.2f} %\")\n",
    "\n",
    "# MSLE calculation must not have negative values in y_test and y_pred\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_pred_best)\n",
    "    print(\"Mean Squared Logarithmic Error (MSLE):\", msle)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d59b2-6802-4712-9319-159868527681",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best LR\", best_learning_rate)\n",
    "print(\"Best Max Depth\", best_max_depth)\n",
    "print(\"Best L2\", best_lambda)\n",
    "print(\"Best L1\", best_alpha)\n",
    "print(\"Best Gamma\", best_gamma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
