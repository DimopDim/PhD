{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6671463d-f9c0-4542-b28c-eec0dfb36e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a182eba9-001e-43e2-b928-7bdd96cf667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the stroke_diagnosis.csv file\n",
    "df = pd.read_csv(\"CSV\\imports\\o05_30_percent_filled_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de925e7b-1cdb-43d0-9eb4-b5d91aec3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named df\n",
    "df = df[df['los'] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e07bf1b7-547d-44bf-946e-d8afadc7be5e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_count</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>Time_Zone</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>language</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>race</th>\n",
       "      <th>Base Excess</th>\n",
       "      <th>...</th>\n",
       "      <th>CK-MB</th>\n",
       "      <th>Glucose.2</th>\n",
       "      <th>Potassium Whole Blood</th>\n",
       "      <th>Glucose (whole blood)</th>\n",
       "      <th>Potassium (whole blood)</th>\n",
       "      <th>Creatine Kinase MB Isoenzyme</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>los</th>\n",
       "      <th>GCS</th>\n",
       "      <th>Braden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8.357373</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8.357373</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8.357373</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8.357373</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10004733</td>\n",
       "      <td>27411876</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8.357373</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55787</th>\n",
       "      <td>55788</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>12</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.937847</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55788</th>\n",
       "      <td>55789</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>13</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.937847</td>\n",
       "      <td>8.111111</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55789</th>\n",
       "      <td>55790</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>14</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.937847</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55790</th>\n",
       "      <td>55791</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>15</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.937847</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55791</th>\n",
       "      <td>55792</td>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>16</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.937847</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48992 rows Ã— 232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_count  subject_id   hadm_id  Time_Zone gender  age language  \\\n",
       "0              1    10004733  27411876          1      M   51  ENGLISH   \n",
       "1              2    10004733  27411876          2      M   51  ENGLISH   \n",
       "2              3    10004733  27411876          3      M   51  ENGLISH   \n",
       "3              4    10004733  27411876          4      M   51  ENGLISH   \n",
       "4              5    10004733  27411876          5      M   51  ENGLISH   \n",
       "...          ...         ...       ...        ...    ...  ...      ...   \n",
       "55787      55788    19999987  23865745         12      F   57  ENGLISH   \n",
       "55788      55789    19999987  23865745         13      F   57  ENGLISH   \n",
       "55789      55790    19999987  23865745         14      F   57  ENGLISH   \n",
       "55790      55791    19999987  23865745         15      F   57  ENGLISH   \n",
       "55791      55792    19999987  23865745         16      F   57  ENGLISH   \n",
       "\n",
       "      marital_status     race  Base Excess  ...  CK-MB  Glucose.2  \\\n",
       "0             SINGLE  UNKNOWN          0.0  ...    NaN        NaN   \n",
       "1             SINGLE  UNKNOWN          0.0  ...    NaN        NaN   \n",
       "2             SINGLE  UNKNOWN          0.0  ...    NaN        NaN   \n",
       "3             SINGLE  UNKNOWN          0.0  ...    NaN        NaN   \n",
       "4             SINGLE  UNKNOWN          0.0  ...    NaN        NaN   \n",
       "...              ...      ...          ...  ...    ...        ...   \n",
       "55787            NaN  UNKNOWN          1.0  ...   43.0        NaN   \n",
       "55788            NaN  UNKNOWN          1.0  ...   45.5        NaN   \n",
       "55789            NaN  UNKNOWN          1.0  ...   43.0        NaN   \n",
       "55790            NaN  UNKNOWN          1.0  ...   44.0        NaN   \n",
       "55791            NaN  UNKNOWN          1.0  ...   43.0        NaN   \n",
       "\n",
       "       Potassium Whole Blood  Glucose (whole blood)  Potassium (whole blood)  \\\n",
       "0                        NaN                    NaN                      NaN   \n",
       "1                        NaN                    NaN                      NaN   \n",
       "2                        NaN                    NaN                      NaN   \n",
       "3                        NaN                    NaN                      NaN   \n",
       "4                        NaN                    NaN                      NaN   \n",
       "...                      ...                    ...                      ...   \n",
       "55787                    NaN                    NaN                      NaN   \n",
       "55788                    NaN                    NaN                      NaN   \n",
       "55789                    NaN                    NaN                      NaN   \n",
       "55790                    NaN                    NaN                      NaN   \n",
       "55791                    NaN                    NaN                      NaN   \n",
       "\n",
       "       Creatine Kinase MB Isoenzyme  hospital_expire_flag       los       GCS  \\\n",
       "0                               NaN                     0  8.357373  8.000000   \n",
       "1                               NaN                     0  8.357373  8.500000   \n",
       "2                               NaN                     0  8.357373  8.000000   \n",
       "3                               NaN                     0  8.357373  8.333333   \n",
       "4                               NaN                     0  8.357373  8.333333   \n",
       "...                             ...                   ...       ...       ...   \n",
       "55787                          43.0                     0  1.937847  8.250000   \n",
       "55788                          45.5                     0  1.937847  8.111111   \n",
       "55789                          43.0                     0  1.937847  8.250000   \n",
       "55790                          44.0                     0  1.937847  7.000000   \n",
       "55791                          43.0                     0  1.937847  8.250000   \n",
       "\n",
       "       Braden  \n",
       "0        11.0  \n",
       "1        11.0  \n",
       "2        11.0  \n",
       "3        11.0  \n",
       "4        11.0  \n",
       "...       ...  \n",
       "55787    12.8  \n",
       "55788    12.5  \n",
       "55789    12.8  \n",
       "55790    13.0  \n",
       "55791    12.8  \n",
       "\n",
       "[48992 rows x 232 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b47b5051-b8fb-4778-9cc2-a5ecbe315310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimop\\AppData\\Local\\Temp\\ipykernel_8244\\121497339.py:24: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train_df = pd.concat([train_df, subject_data])\n",
      "C:\\Users\\dimop\\AppData\\Local\\Temp\\ipykernel_8244\\121497339.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_df = pd.concat([test_df, subject_data])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The last row of the training set is -> 39008'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set training percentage. The difference goes to test set\n",
    "training_percentage = 0.7\n",
    "\n",
    "# It's already sorted. Just for precaution. Sort by 'subject_id' and 'Time_Zone')\n",
    "df = df.sort_values(by=['subject_id', 'Time_Zone'])\n",
    "\n",
    "# Calculate the total number of unique subject IDs\n",
    "unique_subject_ids = df['subject_id'].nunique()\n",
    "\n",
    "# Calculate the number of unique subject IDs to include in the training set\n",
    "train_subject_ids_count = int(training_percentage * unique_subject_ids)\n",
    "\n",
    "# Initialize variables to track the number of subject IDs included in the training set\n",
    "subject_ids_in_training = 0\n",
    "\n",
    "# Initialize empty DataFrames for the training and test sets\n",
    "train_df = pd.DataFrame(columns=df.columns)\n",
    "test_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# Iterate through the sorted DataFrame\n",
    "for subject_id, subject_data in df.groupby('subject_id'):\n",
    "    if subject_ids_in_training < train_subject_ids_count:\n",
    "        # Add this subject's data to the training set\n",
    "        train_df = pd.concat([train_df, subject_data])\n",
    "        subject_ids_in_training += 1\n",
    "    else:\n",
    "        # Add this subject's data to the test set\n",
    "        test_df = pd.concat([test_df, subject_data])\n",
    "\n",
    "# Reset the index of the resulting DataFrames\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# I'm going to use those numbers as the split point in rapidminer filter operator\n",
    "display(\"The last row of the training set is -> \" + str(train_df.tail(1)[\"row_count\"].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6eea57c-3f42-476c-ae67-10888d3ebcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train_df and test_df for consistent encoding of categorical variables\n",
    "combined_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['gender', 'language', 'marital_status', 'race']\n",
    "combined_df_encoded = pd.get_dummies(combined_df, columns=categorical_cols)\n",
    "\n",
    "# Convert 'age' column to numeric type\n",
    "combined_df_encoded['age'] = pd.to_numeric(combined_df_encoded['age'], errors='coerce')\n",
    "\n",
    "# Convert 'hospital_expire_flag' column to boolean type\n",
    "combined_df_encoded['hospital_expire_flag'] = combined_df_encoded['hospital_expire_flag'].astype(bool)\n",
    "\n",
    "\n",
    "# Split the dataframe at the original row index (before concatenation)\n",
    "combined_df_encoded_train = combined_df_encoded.iloc[:len(train_df)]\n",
    "combined_df_encoded_test = combined_df_encoded.iloc[len(train_df):]\n",
    "\n",
    "# Split data into features and target variable again\n",
    "X_train = combined_df_encoded_train.drop(['row_count', 'subject_id', 'hadm_id', 'Time_Zone', 'los'], axis=1)\n",
    "y_train = combined_df_encoded_train['los']\n",
    "X_test = combined_df_encoded_test.drop(['row_count', 'subject_id', 'hadm_id', 'Time_Zone', 'los'], axis=1)\n",
    "y_test = combined_df_encoded_test['los']\n",
    "\n",
    "# Train XGBoost model\n",
    "#model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.069)\n",
    "#model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.7, max_depth=6)\n",
    "#model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.7, max_depth=6, reg_lambda=3.7)\n",
    "#model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.7, max_depth=6, reg_lambda=3.7, reg_alpha=0.1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75d1edc9-0621-4568-89e6-b0121161b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error (MSE): 2.420547587842547\n",
      "Mean Absolute Error (MAE): 1.0985796399938885\n",
      "Root Mean Squared Error (RMSE): 1.5558109100538366\n",
      "Mean Squared Logarithmic Error (MSLE): 0.12038340984295598\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "print(\"Mean Square Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error (RMSE):\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "# MSLE calculation must not have negative values in y_test and y_pred\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Logarithmic Error (MSLE):\", msle)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5ee89-6e54-49fa-8e9b-40c5629f7ee6",
   "metadata": {},
   "source": [
    "# Most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f15d96-8eab-4129-9b21-4932c1701469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most important features:\n",
      "                                            Feature  Importance\n",
      "92                                    Spont Vt (mL)    0.116994\n",
      "45                                             pH.1    0.050216\n",
      "202      Arterial Blood Pressure Alarm - Low (mmHg)    0.045596\n",
      "153                   High risk (>51) interventions    0.034696\n",
      "121                                        Eye Care    0.033455\n",
      "138                              ICU Consent Signed    0.030525\n",
      "9                                         Albumin.1    0.025447\n",
      "98                  Tidal Volume (spontaneous) (mL)    0.024918\n",
      "72                                    PH (dipstick)    0.023688\n",
      "208                    Arterial Line Zero/Calibrate    0.020562\n",
      "5                                                pH    0.017388\n",
      "183                             Pain Level Response    0.017156\n",
      "198                    20 Gauge placed in the field    0.016680\n",
      "46                                 Specific Gravity    0.014439\n",
      "70                                              ALT    0.014414\n",
      "206                                    Free Calcium    0.013929\n",
      "94                                Daily Weight (kg)    0.013824\n",
      "258                                      race_WHITE    0.013270\n",
      "251  race_NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER    0.011804\n",
      "142                                   Height (Inch)    0.011737\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances along with their corresponding names\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n",
    "\n",
    "# Sort the DataFrame by feature importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the top N most important features\n",
    "top_n = 20  # Change this value to display more or fewer top features\n",
    "print(f\"Top {top_n} most important features:\")\n",
    "print(feature_importance_df.head(top_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b86e0f0-487e-4549-a183-11113684f3fb",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c6ff9f-1aa9-4b92-bbb1-8e45e7a92f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n",
      "Best Hyperparameters: {'learning_rate': 0.06999999999999999}\n",
      "Mean Square Error (MSE): 2.3923009896945806\n",
      "Mean Absolute Error (MAE): 1.0991349113100208\n",
      "Root Mean Squared Error (RMSE): 1.5467064975924103\n",
      "R-squared (R2): 0.5852396136150441\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing field\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': np.arange(0.01, 1.00, 0.01),  # Learning rate from 0.01 to 0.51 with step 0.01\n",
    "    #'learning_rate': [], # Learning rate set value\n",
    "    #------------------------------------------------\n",
    "    #'max_depth': np.arange(1, 11, 1),  # Max depth from 1 to 10 with step 1\n",
    "    #'max_depth': [], # Max depth set value\n",
    "    #------------------------------------------------\n",
    "    #'lambda': np.arange(0.0, 10.0, 0.1),  # L2 from 0.0 to 10.0 with step 0.1\n",
    "    #'lambda': [], # L2 set value\n",
    "    #------------------------------------------------\n",
    "    #'alpha': np.arange(0.0, 10.0, 0.1),  # L1 regularization from 0.0 to 10.0 with step 0.1\n",
    "    #'alpha': [], # L1 set value\n",
    "    #------------------------------------------------\n",
    "    #'n_estimators': np.arange(1, 100, 1), # Number of trees from 1 to 100 with step 1\n",
    "    #'n_estimators': [],  # Number of trees\n",
    "    #-------------------------------------------------\n",
    "    #'gamma': np.arange(0.0, 1.0, 0.1), # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    #'gamma': [0, 0.1, 0.2]  # Minimum loss reduction value\n",
    "}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror'),\n",
    "                            param_grid=param_grid,\n",
    "                            cv=5,  # Number of folds in cross-validation\n",
    "                            scoring='neg_mean_squared_error',  # Scoring metric\n",
    "                            verbose=1,  # Controls the verbosity\n",
    "                            n_jobs=-1)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search for best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on test set using the best model\n",
    "y_pred_best_stand_alone = best_model.predict(X_test)\n",
    "\n",
    "# Best model evaluation\n",
    "print(\"Mean Square Error (MSE):\", mean_squared_error(y_test, y_pred_best_stand_alone))\n",
    "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred_best_stand_alone))\n",
    "print(\"Root Mean Squared Error (RMSE):\", mean_squared_error(y_test, y_pred_best_stand_alone, squared=False))\n",
    "print(\"R-squared (R2):\", r2_score(y_test, y_pred_best_stand_alone))\n",
    "\n",
    "# MSLE calculation must not have negative values in y_test and y_pred\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_pred_best_stand_alone)\n",
    "    print(\"Mean Squared Logarithmic Error (MSLE):\", msle)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf1181-5225-41a0-8660-c739b744059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate HyperParameter\n",
    "param_grid_learning_rate = {\n",
    "    'learning_rate': np.arange(0.01, 1.01, 0.01),  # Learning rate from 0.01 to 1.00 with step 0.01\n",
    "}\n",
    "\n",
    "# Create a grid search object for learning rate\n",
    "grid_search_learning_rate = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror'),\n",
    "                            param_grid=param_grid_learning_rate,\n",
    "                            cv=5,  # Number of folds in cross-validation\n",
    "                            scoring='neg_mean_squared_error',  # Scoring metric\n",
    "                            verbose=1,  # Controls the verbosity\n",
    "                            n_jobs=-1)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search best learning rate\n",
    "grid_search_learning_rate.fit(X_train, y_train)\n",
    "\n",
    "# Best learning rate\n",
    "best_learning_rate = grid_search_learning_rate.best_params_['learning_rate']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Max Depth HyperParameter\n",
    "param_grid_max_depth = {\n",
    "    'max_depth': np.arange(1, 11, 1),  # Max depth from 1 to 10 with step 1\n",
    "}\n",
    "\n",
    "# Create a grid search object for max depth\n",
    "grid_search_max_depth = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate),\n",
    "                            param_grid=param_grid_max_depth,\n",
    "                            cv=5,  # Number of folds in cross-validation\n",
    "                            scoring='neg_mean_squared_error',  # Scoring metric\n",
    "                            verbose=1,  # Controls the verbosity\n",
    "                            n_jobs=-1)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search best max depth\n",
    "grid_search_max_depth.fit(X_train, y_train)\n",
    "\n",
    "# Best max depth\n",
    "best_max_depth = grid_search_max_depth.best_params_['max_depth']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# L2 HyperParameter\n",
    "param_grid_lambda = {\n",
    "    'lambda': np.arange(0.0, 10.1, 0.1),  # Lambda (L2 regularization) from 0.0 to 10.0 with step 0.1\n",
    "}\n",
    "\n",
    "# Create a grid search object for lambda\n",
    "grid_search_lambda = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate, max_depth=best_max_depth),\n",
    "                            param_grid=param_grid_lambda,\n",
    "                            cv=5,  # Number of folds in cross-validation\n",
    "                            scoring='neg_mean_squared_error',  # Scoring metric\n",
    "                            verbose=1,  # Controls the verbosity\n",
    "                            n_jobs=-1)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search for best L2\n",
    "grid_search_lambda.fit(X_train, y_train)\n",
    "\n",
    "# Best L2\n",
    "best_lambda = grid_search_lambda.best_params_['lambda']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# L1 HyperParameter\n",
    "param_grid_alpha = {\n",
    "    'alpha': np.arange(0.0, 10.1, 0.1),  # Alpha (L1 regularization) from 0.0 to 10.0 with step 0.1\n",
    "}\n",
    "\n",
    "# Create a grid search object for alpha\n",
    "grid_search_alpha = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate, max_depth=best_max_depth, lambda=best_lambda),\n",
    "                            param_grid=param_grid_alpha,\n",
    "                            cv=5,  # Number of folds in cross-validation\n",
    "                            scoring='neg_mean_squared_error',  # Scoring metric\n",
    "                            verbose=1,  # Controls the verbosity\n",
    "                            n_jobs=-1)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search for best L1\n",
    "grid_search_alpha.fit(X_train, y_train)\n",
    "\n",
    "# Best L1\n",
    "best_alpha = grid_search_alpha.best_params_['alpha']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Gamma HyperParameter\n",
    "param_grid_gamma = {\n",
    "    'gamma': np.arange(0.0, 1.1, 0.1),  # Gamma from 0.0 to 1.0 with step 0.1\n",
    "}\n",
    "\n",
    "# Create a grid search object for gamma\n",
    "grid_search_gamma = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate, max_depth=best_max_depth, lambda=best_lambda, alpha=best_alpha),\n",
    "                            param_grid=param_grid_gamma,\n",
    "                            cv=5,  # Number of folds in cross-validation\n",
    "                            scoring='neg_mean_squared_error',  # Scoring metric\n",
    "                            verbose=1,  # Controls the verbosity\n",
    "                            n_jobs=-1)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search for best gamma\n",
    "grid_search_gamma.fit(X_train, y_train)\n",
    "\n",
    "# Best gamma\n",
    "best_gamma = grid_search_gamma.best_params_['gamma']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Train the model using the best hyperparameter values\n",
    "best_model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate, max_depth=best_max_depth, lambda=best_lambda, alpha=best_alpha, gamma=best_gamma)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set using the best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Best model evaluation\n",
    "print(\"Mean Square Error (MSE):\", mean_squared_error(y_test, y_pred_best))\n",
    "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred_best))\n",
    "print(\"Root Mean Squared Error (RMSE):\", mean_squared_error(y_test, y_pred_best, squared=False))\n",
    "print(\"R-squared (R2):\", r2_score(y_test, y_pred_best))\n",
    "\n",
    "\n",
    "# MSLE calculation must not have negative values in y_test and y_pred\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_pred_best)\n",
    "    print(\"Mean Squared Logarithmic Error (MSLE):\", msle)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
