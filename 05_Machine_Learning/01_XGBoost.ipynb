{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6671463d-f9c0-4542-b28c-eec0dfb36e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a182eba9-001e-43e2-b928-7bdd96cf667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the stroke_diagnosis.csv file\n",
    "df = pd.read_csv(\"CSV\\imports\\o05_30_percent_filled_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de925e7b-1cdb-43d0-9eb4-b5d91aec3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter icu stay less than 10 days\n",
    "df = df[df['los'] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07bf1b7-547d-44bf-946e-d8afadc7be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47b5051-b8fb-4778-9cc2-a5ecbe315310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimopoulos\\AppData\\Local\\Temp\\ipykernel_980\\121497339.py:24: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train_df = pd.concat([train_df, subject_data])\n",
      "C:\\Users\\Dimopoulos\\AppData\\Local\\Temp\\ipykernel_980\\121497339.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_df = pd.concat([test_df, subject_data])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The last row of the training set is -> 39008'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set training percentage. The difference goes to test set\n",
    "training_percentage = 0.7\n",
    "\n",
    "# It's already sorted. Just for precaution. Sort by 'subject_id' and 'Time_Zone')\n",
    "df = df.sort_values(by=['subject_id', 'Time_Zone'])\n",
    "\n",
    "# Calculate the total number of unique subject IDs\n",
    "unique_subject_ids = df['subject_id'].nunique()\n",
    "\n",
    "# Calculate the number of unique subject IDs to include in the training set\n",
    "train_subject_ids_count = int(training_percentage * unique_subject_ids)\n",
    "\n",
    "# Initialize variables to track the number of subject IDs included in the training set\n",
    "subject_ids_in_training = 0\n",
    "\n",
    "# Initialize empty DataFrames for the training and test sets\n",
    "train_df = pd.DataFrame(columns=df.columns)\n",
    "test_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# Iterate through the sorted DataFrame\n",
    "for subject_id, subject_data in df.groupby('subject_id'):\n",
    "    if subject_ids_in_training < train_subject_ids_count:\n",
    "        # Add this subject's data to the training set\n",
    "        train_df = pd.concat([train_df, subject_data])\n",
    "        subject_ids_in_training += 1\n",
    "    else:\n",
    "        # Add this subject's data to the test set\n",
    "        test_df = pd.concat([test_df, subject_data])\n",
    "\n",
    "# Reset the index of the resulting DataFrames\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# I'm going to use those numbers as the split point in rapidminer filter operator\n",
    "display(\"The last row of the training set is -> \" + str(train_df.tail(1)[\"row_count\"].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6eea57c-3f42-476c-ae67-10888d3ebcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train_df and test_df for consistent encoding of categorical variables\n",
    "combined_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['gender', 'language', 'marital_status', 'race']\n",
    "combined_df_encoded = pd.get_dummies(combined_df, columns=categorical_cols)\n",
    "\n",
    "# Convert 'age' column to numeric type\n",
    "combined_df_encoded['age'] = pd.to_numeric(combined_df_encoded['age'], errors='coerce')\n",
    "\n",
    "# Convert 'hospital_expire_flag' column to boolean type\n",
    "combined_df_encoded['hospital_expire_flag'] = combined_df_encoded['hospital_expire_flag'].astype(bool)\n",
    "\n",
    "\n",
    "# Split the dataframe at the original row index (before concatenation)\n",
    "combined_df_encoded_train = combined_df_encoded.iloc[:len(train_df)]\n",
    "combined_df_encoded_test = combined_df_encoded.iloc[len(train_df):]\n",
    "\n",
    "# Split data into features and target variable again\n",
    "X_train = combined_df_encoded_train.drop(['row_count', 'subject_id', 'hadm_id', 'Time_Zone', 'los'], axis=1)\n",
    "y_train = combined_df_encoded_train['los']\n",
    "X_test = combined_df_encoded_test.drop(['row_count', 'subject_id', 'hadm_id', 'Time_Zone', 'los'], axis=1)\n",
    "y_test = combined_df_encoded_test['los']\n",
    "\n",
    "# Train XGBoost model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "#model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.069)\n",
    "#model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.069, max_depth=6)\n",
    "#model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.069, max_depth=6, reg_lambda=4.7)\n",
    "#model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.069, max_depth=6, reg_lambda=4.7, reg_alpha=0.0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75d1edc9-0621-4568-89e6-b0121161b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error (MSE): 2.7111381547239803\n",
      "Mean Absolute Error (MAE): 1.177213571688383\n",
      "Root Mean Squared Error (RMSE): 1.6465534169057439\n",
      "Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "print(\"Mean Square Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error (RMSE):\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "# MSLE calculation must not have negative values in y_test and y_pred\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Logarithmic Error (MSLE):\", msle)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5ee89-6e54-49fa-8e9b-40c5629f7ee6",
   "metadata": {},
   "source": [
    "# Most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f15d96-8eab-4129-9b21-4932c1701469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most important features:\n",
      "                                            Feature  Importance\n",
      "92                                    Spont Vt (mL)    0.116994\n",
      "45                                             pH.1    0.050216\n",
      "202      Arterial Blood Pressure Alarm - Low (mmHg)    0.045596\n",
      "153                   High risk (>51) interventions    0.034696\n",
      "121                                        Eye Care    0.033455\n",
      "138                              ICU Consent Signed    0.030525\n",
      "9                                         Albumin.1    0.025447\n",
      "98                  Tidal Volume (spontaneous) (mL)    0.024918\n",
      "72                                    PH (dipstick)    0.023688\n",
      "208                    Arterial Line Zero/Calibrate    0.020562\n",
      "5                                                pH    0.017388\n",
      "183                             Pain Level Response    0.017156\n",
      "198                    20 Gauge placed in the field    0.016680\n",
      "46                                 Specific Gravity    0.014439\n",
      "70                                              ALT    0.014414\n",
      "206                                    Free Calcium    0.013929\n",
      "94                                Daily Weight (kg)    0.013824\n",
      "258                                      race_WHITE    0.013270\n",
      "251  race_NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER    0.011804\n",
      "142                                   Height (Inch)    0.011737\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances along with their corresponding names\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n",
    "\n",
    "# Sort the DataFrame by feature importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the top N most important features\n",
    "top_n = 20  # Change this value to display more or fewer top features\n",
    "print(f\"Top {top_n} most important features:\")\n",
    "print(feature_importance_df.head(top_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b86e0f0-487e-4549-a183-11113684f3fb",
   "metadata": {},
   "source": [
    "# Stand Alone Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6ff9f-1aa9-4b92-bbb1-8e45e7a92f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing field\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': np.arange(0.01, 1.00, 0.01),  # Learning rate from 0.01 to 0.51 with step 0.01\n",
    "    #'learning_rate': [], # Learning rate set value\n",
    "    #------------------------------------------------\n",
    "    #'max_depth': np.arange(1, 11, 1),  # Max depth from 1 to 10 with step 1\n",
    "    #'max_depth': [], # Max depth set value\n",
    "    #------------------------------------------------\n",
    "    #'lambda': np.arange(0.0, 10.0, 0.1),  # L2 from 0.0 to 10.0 with step 0.1\n",
    "    #'lambda': [], # L2 set value\n",
    "    #------------------------------------------------\n",
    "    #'alpha': np.arange(0.0, 10.0, 0.1),  # L1 regularization from 0.0 to 10.0 with step 0.1\n",
    "    #'alpha': [], # L1 set value\n",
    "    #------------------------------------------------\n",
    "    #'n_estimators': np.arange(1, 100, 1), # Number of trees from 1 to 100 with step 1\n",
    "    #'n_estimators': [],  # Number of trees\n",
    "    #-------------------------------------------------\n",
    "    #'gamma': np.arange(0.0, 1.0, 0.1), # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    #'gamma': [0, 0.1, 0.2]  # Minimum loss reduction value\n",
    "}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror'),\n",
    "                            param_grid=param_grid,\n",
    "                            cv=5,  # Number of folds in cross-validation\n",
    "                            scoring='neg_mean_squared_error',  # Scoring metric\n",
    "                            verbose=1,  # Controls the verbosity\n",
    "                            n_jobs=-1)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search for best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on test set using the best model\n",
    "y_pred_best_stand_alone = best_model.predict(X_test)\n",
    "\n",
    "# Best model evaluation\n",
    "print(\"Mean Square Error (MSE):\", mean_squared_error(y_test, y_pred_best_stand_alone))\n",
    "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred_best_stand_alone))\n",
    "print(\"Root Mean Squared Error (RMSE):\", mean_squared_error(y_test, y_pred_best_stand_alone, squared=False))\n",
    "print(\"R-squared (R2):\", r2_score(y_test, y_pred_best_stand_alone))\n",
    "\n",
    "# MSLE calculation must not have negative values in y_test and y_pred\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_pred_best_stand_alone)\n",
    "    print(\"Mean Squared Logarithmic Error (MSLE):\", msle)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309165c-5a06-40e8-a91d-a1c0da0f3cd5",
   "metadata": {},
   "source": [
    "# Set all HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf1181-5225-41a0-8660-c739b744059e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate HyperParameter\n",
    "param_grid_learning_rate = {\n",
    "    'learning_rate': np.arange(0.01, 1.01, 0.01),  # Learning rate from 0.01 to 1.00 with step 0.01\n",
    "}\n",
    "\n",
    "# Create a grid search object for learning rate\n",
    "grid_search_learning_rate = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror'),\n",
    "                            param_grid=param_grid_learning_rate,\n",
    "                            cv=5,  # Number of folds in cross-validation\n",
    "                            scoring='neg_mean_squared_error',  # Scoring metric\n",
    "                            verbose=1,  # Controls the verbosity\n",
    "                            n_jobs=-1)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search best learning rate\n",
    "grid_search_learning_rate.fit(X_train, y_train)\n",
    "\n",
    "# Best learning rate\n",
    "best_learning_rate = grid_search_learning_rate.best_params_['learning_rate']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Max Depth HyperParameter\n",
    "param_grid_max_depth = {\n",
    "    'max_depth': np.arange(1, 11, 1),  # Max depth from 1 to 10 with step 1\n",
    "}\n",
    "\n",
    "# Create a grid search object for max depth\n",
    "grid_search_max_depth = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate),\n",
    "                            param_grid=param_grid_max_depth,\n",
    "                            cv=5,  # Number of folds in cross-validation\n",
    "                            scoring='neg_mean_squared_error',  # Scoring metric\n",
    "                            verbose=1,  # Controls the verbosity\n",
    "                            n_jobs=-1)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search best max depth\n",
    "grid_search_max_depth.fit(X_train, y_train)\n",
    "\n",
    "# Best max depth\n",
    "best_max_depth = grid_search_max_depth.best_params_['max_depth']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# L2 HyperParameter\n",
    "param_grid_lambda = {\n",
    "    'reg_lambda': np.arange(0.0, 10.1, 0.1),  # Lambda (L2 regularization) from 0.0 to 10.0 with step 0.1\n",
    "}\n",
    "\n",
    "# Create a grid search object for lambda\n",
    "grid_search_lambda = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate, max_depth=best_max_depth),\n",
    "                            param_grid=param_grid_lambda,\n",
    "                            cv=5,  # Number of folds in cross-validation\n",
    "                            scoring='neg_mean_squared_error',  # Scoring metric\n",
    "                            verbose=1,  # Controls the verbosity\n",
    "                            n_jobs=-1)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search for best L2\n",
    "grid_search_lambda.fit(X_train, y_train)\n",
    "\n",
    "# Best L2\n",
    "best_lambda = grid_search_lambda.best_params_['reg_lambda']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# L1 HyperParameter\n",
    "param_grid_alpha = {\n",
    "    'reg_alpha': np.arange(0.0, 10.1, 0.1),  # Alpha (L1 regularization) from 0.0 to 10.0 with step 0.1\n",
    "}\n",
    "\n",
    "# Create a grid search object for alpha\n",
    "grid_search_alpha = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate, max_depth=best_max_depth, reg_lambda=best_lambda),\n",
    "                            param_grid=param_grid_alpha,\n",
    "                            cv=5,  # Number of folds in cross-validation\n",
    "                            scoring='neg_mean_squared_error',  # Scoring metric\n",
    "                            verbose=1,  # Controls the verbosity\n",
    "                            n_jobs=-1)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search for best L1\n",
    "grid_search_alpha.fit(X_train, y_train)\n",
    "\n",
    "# Best L1\n",
    "best_alpha = grid_search_alpha.best_params_['reg_alpha']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Gamma HyperParameter\n",
    "param_grid_gamma = {\n",
    "    'gamma': np.arange(0.0, 1.1, 0.1),  # Gamma from 0.0 to 1.0 with step 0.1\n",
    "}\n",
    "\n",
    "# Create a grid search object for gamma\n",
    "grid_search_gamma = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate, max_depth=best_max_depth, reg_lambda=best_lambda, alpha=best_alpha),\n",
    "                            param_grid=param_grid_gamma,\n",
    "                            cv=5,  # Number of folds in cross-validation\n",
    "                            scoring='neg_mean_squared_error',  # Scoring metric\n",
    "                            verbose=1,  # Controls the verbosity\n",
    "                            n_jobs=-1)  # Number of jobs to run in parallel (-1: all processors)\n",
    "\n",
    "# Search for best gamma\n",
    "grid_search_gamma.fit(X_train, y_train)\n",
    "\n",
    "# Best gamma\n",
    "best_gamma = grid_search_gamma.best_params_['gamma']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Train the model using the best hyperparameter values\n",
    "best_model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=best_learning_rate, max_depth=best_max_depth, reg_lambda=best_lambda, reg_alpha=best_alpha, gamma=best_gamma)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set using the best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Best model evaluation\n",
    "print(\"Mean Square Error (MSE):\", mean_squared_error(y_test, y_pred_best))\n",
    "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred_best))\n",
    "print(\"Root Mean Squared Error (RMSE):\", mean_squared_error(y_test, y_pred_best, squared=False))\n",
    "print(\"R-squared (R2):\", r2_score(y_test, y_pred_best))\n",
    "\n",
    "\n",
    "# MSLE calculation must not have negative values in y_test and y_pred\n",
    "try:\n",
    "    msle = mean_squared_log_error(y_test, y_pred_best)\n",
    "    print(\"Mean Squared Logarithmic Error (MSLE):\", msle)\n",
    "except ValueError:\n",
    "    print(\"Mean Squared Logarithmic Error cannot be calculated because targets contain negative values.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
