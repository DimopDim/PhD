{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6243b02-165d-496a-a198-f00704a71eb6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671463d-f9c0-4542-b28c-eec0dfb36e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import textwrap\n",
    "import logging\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "#from matplotlib import pyplot\n",
    "#from skopt import BayesSearchCV\n",
    "#from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterGrid, train_test_split, ParameterSampler\n",
    "#from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error, mean_squared_log_error, r2_score\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import LSTM, Dense, Dropout, Input\n",
    "#from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8efbb-083c-4230-b048-59045707bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"logging.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b069035-3469-4e8c-8849-9225e0fad52e",
   "metadata": {},
   "source": [
    "# Setup paths and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566b7a2-c82e-495f-bd3e-40eff4b34490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETTINGS ===\n",
    "base_path = \"../../07_Imputation/CSV/exports/CIR-16/impute/\"\n",
    "observation_window = 'o4'\n",
    "label = 'los'\n",
    "model_output_dir = \"models/\"\n",
    "plot_dir_error = \"plots/03_Error_Metric_Plots\"\n",
    "plot_dir_most_important_shap = \"plots/01_Most_Important_SHAP\"\n",
    "plot_dir_true_vs_predict = \"plots/02_Prediction_Plot/02_true_vs_pred\"\n",
    "plot_dir_residuals = \"plots/02_Prediction_Plot/01_residuals\"\n",
    "plot_dir_calibration = \"plots/04_Calibration_Plots\"\n",
    "\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "os.makedirs(plot_dir_error, exist_ok=True)\n",
    "os.makedirs(plot_dir_most_important_shap, exist_ok=True)\n",
    "os.makedirs(plot_dir_true_vs_predict, exist_ok=True)\n",
    "os.makedirs(plot_dir_residuals, exist_ok=True)\n",
    "os.makedirs(plot_dir_calibration, exist_ok=True)\n",
    "\n",
    "# No HP\n",
    "#plot_dir_error_noHP = os.path.join(plot_dir_error, \"no_HP\")\n",
    "#plot_dir_most_important_shap_noHP = os.path.join(plot_dir_most_important_shap, \"no_HP\")\n",
    "#plot_dir_true_vs_predict_noHP = os.path.join(plot_dir_true_vs_predict, \"no_HP\")\n",
    "#plot_dir_residuals_noHP = os.path.join(plot_dir_residuals, \"no_HP\")\n",
    "#plot_dir_calibration_noHP = os.path.join(plot_dir_calibration, \"no_HP\")\n",
    "\n",
    "#os.makedirs(plot_dir_error_noHP, exist_ok=True)\n",
    "#os.makedirs(plot_dir_most_important_shap_noHP, exist_ok=True)\n",
    "#os.makedirs(plot_dir_true_vs_predict_noHP, exist_ok=True)\n",
    "#os.makedirs(plot_dir_residuals_noHP, exist_ok=True)\n",
    "#os.makedirs(plot_dir_calibration_noHP, exist_ok=True)\n",
    "\n",
    "# HyperOpt Folders\n",
    "#plot_dir_error_hopt = os.path.join(plot_dir_error, \"hyperopt\")\n",
    "#plot_dir_most_important_shap_hopt = os.path.join(plot_dir_most_important_shap, \"hyperopt\")\n",
    "#plot_dir_true_vs_predict_hopt = os.path.join(plot_dir_true_vs_predict, \"hyperopt\")\n",
    "#plot_dir_residuals_hopt = os.path.join(plot_dir_residuals, \"hyperopt\")\n",
    "#plot_dir_calibration_hopt = os.path.join(plot_dir_calibration, \"hyperopt\")\n",
    "\n",
    "#os.makedirs(plot_dir_error_hopt, exist_ok=True)\n",
    "#os.makedirs(plot_dir_most_important_shap_hopt, exist_ok=True)\n",
    "#os.makedirs(plot_dir_true_vs_predict_hopt, exist_ok=True)\n",
    "#os.makedirs(plot_dir_residuals_hopt, exist_ok=True)\n",
    "#os.makedirs(plot_dir_calibration_hopt, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80845ae1-62fa-45d8-8b69-a5a3c959c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HELPER FUNCTION TO MATCH FILES ===\n",
    "def find_file(path, pattern):\n",
    "    matches = glob.glob(os.path.join(path, pattern))\n",
    "    return matches[0] if matches else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbe6c8-1f81-421c-9207-844c985baeef",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f7fa12-1a8b-4560-9bbb-bb9d98e656bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_dirs(config_label: str):\n",
    "    dirs = {\n",
    "        \"error\": os.path.join(plot_dir_error, config_label),\n",
    "        \"shap\": os.path.join(plot_dir_most_important_shap, config_label),\n",
    "        \"true_vs_pred\": os.path.join(plot_dir_true_vs_predict, config_label),\n",
    "        \"residuals\": os.path.join(plot_dir_residuals, config_label),\n",
    "        \"calibration\": os.path.join(plot_dir_calibration, config_label)\n",
    "    }\n",
    "    for d in dirs.values():\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bd7766-7b41-4042-96b2-c151da2288e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plots error metrics (MSE, MAE, RMSE, MSLE if possible) and R².\n",
    "\"\"\"\n",
    "def plot_error_metrics(y_true, y_pred, file_prefix: str, plot_label: str, config_label: str, save_dir: str):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred) * 100  # Convert to percentage\n",
    "\n",
    "    error_metrics = ['MSE', 'MAE', 'RMSE']\n",
    "    values = [mse, mae, rmse]\n",
    "\n",
    "    msle = np.nan\n",
    "    try:\n",
    "        msle = mean_squared_log_error(y_true, y_pred)\n",
    "        logging.info(f\"{plot_label.title()} Set MSLE: {msle:.4f}\")\n",
    "        error_metrics.append('MSLE')\n",
    "        values.append(msle)\n",
    "    except ValueError:\n",
    "        logging.info(f\"{plot_label.title()} Set MSLE: Not computable due to negative values.\")\n",
    "\n",
    "    # Plot bar chart of error metrics\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(error_metrics, values, color=['blue', 'green', 'red', 'orange'][:len(error_metrics)])\n",
    "    plt.xlabel('Error Metric')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'Error Metrics ({plot_label.title()} Set) - {config_label}')\n",
    "\n",
    "    # Add value labels on top of each bar\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2.0, height,\n",
    "                 f'{value:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    bar_path = os.path.join(save_dir, f\"{file_prefix}_{plot_label}_{config_label}_error_metrics.png\")\n",
    "    plt.savefig(bar_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot R² pie chart\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    if r2 >= 0:\n",
    "        plt.pie([r2, 100 - r2], labels=['Explained Variance (R2)', 'Unexplained Variance'],\n",
    "                colors=['lightblue', 'lightgrey'], autopct='%1.1f%%')\n",
    "    else:\n",
    "        plt.pie([100], labels=['Unexplained Variance'], colors=['lightgrey'], autopct='%1.1f%%')\n",
    "    plt.title(f'Explained Variance by R² ({plot_label.title()} Set) - {config_label}')\n",
    "    pie_path = os.path.join(save_dir, f\"{file_prefix}_{plot_label}_{config_label}_R2.png\")\n",
    "    plt.savefig(pie_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    logging.info(f\"Saved error metrics plot to {bar_path}\")\n",
    "    logging.info(f\"Saved R² pie chart to {pie_path}\")\n",
    "\n",
    "    return {\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"MSLE\": msle\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6feeb8-1c71-44e1-93b7-1f5817a0e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate and save a plot of the top N most important features.\n",
    "\"\"\"\n",
    "def feature_importance_plot(model, X_train, file_prefix: str, top_n: int = 20, save_dir: str = None):\n",
    "    importances = model.feature_importances_\n",
    "    feature_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False).head(top_n)\n",
    "    feature_df['Importance'] *= 100000  # Optional scaling\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_df, x='Importance', y='Feature', color='steelblue')\n",
    "    plt.title(f'Top {top_n} Most Important Features - {file_prefix}')\n",
    "    plt.xlabel('Importance (scaled)')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save to specified directory\n",
    "    path = os.path.join(save_dir, f\"{file_prefix}_top{top_n}_feature_importance.png\")\n",
    "    plt.savefig(path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    logging.info(f\"Saved feature importance plot to {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c217a22-872b-4131-aca9-0bff0e8cce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate and save a SHAP summary plot (dot type) for the given model and training data.\n",
    "\"\"\"\n",
    "def generate_shap_plot(model, X_train, file_prefix: str, top_n: int = 20, save_dir: str = \".\"):\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "        mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "        shap_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Mean Absolute SHAP Value': mean_abs_shap\n",
    "        }).sort_values(by='Mean Absolute SHAP Value', ascending=False)\n",
    "\n",
    "        logging.info(f\"Top {top_n} features by SHAP for {file_prefix}:\\n{shap_df.head(top_n)}\")\n",
    "\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X_train, plot_type=\"dot\", show=False)\n",
    "        shap_path = os.path.join(save_dir, f\"{file_prefix}_shap_plot_top{top_n}.png\")\n",
    "        plt.savefig(shap_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        logging.info(f\"Saved SHAP summary plot to {shap_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to generate SHAP plot for {file_prefix}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829e517-4399-4f73-819f-cb39cc25e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot predicted vs. true values for LOS.\n",
    "\"\"\"\n",
    "\n",
    "def plot_true_vs_pred(y_true, y_pred, file_prefix: str, set_name: str, save_dir: str):\n",
    "    # Ensure 1D arrays\n",
    "    y_true = np.ravel(y_true)\n",
    "    y_pred = np.ravel(y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, y_pred, color='blue', label='Prediction', alpha=0.6)\n",
    "\n",
    "    # Compute robust line range\n",
    "    min_val = float(np.min([y_true.min(), y_pred.min()]))\n",
    "    max_val = float(np.max([y_true.max(), y_pred.max()]))\n",
    "    line = np.linspace(min_val, max_val, 100)\n",
    "    plt.plot(line, line, color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "    plt.xlabel('True LOS')\n",
    "    plt.ylabel('Predicted LOS')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(f'Predicted vs. True LOS ({set_name.title()})')\n",
    "\n",
    "    # Save to universal directory\n",
    "    path = os.path.join(save_dir, f\"{file_prefix}_true_vs_pred_{set_name}_plot.png\")\n",
    "    plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    logging.info(f\"Saved {set_name} True vs. Predicted plot to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c19d23-f12c-4d5c-adaa-7ffdf7040f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot residuals (true - predicted) with MAE bounds and save to file.\n",
    "\"\"\"\n",
    "def plot_residuals(y_true, y_pred, mae: float, file_prefix: str, save_dir: str):\n",
    "    y_true = np.ravel(y_true)\n",
    "    y_pred = np.ravel(y_pred)\n",
    "    residuals = y_true - y_pred\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, residuals, color='blue', alpha=0.5, label=\"Residuals\")\n",
    "    plt.axhline(y=0, color='red', linestyle='--', label=\"Zero Line\")\n",
    "    plt.axhline(y=mae, color='green', linestyle='--', label=f\"MAE = {mae:.2f}\")\n",
    "    plt.axhline(y=-mae, color='green', linestyle='--')\n",
    "\n",
    "    plt.xlabel('True LOS')\n",
    "    plt.ylabel('Residuals (True - Predicted)')\n",
    "    plt.title('Residuals Plot with MAE Bounds')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "    # Save to provided directory\n",
    "    residuals_path = os.path.join(save_dir, f\"{file_prefix}_residuals_plot.png\")\n",
    "    plt.savefig(residuals_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    logging.info(f\"Saved residuals plot to {residuals_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d68c96-50d8-4677-af41-99c4273b4384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate and save a calibration plot comparing predicted and actual LOS values.\n",
    "\"\"\"\n",
    "def plot_calibration(y_true, y_pred, file_prefix: str, set_name: str, save_dir: str):\n",
    "    y_true = np.ravel(y_true)\n",
    "    y_pred = np.ravel(y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.regplot(\n",
    "        x=y_true,\n",
    "        y=y_pred,\n",
    "        lowess=True,\n",
    "        line_kws={'color': 'red'},\n",
    "        scatter_kws={'alpha': 0.4}\n",
    "    )\n",
    "\n",
    "    # Perfect calibration line (y = x)\n",
    "    min_val = float(np.min([y_true.min(), y_pred.min()]))\n",
    "    max_val = float(np.max([y_true.max(), y_pred.max()]))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "\n",
    "    plt.xlabel('Actual LOS')\n",
    "    plt.ylabel('Predicted LOS')\n",
    "    plt.title(f'Calibration Plot: {set_name.title()}')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save to universal directory\n",
    "    calibration_path = os.path.join(save_dir, f\"{file_prefix}_calibration_{set_name}.png\")\n",
    "    plt.savefig(calibration_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    logging.info(f\"Saved calibration plot to {calibration_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0016784f-24f0-4ecb-b517-f988d612c0e5",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "## Without HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9715fb-d168-46ff-873c-0bfc9f1e5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgboost(config_label=\"noHP\"):\n",
    "    # Set up dynamic subfolder structure for plots\n",
    "    plot_dirs = {\n",
    "        \"error\": os.path.join(plot_dir_error, config_label),\n",
    "        \"shap\": os.path.join(plot_dir_most_important_shap, config_label),\n",
    "        \"true_vs_pred\": os.path.join(plot_dir_true_vs_predict, config_label),\n",
    "        \"residuals\": os.path.join(plot_dir_residuals, config_label),\n",
    "        \"calibration\": os.path.join(plot_dir_calibration, config_label)\n",
    "    }\n",
    "\n",
    "    # Create folders if they don't exist\n",
    "    for d in plot_dirs.values():\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    all_metrics = []\n",
    "    seq_folders = sorted([f for f in os.listdir(base_path) if f.startswith(\"seq_\")])\n",
    "\n",
    "    for folder in seq_folders:\n",
    "        logging.info(f\"Processing folder: {folder}\")\n",
    "        load_path = os.path.join(base_path, folder)\n",
    "        load_path_label = os.path.join(base_path, \"labels\")\n",
    "\n",
    "        try:\n",
    "            # Load data\n",
    "            X_train = pd.read_csv(find_file(load_path, f\"{observation_window}_X_train*.csv\"))\n",
    "            y_train = pd.read_csv(find_file(load_path_label, f\"{observation_window}_y_train_{label}.csv\"))\n",
    "            X_validate = pd.read_csv(find_file(load_path, f\"{observation_window}_X_validate*.csv\"))\n",
    "            y_validate = pd.read_csv(find_file(load_path_label, f\"{observation_window}_y_validate_{label}.csv\"))\n",
    "            X_test = pd.read_csv(find_file(load_path, f\"{observation_window}_X_test*.csv\"))\n",
    "            y_test = pd.read_csv(find_file(load_path_label, f\"{observation_window}_y_test_{label}.csv\"))\n",
    "            X_external = pd.read_csv(find_file(load_path, f\"{observation_window}_X_external*.csv\"))\n",
    "            y_external = pd.read_csv(find_file(load_path_label, f\"{observation_window}_y_external_{label}.csv\"))\n",
    "\n",
    "            # Train model\n",
    "            model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            file_prefix = f\"{folder}_{observation_window}_{label}_{config_label}\"\n",
    "            model.save_model(os.path.join(model_output_dir, f\"{file_prefix}_model.json\"))\n",
    "\n",
    "            # Predict\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_pred_ext = model.predict(X_external)\n",
    "\n",
    "            # Generate plots (all with universal save_dir)\n",
    "            plot_true_vs_pred(y_test, y_pred_test, file_prefix, set_name=\"test\", save_dir=plot_dirs[\"true_vs_pred\"])\n",
    "            plot_true_vs_pred(y_external, y_pred_ext, file_prefix, set_name=\"external\", save_dir=plot_dirs[\"true_vs_pred\"])\n",
    "\n",
    "            plot_residuals(y_test, y_pred_test, mae=mean_absolute_error(y_test, y_pred_test),\n",
    "                           file_prefix=file_prefix, save_dir=plot_dirs[\"residuals\"])\n",
    "\n",
    "            plot_calibration(y_test, y_pred_test, file_prefix, set_name=\"internal\", save_dir=plot_dirs[\"calibration\"])\n",
    "            plot_calibration(y_external, y_pred_ext, file_prefix, set_name=\"external\", save_dir=plot_dirs[\"calibration\"])\n",
    "\n",
    "            internal_metrics = plot_error_metrics(y_test, y_pred_test, file_prefix,\n",
    "                                                  plot_label='internal', config_label=config_label,\n",
    "                                                  save_dir=plot_dirs[\"error\"])\n",
    "            external_metrics = plot_error_metrics(y_external, y_pred_ext, file_prefix,\n",
    "                                                  plot_label='external', config_label=config_label,\n",
    "                                                  save_dir=plot_dirs[\"error\"])\n",
    "\n",
    "            feature_importance_plot(model, X_train, file_prefix, top_n=20, save_dir=plot_dirs[\"shap\"])\n",
    "            generate_shap_plot(model, X_train, file_prefix, top_n=20, save_dir=plot_dirs[\"shap\"])\n",
    "\n",
    "            all_metrics.append({\"folder\": folder, \"dataset\": \"internal\", **internal_metrics})\n",
    "            all_metrics.append({\"folder\": folder, \"dataset\": \"external\", **external_metrics})\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed in folder {folder}: {str(e)}\")\n",
    "\n",
    "    # Save metrics CSV\n",
    "    metrics_df = pd.DataFrame(all_metrics)\n",
    "    summary_csv_path = os.path.join(plot_dirs[\"error\"], f\"all_seq_metrics_{config_label}.csv\")\n",
    "    metrics_df.to_csv(summary_csv_path, index=False)\n",
    "    logging.info(f\"Saved summary metrics to: {summary_csv_path}\")\n",
    "\n",
    "    if metrics_df.empty:\n",
    "        logging.warning(\"No metrics were collected. Skipping summary metric plots.\")\n",
    "        return\n",
    "\n",
    "    # Create summary bar plots\n",
    "    metrics_melted = metrics_df.melt(\n",
    "        id_vars=[\"folder\", \"dataset\"],\n",
    "        value_vars=[\"MSE\", \"MAE\", \"RMSE\", \"R2\", \"MSLE\"],\n",
    "        var_name=\"Metric\",\n",
    "        value_name=\"Value\"\n",
    "    ).dropna()\n",
    "\n",
    "    for metric in metrics_melted[\"Metric\"].unique():\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        subset = metrics_melted[metrics_melted[\"Metric\"] == metric]\n",
    "        sns.barplot(data=subset, x=\"folder\", y=\"Value\", hue=\"dataset\", palette=\"Set2\", errorbar=None)\n",
    "        plt.title(f\"{metric} Comparison ({config_label})\")\n",
    "        plt.ylabel(metric)\n",
    "        plt.xlabel(\"Sequence Folder\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(title=\"Dataset\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        metric_plot_path = os.path.join(plot_dirs[\"error\"], f\"metric_{metric}_comparison_plot_{config_label}.png\")\n",
    "        plt.savefig(metric_plot_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# === MAIN FUNCTION ENTRY POINT ===\n",
    "#def main(model_type=\"xgboost\"):\n",
    "#    if model_type.lower() == \"xgboost\":\n",
    "#        run_xgboost()\n",
    "#    else:\n",
    "#        logging.error(f\"Model type '{model_type}' is not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c926a-8dfa-444c-96b4-6541d6d3b56d",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "## With HyperOpt HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e91b117-90f7-4e73-a372-5681ea7a6770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgboost_hyperopt():\n",
    "    all_metrics = []\n",
    "    seq_folders = sorted([f for f in os.listdir(base_path) if f.startswith(\"seq_\")])\n",
    "\n",
    "    # Define plot subfolders under the \"hyperopt\" category\n",
    "    plot_dirs = {\n",
    "        \"error\": os.path.join(plot_dir_error, \"hyperopt\"),\n",
    "        \"shap\": os.path.join(plot_dir_most_important_shap, \"hyperopt\"),\n",
    "        \"true_vs_pred\": os.path.join(plot_dir_true_vs_predict, \"hyperopt\"),\n",
    "        \"residuals\": os.path.join(plot_dir_residuals, \"hyperopt\"),\n",
    "        \"calibration\": os.path.join(plot_dir_calibration, \"hyperopt\")\n",
    "    }\n",
    "\n",
    "    for d in plot_dirs.values():\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    for folder in seq_folders:\n",
    "        logging.info(f\"Processing folder with HyperOpt: {folder}\")\n",
    "        load_path = os.path.join(base_path, folder)\n",
    "        load_path_label = os.path.join(base_path, \"labels\")\n",
    "\n",
    "        try:\n",
    "            # Load datasets\n",
    "            X_train = pd.read_csv(find_file(load_path, f\"{observation_window}_X_train*.csv\"))\n",
    "            y_train = pd.read_csv(find_file(load_path_label, f\"{observation_window}_y_train_{label}.csv\"))\n",
    "            X_validate = pd.read_csv(find_file(load_path, f\"{observation_window}_X_validate*.csv\"))\n",
    "            y_validate = pd.read_csv(find_file(load_path_label, f\"{observation_window}_y_validate_{label}.csv\"))\n",
    "            X_test = pd.read_csv(find_file(load_path, f\"{observation_window}_X_test*.csv\"))\n",
    "            y_test = pd.read_csv(find_file(load_path_label, f\"{observation_window}_y_test_{label}.csv\"))\n",
    "            X_external = pd.read_csv(find_file(load_path, f\"{observation_window}_X_external*.csv\"))\n",
    "            y_external = pd.read_csv(find_file(load_path_label, f\"{observation_window}_y_external_{label}.csv\"))\n",
    "\n",
    "            # HyperOpt objective function\n",
    "            def objective(params):\n",
    "                model = xgb.XGBRegressor(\n",
    "                    objective='reg:squarederror',\n",
    "                    n_estimators=int(params['n_estimators']),\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    max_depth=int(params['max_depth']),\n",
    "                    reg_lambda=params['reg_lambda'],\n",
    "                    reg_alpha=params['reg_alpha']\n",
    "                )\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred_val = model.predict(X_validate)\n",
    "                mse = mean_squared_error(y_validate, y_pred_val)\n",
    "                logging.info(\"Params: %s | Validation MSE: %.4f\", params, mse)\n",
    "                pbar.update(1)\n",
    "                return {'loss': mse, 'status': 'ok'}\n",
    "\n",
    "            # Hyperparameter space\n",
    "            param_space = {\n",
    "                'n_estimators': scope.int(hp.quniform('n_estimators', 100, 300, 50)),\n",
    "                'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1.0)),\n",
    "                'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
    "                'reg_lambda': hp.uniform('reg_lambda', 0.1, 15.0),\n",
    "                'reg_alpha': hp.uniform('reg_alpha', 0.1, 15.0)\n",
    "            }\n",
    "\n",
    "            MAX_EVALS = 50\n",
    "            pbar = tqdm(total=MAX_EVALS, desc=f\"HyperOpt Progress ({folder})\")\n",
    "            trials = Trials()\n",
    "            best = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=MAX_EVALS, trials=trials, show_progressbar=False)\n",
    "            pbar.close()\n",
    "            logging.info(\"Best parameters for %s: %s\", folder, best)\n",
    "\n",
    "            # Final model training\n",
    "            model = xgb.XGBRegressor(\n",
    "                objective='reg:squarederror',\n",
    "                n_estimators=int(best['n_estimators']),\n",
    "                learning_rate=best['learning_rate'],\n",
    "                max_depth=int(best['max_depth']),\n",
    "                reg_lambda=best['reg_lambda'],\n",
    "                reg_alpha=best['reg_alpha']\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            file_prefix = f\"{folder}_{observation_window}_{label}_Hopt\"\n",
    "            model.save_model(os.path.join(model_output_dir, f\"{file_prefix}_model.json\"))\n",
    "            logging.info(f\"Saved model to {model_output_dir}/{file_prefix}_model.json\")\n",
    "\n",
    "            # Predictions\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_pred_ext = model.predict(X_external)\n",
    "\n",
    "            # Generate plots and metrics\n",
    "            internal_metrics = plot_error_metrics(y_test, y_pred_test, file_prefix,\n",
    "                                                  plot_label='internal', config_label=\"Hopt\", save_dir=plot_dirs[\"error\"])\n",
    "            external_metrics = plot_error_metrics(y_external, y_pred_ext, file_prefix,\n",
    "                                                  plot_label='external', config_label=\"Hopt\", save_dir=plot_dirs[\"error\"])\n",
    "\n",
    "            feature_importance_plot(model, X_train, file_prefix, top_n=20, save_dir=plot_dirs[\"shap\"])\n",
    "            generate_shap_plot(model, X_train, file_prefix, top_n=20, save_dir=plot_dirs[\"shap\"])\n",
    "\n",
    "            plot_true_vs_pred(y_test, y_pred_test, file_prefix, set_name=\"test\", save_dir=plot_dirs[\"true_vs_pred\"])\n",
    "            plot_true_vs_pred(y_external, y_pred_ext, file_prefix, set_name=\"external\", save_dir=plot_dirs[\"true_vs_pred\"])\n",
    "\n",
    "            plot_residuals(y_test, y_pred_test, mae=internal_metrics['MAE'], file_prefix=file_prefix, save_dir=plot_dirs[\"residuals\"])\n",
    "            plot_calibration(y_test, y_pred_test, file_prefix, set_name=\"internal\", save_dir=plot_dirs[\"calibration\"])\n",
    "            plot_calibration(y_external, y_pred_ext, file_prefix, set_name=\"external\", save_dir=plot_dirs[\"calibration\"])\n",
    "\n",
    "            # Store metrics\n",
    "            all_metrics.append({\"folder\": folder, \"dataset\": \"internal\", **internal_metrics})\n",
    "            all_metrics.append({\"folder\": folder, \"dataset\": \"external\", **external_metrics})\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed in folder {folder} with HyperOpt: {str(e)}\")\n",
    "\n",
    "    # Save all metrics to CSV\n",
    "    metrics_df = pd.DataFrame(all_metrics)\n",
    "    summary_csv_path = os.path.join(plot_dirs[\"error\"], \"all_seq_metrics_Hopt.csv\")\n",
    "    metrics_df.to_csv(summary_csv_path, index=False)\n",
    "    logging.info(f\"Saved HyperOpt summary metrics to: {summary_csv_path}\")\n",
    "\n",
    "    # Plot summary barplots per metric\n",
    "    metrics_melted = metrics_df.melt(\n",
    "        id_vars=[\"folder\", \"dataset\"],\n",
    "        value_vars=[\"MSE\", \"MAE\", \"RMSE\", \"R2\", \"MSLE\"],\n",
    "        var_name=\"Metric\",\n",
    "        value_name=\"Value\"\n",
    "    ).dropna()\n",
    "\n",
    "    for metric in metrics_melted[\"Metric\"].unique():\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        subset = metrics_melted[metrics_melted[\"Metric\"] == metric]\n",
    "        sns.barplot(data=subset, x=\"folder\", y=\"Value\", hue=\"dataset\", palette=\"Set2\", errorbar=None)\n",
    "        plt.title(f\"{metric} Comparison with HyperOpt\")\n",
    "        plt.ylabel(metric)\n",
    "        plt.xlabel(\"Sequence Folder\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(title=\"Dataset\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        metric_plot_path = os.path.join(plot_dirs[\"error\"], f\"metric_{metric}_comparison_plot_Hopt.png\")\n",
    "        plt.savefig(metric_plot_path, dpi=300)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c32967-33e8-4772-bfde-f57caf7d903e",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "## With Optuna HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2efb0b2-8425-4539-a00f-21478509ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgboost_optuna():\n",
    "    import optuna\n",
    "\n",
    "    all_metrics = []\n",
    "    seq_folders = sorted([f for f in os.listdir(base_path) if f.startswith(\"seq_\")])\n",
    "\n",
    "    # Define Optuna subfolders\n",
    "    plot_dir_error_optuna = os.path.join(plot_dir_error, \"optuna\")\n",
    "    plot_dir_shap_optuna = os.path.join(plot_dir_most_important_shap, \"optuna\")\n",
    "    plot_dir_true_vs_pred_optuna = os.path.join(plot_dir_true_vs_predict, \"optuna\")\n",
    "    plot_dir_residuals_optuna = os.path.join(plot_dir_residuals, \"optuna\")\n",
    "    plot_dir_calibration_optuna = os.path.join(plot_dir_calibration, \"optuna\")\n",
    "\n",
    "    for d in [plot_dir_error_optuna, plot_dir_shap_optuna, plot_dir_true_vs_pred_optuna, plot_dir_residuals_optuna, plot_dir_calibration_optuna]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    for folder in seq_folders:\n",
    "        logging.info(f\"Processing folder with Optuna: {folder}\")\n",
    "        load_path = os.path.join(base_path, folder)\n",
    "        load_path_label = os.path.join(base_path, \"labels\")\n",
    "\n",
    "        try:\n",
    "            # Load data\n",
    "            X_train = pd.read_csv(find_file(load_path, f\"{observation_window}_X_train*.csv\"))\n",
    "            y_train = pd.read_csv(find_file(load_path_label, f\"{observation_window}_y_train_{label}.csv\"))\n",
    "            X_validate = pd.read_csv(find_file(load_path, f\"{observation_window}_X_validate*.csv\"))\n",
    "            y_validate = pd.read_csv(find_file(load_path_label, f\"{observation_window}_y_validate_{label}.csv\"))\n",
    "            X_test = pd.read_csv(find_file(load_path, f\"{observation_window}_X_test*.csv\"))\n",
    "            y_test = pd.read_csv(find_file(load_path_label, f\"{observation_window}_y_test_{label}.csv\"))\n",
    "            X_external = pd.read_csv(find_file(load_path, f\"{observation_window}_X_external*.csv\"))\n",
    "            y_external = pd.read_csv(find_file(load_path_label, f\"{observation_window}_y_external_{label}.csv\"))\n",
    "\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    'objective': 'reg:squarederror',\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 300, step=50),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0, log=True),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 15.0),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 15.0)\n",
    "                }\n",
    "                model = xgb.XGBRegressor(**params)\n",
    "                model.fit(X_train, y_train)\n",
    "                preds = model.predict(X_validate)\n",
    "                return mean_squared_error(y_validate, preds)\n",
    "\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=3, show_progress_bar=True)\n",
    "\n",
    "            logging.info(f\"Best parameters for {folder}: {study.best_params}\")\n",
    "            best_params = study.best_params\n",
    "\n",
    "            model = xgb.XGBRegressor(objective='reg:squarederror', **best_params)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            file_prefix = f\"{folder}_{observation_window}_{label}_Optuna\"\n",
    "            model_output_path = os.path.join(model_output_dir, f\"{file_prefix}_model.json\")\n",
    "            model.save_model(model_output_path)\n",
    "\n",
    "            # Predictions\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_pred_ext = model.predict(X_external)\n",
    "\n",
    "            # Error Metrics\n",
    "            internal_metrics = plot_error_metrics(\n",
    "                y_true=y_test,\n",
    "                y_pred=y_pred_test,\n",
    "                save_dir=plot_dir_error_optuna,\n",
    "                file_prefix=file_prefix,\n",
    "                plot_label='internal',\n",
    "                config_label=\"Optuna\"\n",
    "            )\n",
    "\n",
    "            external_metrics = plot_error_metrics(\n",
    "                y_true=y_external,\n",
    "                y_pred=y_pred_ext,\n",
    "                save_dir=plot_dir_error_optuna,\n",
    "                file_prefix=file_prefix,\n",
    "                plot_label='external',\n",
    "                config_label=\"Optuna\"\n",
    "            )\n",
    "\n",
    "\n",
    "            # Feature Importance and SHAP\n",
    "            feature_importance_plot(model, X_train, file_prefix=os.path.join(plot_dir_shap_optuna, file_prefix), top_n=20)\n",
    "            generate_shap_plot(model, X_train, file_prefix=os.path.join(plot_dir_shap_optuna, file_prefix), top_n=20)\n",
    "\n",
    "            # True vs Pred\n",
    "            plot_true_vs_pred(y_test, y_pred_test, file_prefix=os.path.join(plot_dir_true_vs_pred_optuna, file_prefix), set_name=\"test\")\n",
    "            plot_true_vs_pred(y_external, y_pred_ext, file_prefix=os.path.join(plot_dir_true_vs_pred_optuna, file_prefix), set_name=\"external\")\n",
    "\n",
    "            # Residuals\n",
    "            residual_path_test = os.path.join(plot_dir_residuals_optuna, f\"{file_prefix}_residuals_test.png\")\n",
    "            plot_residuals(y_test, y_pred_test, mae=internal_metrics.get(\"MAE\", None), file_prefix=residual_path_test)\n",
    "\n",
    "            # Calibration\n",
    "            plot_calibration(y_test, y_pred_test, file_prefix=os.path.join(plot_dir_calibration_optuna, file_prefix), set_name=\"internal\")\n",
    "            plot_calibration(y_external, y_pred_ext, file_prefix=os.path.join(plot_dir_calibration_optuna, file_prefix), set_name=\"external\")\n",
    "\n",
    "            # Save metrics\n",
    "            all_metrics.append({\"folder\": folder, \"dataset\": \"internal\", **internal_metrics})\n",
    "            all_metrics.append({\"folder\": folder, \"dataset\": \"external\", **external_metrics})\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed in folder {folder} with Optuna: {str(e)}\")\n",
    "\n",
    "    # Save all metrics summary\n",
    "    metrics_df = pd.DataFrame(all_metrics)\n",
    "    summary_csv_path = os.path.join(plot_dir_error_optuna, \"all_seq_metrics_Optuna.csv\")\n",
    "    metrics_df.to_csv(summary_csv_path, index=False)\n",
    "    logging.info(f\"Saved Optuna summary metrics to: {summary_csv_path}\")\n",
    "\n",
    "    # Defensive check before plotting\n",
    "    required_cols = {\"folder\", \"dataset\", \"MSE\", \"MAE\", \"RMSE\", \"R2\", \"MSLE\"}\n",
    "    if metrics_df.empty or not required_cols.issubset(metrics_df.columns):\n",
    "        logging.error(\"Failed to generate Optuna metric plots: required columns missing from metrics_df.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        metrics_melted = metrics_df.melt(\n",
    "            id_vars=[\"folder\", \"dataset\"],\n",
    "            value_vars=[\"MSE\", \"MAE\", \"RMSE\", \"R2\", \"MSLE\"],\n",
    "            var_name=\"Metric\",\n",
    "            value_name=\"Value\"\n",
    "        ).dropna()\n",
    "\n",
    "        for metric in metrics_melted[\"Metric\"].unique():\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            subset = metrics_melted[metrics_melted[\"Metric\"] == metric]\n",
    "            sns.barplot(data=subset, x=\"folder\", y=\"Value\", hue=\"dataset\", palette=\"Set2\", errorbar=None)\n",
    "            plt.title(f\"{metric} Comparison with Optuna\")\n",
    "            plt.ylabel(metric)\n",
    "            plt.xlabel(\"Sequence Folder\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.legend(title=\"Dataset\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            metric_plot_path = os.path.join(plot_dir_error_optuna, f\"metric_{metric}_comparison_plot_Optuna.png\")\n",
    "            plt.savefig(metric_plot_path, dpi=300)\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to generate Optuna metric plots: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62f381-6a90-49fc-8487-d635c412101d",
   "metadata": {},
   "source": [
    "# Call the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a46ee6-b312-46a3-91a0-7cc94b3829ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_xgboost()\n",
    "\n",
    "run_xgboost_hyperopt()\n",
    "\n",
    "#run_xgboost_optuna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9175b32-fac4-4875-b1f2-f8e64eb9a43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa322981-eeeb-4556-b672-3e8dfa4e7f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e994ccda-f0ac-4dc4-94b3-64335da6ae7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2445f552-ef19-4141-a4b2-06ff908f1213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204f05d-1a46-4008-a679-8d456a015238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74908869-bd36-4ad7-962a-1282350a8921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc53466-a9ba-4bdb-8122-61d43c36d358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b7c88-d88d-4b65-a92a-2ed976033f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a85ce-332e-40d9-aa3f-4145ddcd15ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71755487-5db6-4b99-bc48-151e89cf0780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a26790-948e-4503-bd10-ab3f17979c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c928a-f8bd-4e04-8081-f58864576a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e904c-59f0-4c19-be34-728f3dea52ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df8061-0d9e-4607-906e-06db392d6467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4188a-e654-4c22-a3e9-9119ee9b18c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb72421-8e33-4429-a1d9-e180a15a1d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7ad27-4477-461c-99b9-2f03787a15f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e16d8e0-d5c0-4e7e-a9c4-4ed697c852dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76faa84b-88b6-4b34-b000-8af0a4281216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044931cf-e025-4772-a5a5-31a5f1b5b4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7f727d0-6369-4e0e-9d2f-ae447500734d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HP GridSearchCV\n",
    "## To slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e5d12-f1bd-4242-9d64-35a14ba91996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A smaller learning rate makes the boosting\n",
    "process more robust and can lead to better\n",
    "generalization but requires more trees\n",
    "(higher n_estimators) to achieve the same result.\n",
    "A larger learning rate speeds up training bu\n",
    "may risk overfitting.\n",
    "\"\"\"\n",
    "\n",
    "# Define the parameter grid\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300], # controls the total number of trees in the ensemble\n",
    "    'learning_rate': np.arange(0.01, 1.02, 0.2),\n",
    "    'max_depth': np.arange(1, 10, 1),\n",
    "    'reg_lambda': np.arange(0.1, 15.1, 1),\n",
    "    'reg_alpha': np.arange(0.1, 15.1, 1)\n",
    "}\n",
    "\n",
    "# Create an XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', \n",
    "                           cv=3,  # Number of folds for cross-validation\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score (negative MSE):\", grid_search.best_score_)\n",
    "\n",
    "# Predict on the validation set with the best model\n",
    "y_pred_validate = grid_search.predict(X_validate)\n",
    "\n",
    "# Optionally: Evaluate the model on the validation set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_validate, y_pred_validate)\n",
    "print(\"Validation MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae09b09f-e87b-436b-bf5d-a35bb597659b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HP RandomizedSearchCV & Train Model\n",
    "Choose randomly samples a subset of hyperparameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3316a365-114c-4f10-9553-469d66342b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': np.arange(0.01, 1.02, 0.2),\n",
    "    'max_depth': np.arange(1, 10, 1),\n",
    "    'reg_lambda': np.arange(0.1, 15.1, 1),\n",
    "    'reg_alpha': np.arange(0.1, 15.1, 1)\n",
    "}\n",
    "\n",
    "# Number of random samples\n",
    "n_iter = 50\n",
    "\n",
    "# Generate random combinations\n",
    "param_list = list(ParameterSampler(param_distributions, n_iter=n_iter, random_state=42))\n",
    "\n",
    "# Tracking best model\n",
    "best_score = float('inf')\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Progress bar\n",
    "for params in tqdm(param_list, desc=\"Hyperparameter tuning\"):\n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror', **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred_val = model.predict(X_validate)\n",
    "    \n",
    "    # Evaluate with MSE\n",
    "    mse = mean_squared_error(y_validate, y_pred_val)\n",
    "    \n",
    "    if mse < best_score:\n",
    "        best_score = mse\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "\n",
    "# Evaluate best model on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Evaluate on external validation set\n",
    "y_pred_external = best_model.predict(X_external)\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "\n",
    "# Results\n",
    "logging.info(f\"Best parameters: {best_params}\")\n",
    "logging.info(f\"Best validation MSE: {best_score}\")\n",
    "logging.info(f\"Test Set - MSE: {mse_test}, MAE: {mae_test}\")\n",
    "logging.info(f\"External Validation Set - MSE: {mse_external}, MAE: {mae_external}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110b1d7-7f38-42df-9eed-ed0829249eee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HP Bayesian Optimization & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09340dad-8d72-4555-9b84-07c6afa84bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=50, desc=\"Bayesian Optimization Progress\")\n",
    "\n",
    "# Callback to update tqdm\n",
    "def on_step(optim_result):\n",
    "    pbar.update(1)\n",
    "\n",
    "# Define the parameter search space\n",
    "param_space = {\n",
    "    'n_estimators': (100, 300),\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'max_depth': (1, 10),\n",
    "    'reg_lambda': (0.1, 15.0),\n",
    "    'reg_alpha': (0.1, 15.0)\n",
    "}\n",
    "\n",
    "# Create the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Create BayesSearchCV for Bayesian Optimization\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    search_spaces=param_space,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    verbose=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit BayesSearchCV with tqdm callback\n",
    "bayes_search.fit(X_train, y_train, callback=on_step)\n",
    "pbar.close()\n",
    "\n",
    "# Log best parameters and score\n",
    "logging.info(\"Best parameters: %s\", bayes_search.best_params_)\n",
    "logging.info(\"Best score (negative MSE): %.4f\", bayes_search.best_score_)\n",
    "\n",
    "# Predict on the validation set with the best model\n",
    "y_pred_validate = bayes_search.predict(X_validate)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "mse_validate = mean_squared_error(y_validate, y_pred_validate)\n",
    "mae_validate = mean_absolute_error(y_validate, y_pred_validate)\n",
    "logging.info(\"Validation MSE: %.4f\", mse_validate)\n",
    "logging.info(\"Validation MAE: %.4f\", mae_validate)\n",
    "\n",
    "# Extract the best hyperparameters from BayesSearchCV\n",
    "best_params = bayes_search.best_params_\n",
    "\n",
    "# Initialize the XGBoost model with the best hyperparameters\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    reg_lambda=best_params['reg_lambda'],\n",
    "    reg_alpha=best_params['reg_alpha']\n",
    ")\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set (eICU data)\n",
    "y_pred_external = model.predict(X_external)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on the external validation set\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "\n",
    "# Log final evaluation metrics\n",
    "logging.info(\"Test Set - MSE: %.4f, MAE: %.4f\", mse_test, mae_test)\n",
    "logging.info(\"External Validation Set (eICU) - MSE: %.4f, MAE: %.4f\", mse_external, mae_external)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63d430b-79c1-4340-835e-119a468f1483",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# HP HyperOpt & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b2115-569e-4929-90c0-da230a015f50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the number of evaluations\n",
    "MAX_EVALS = 50\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=MAX_EVALS, desc=\"HyperOpt Progress\")\n",
    "\n",
    "# Define the wrapped objective function\n",
    "def objective(params):\n",
    "    model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=int(params['n_estimators']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=int(params['max_depth']),\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        reg_alpha=params['reg_alpha']\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred_validate = model.predict(X_validate)\n",
    "    \n",
    "    # Compute the MSE\n",
    "    mse = mean_squared_error(y_validate, y_pred_validate)\n",
    "\n",
    "    # Log the result\n",
    "    logging.info(\"Params: %s | Validation MSE: %.4f\", params, mse)\n",
    "    \n",
    "    # Update progress bar\n",
    "    pbar.update(1)\n",
    "\n",
    "    return {'loss': mse, 'status': 'ok'}\n",
    "\n",
    "# Define the parameter search space\n",
    "param_space = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 100, 300, 50)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1.0)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.1, 15.0),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.1, 15.0)\n",
    "}\n",
    "\n",
    "# Create a Trials object to keep track of the search\n",
    "trials = Trials()\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=param_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=MAX_EVALS,\n",
    "    trials=trials,\n",
    "    show_progressbar=False  # Disable internal bar to avoid overlap with tqdm\n",
    ")\n",
    "\n",
    "# Close progress bar\n",
    "pbar.close()\n",
    "\n",
    "# Log the best parameters\n",
    "logging.info(\"Best parameters: %s\", best)\n",
    "\n",
    "# Initialize the XGBoost model with the best hyperparameters\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=int(best['n_estimators']),\n",
    "    learning_rate=best['learning_rate'],\n",
    "    max_depth=int(best['max_depth']),\n",
    "    reg_lambda=best['reg_lambda'],\n",
    "    reg_alpha=best['reg_alpha']\n",
    ")\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set (eICU data)\n",
    "y_pred_external = model.predict(X_external)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on the external validation set\n",
    "mse_external = mean_squared_error(y_external, y_pred_external)\n",
    "mae_external = mean_absolute_error(y_external, y_pred_external)\n",
    "\n",
    "# Log final evaluation results\n",
    "logging.info(\"Test Set - MSE: %.4f, MAE: %.4f\", mse_test, mae_test)\n",
    "logging.info(\"External Validation Set (eICU) - MSE: %.4f, MAE: %.4f\", mse_external, mae_external)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a810c9c-58b5-41f7-b700-e62558004bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "save_path = 'CSV/exports/impute/o03_Interpolation/'\n",
    "\n",
    "# Check if the directory exists, and if not, create it\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Save external validation set from eICU\n",
    "X_external.to_csv(save_path + 'X_external.csv', index=False)\n",
    "y_external.to_csv(save_path + 'y_external.csv', index=False)\n",
    "\n",
    "# Save training, validation, and test sets\n",
    "X_train.to_csv(save_path + 'X_train.csv', index=False)\n",
    "y_train.to_csv(save_path + 'y_train.csv', index=False)\n",
    "\n",
    "X_validate.to_csv(save_path + 'X_validate.csv', index=False)\n",
    "y_validate.to_csv(save_path + 'y_validate.csv', index=False)\n",
    "\n",
    "X_test.to_csv(save_path + 'X_test.csv', index=False)\n",
    "y_test.to_csv(save_path + 'y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1530e609-4239-4d84-88f6-48ed91bf5a28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e03eb8-70ee-4578-bd93-2b7032252210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and file path\n",
    "\n",
    "name = f\"{file_name}_model.json\"\n",
    "directory = 'models/'\n",
    "\n",
    "file_path = os.path.join(directory, name)\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the model as a JSON file\n",
    "model.save_model(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b303f38-a8c0-4488-8e9c-05a83768a275",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105cdba2-86f9-41a6-ba38-c11d492f9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model file path\n",
    "file_name = \"06\"  # replace with the actual name you used before saving\n",
    "directory = 'models/'\n",
    "file_path = os.path.join(directory, f\"{file_name}_model.json\")\n",
    "\n",
    "# Load the model\n",
    "model = xgb.XGBRegressor()\n",
    "model.load_model(file_path)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Predict on the external validation set\n",
    "y_pred_external = model.predict(X_external)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
